<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>All Posts - 152334H</title><link>https://152334H.github.io/posts/</link><description>All Posts | 152334H</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 13 Dec 2023 20:12:34 +0800</lastBuildDate><atom:link href="https://152334H.github.io/posts/" rel="self" type="application/rss+xml"/><item><title>Blog Refurbishment</title><link>https://152334H.github.io/blog/blog-refurbishment/</link><pubDate>Sat, 13 Aug 2022 05:30:32 +0100</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/blog-refurbishment/</guid><description>Under new management</description></item><item><title>Rough thoughts on Mixtral vs Open Source</title><link>https://152334H.github.io/blog/mixtral-vs-oss/</link><pubDate>Wed, 13 Dec 2023 20:12:34 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/mixtral-vs-oss/</guid><description><![CDATA[<p>Here&rsquo;s a <strong>thesis</strong> (hypothesis, predicate, etc) to chew on:</p>
<blockquote>
<p>The mixture-of-experts paradigm is fundamentally a hinderance to open source development, and mixtral-8x5B+2B will be summarily supplanted by a dense model like <a href="https://twitter.com/futuristflower/status/1716555972452184463" target="_blank" rel="noopener noreffer ">llama3</a>/<a href="https://techcrunch.com/2023/11/09/theres-something-going-on-with-ai-startups-in-france/" target="_blank" rel="noopener noreffer ">mistral-70b</a>/yi/qwen/&hellip; in the near future.</p>
</blockquote>]]></description></item><item><title>Knowing Enough About MoE to Explain Dropped Tokens in GPT-4</title><link>https://152334H.github.io/blog/knowing-enough-about-moe/</link><pubDate>Wed, 09 Aug 2023 05:15:14 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/knowing-enough-about-moe/</guid><description><![CDATA[<p>In a previous blogpost, I made a <a href="/blog/non-determinism-in-gpt-4/" rel="">simple observation</a> about GPT-4 from a <a href="https://arxiv.org/pdf/2308.00951.pdf" target="_blank" rel="noopener noreffer ">paper</a> I had incidentally read on a whim. After finishing the post, I realised I didn&rsquo;t actually ever figure out how <em>token dropping</em> could occur; only learning a black-box rule that it could occur in batched MoE inference for <em>reasons</em>.</p>
<p>This post is here to fix that &ndash; to collect enough info from important MoE papers (and alleged GPT-4 leaks) to explain the full mechanism of token dropping.</p>]]></description></item><item><title>Non-determinism in GPT-4 is caused by Sparse MoE</title><link>https://152334H.github.io/blog/non-determinism-in-gpt-4/</link><pubDate>Sat, 05 Aug 2023 04:09:15 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/non-determinism-in-gpt-4/</guid><description><![CDATA[<p>It&rsquo;s <a href="https://twitter.com/BorisMPower/status/1608522707372740609" target="_blank" rel="noopener noreffer ">well-known</a> at this point that GPT-4/GPT-3.5-turbo is non-deterministic, even at <code>temperature=0.0</code>. This is an odd behavior if you&rsquo;re used to dense decoder-only models, where temp=0 should imply <a href="https://nn.labml.ai/sampling/greedy.html" target="_blank" rel="noopener noreffer ">greedy sampling</a> which should imply full determinism, because the logits for the next token should be a pure function of the input sequence &amp; the model weights.</p>]]></description></item><item><title>Dumped Blog Ideas</title><link>https://152334H.github.io/blog/dumped-blog-ideas/</link><pubDate>Sun, 02 Jul 2023 01:07:44 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/dumped-blog-ideas/</guid><description>&lt;p>Despite the dead appearance of this blog, I actually think about it surprisingly often! Over the years, I&amp;rsquo;ve written a number of draft blogs or summaries that I simply ended up dumped at varying stages of completion.&lt;/p></description></item><item><title>Why can TorToiSe be fine-tuned?</title><link>https://152334H.github.io/blog/tortoise-fine-tuned/</link><pubDate>Thu, 16 Feb 2023 11:18:28 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/tortoise-fine-tuned/</guid><description><![CDATA[<p>Five days ago, I published a blog post, describing <a href="https://152334h.github.io/blog/tortoise-fine-tuning/" target="_blank" rel="noopener noreffer ">why TorToiSe could not be fine-tuned</a>.</p>
<p>Today, I have released a <a href="https://github.com/152334H/DL-Art-School" target="_blank" rel="noopener noreffer ">fork of DL-Art-School</a> with TorToiSe fine-tuning code. How did that happen?</p>]]></description></item><item><title>Why can't TorToiSe be fine-tuned?</title><link>https://152334H.github.io/blog/tortoise-fine-tuning/</link><pubDate>Sat, 11 Feb 2023 08:13:30 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/tortoise-fine-tuning/</guid><description><![CDATA[<p><a href="http://nonint.com/static/tortoise_v2_examples.html" target="_blank" rel="noopener noreffer ">TorToiSe üê¢</a> is an open-source Text-To-Speech (TTS) neural network that <abbr tabindex="0" title="It is also notable for its alleged similiarity to 11labs's TTS stack." style="color:#bbb;text-decoration-style: dotted;text-decoration-line:underline">creates fairly authentic &amp; realistic voices</abbr>. Checkpoints for local inference have been available since <a href="https://huggingface.co/jbetker/tortoise-tts-v2/tree/main/.models" target="_blank" rel="noopener noreffer ">April last year</a>, but its users are seemingly unable to fine-tune the model with additional voice data.</p>
<p>Why is this the case, and how could it be fixed?</p>]]></description></item><item><title>The bleak future of Artificial Intelligence in Singapore</title><link>https://152334H.github.io/blog/nus-mistake/</link><pubDate>Fri, 10 Feb 2023 21:57:53 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/nus-mistake/</guid><description><![CDATA[<p>On 9th Feb, the <a href="https://www.nus.edu.sg/" target="_blank" rel="noopener noreffer ">National University of Singapore</a> implemented <a href="https://web.archive.org/web/20230209131829/https://libguides.nus.edu.sg/new2nus/acadintegrity#s-lib-ctab-22144949-5" target="_blank" rel="noopener noreffer ">broad restrictions against the use of AI tools to generate work</a>:</p>]]></description></item><item><title>Fast (5x) Inference with TorToiSe-TTS</title><link>https://152334H.github.io/blog/tortoise-tts-fast/</link><pubDate>Sun, 05 Feb 2023 20:28:55 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/tortoise-tts-fast/</guid><description>&lt;p>I made a fork of TorToiSe with much faster inference speed. Here are the summarised results:&lt;/p></description></item><item><title>Contrastive Search might-not-be What You Need</title><link>https://152334H.github.io/blog/anisotropy/</link><pubDate>Mon, 12 Dec 2022 19:08:08 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/anisotropy/</guid><description><![CDATA[<h4>TL;DR</h4>
<ul>
<li><u><strong>GPT-NeoX-20B appears to be anisotropic</strong></u> (Isotropy: 0.197)</li>
<li>Int8 quantisation appears to have ~no effect on isotropy</li>
<li>I am new to ML so the above could be false, feel free to poke at my findings <a href="https://github.com/152334H/Contrastive_Search_Is_What_You_Need/tree/main/isotropy_analysis" target="_blank" rel="noopener noreffer ">here</a></li>
</ul>]]></description></item></channel></rss>