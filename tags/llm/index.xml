<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>llm - Tag - 152334H</title><link>https://152334H.github.io/tags/llm/</link><description>llm - Tag - 152334H</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 30 Jun 2024 00:00:00 +0800</lastBuildDate><atom:link href="https://152334H.github.io/tags/llm/" rel="self" type="application/rss+xml"/><item><title>DeepSeek Core Readings 0 - Coder</title><link>https://152334H.github.io/blog/deepseek-0/</link><pubDate>Sun, 30 Jun 2024 00:00:00 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/deepseek-0/</guid><description><![CDATA[<p><a href="https://arxiv.org/pdf/2401.14196" target="_blank" rel="noopener noreffer ">Paper</a> summary: 1.3B to 33B LLMs on 1/2T code tokens (87 langs) w/ FiM and 16K seqlen. Strong effort in constructing pretraining data from Github from scratch, with repository-level samples. Evals beat OSS code models solidly + GPT-3.5 a bit; Coder-7B &gt; CodeLlama-33B often.</p>
<p>They don&rsquo;t spend much effort on Instruction tuning. They commit a continued pretrain of DeepSeek LLM -&gt; Coder: I believe it underperforms; they don&rsquo;t.</p>]]></description></item><item><title>DeepSeek Core Readings 1 - LLM</title><link>https://152334H.github.io/blog/deepseek-1/</link><pubDate>Sun, 23 Jun 2024 00:00:00 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/deepseek-1/</guid><description><![CDATA[<p><a href="https://arxiv.org/abs/2401.02954" target="_blank" rel="noopener noreffer ">Paper</a> summary: LLaMA-like 7B/67B pretrain (Base) + SFT&amp;DPO (Chat). 2T tokens with strong CN/EN mix, &gt;1mil SFT examples. Well-executed exploration of scaling laws. Good details about evals and safety. Not much described about their actual data.</p>]]></description></item></channel></rss>