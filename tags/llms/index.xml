<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>llms - Tag - 152334H</title><link>https://152334H.github.io/tags/llms/</link><description>llms - Tag - 152334H</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 13 Dec 2023 20:12:34 +0800</lastBuildDate><atom:link href="https://152334H.github.io/tags/llms/" rel="self" type="application/rss+xml"/><item><title>Rough thoughts on Mixtral vs Open Source</title><link>https://152334H.github.io/blog/mixtral-vs-oss/</link><pubDate>Wed, 13 Dec 2023 20:12:34 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/mixtral-vs-oss/</guid><description><![CDATA[<p>Here&rsquo;s a <strong>thesis</strong> (hypothesis, predicate, etc) to chew on:</p>
<blockquote>
<p>The mixture-of-experts paradigm is fundamentally a hinderance to open source development, and mixtral-8x5B+2B will be summarily supplanted by a dense model like <a href="https://twitter.com/futuristflower/status/1716555972452184463" target="_blank" rel="noopener noreffer ">llama3</a>/<a href="https://techcrunch.com/2023/11/09/theres-something-going-on-with-ai-startups-in-france/" target="_blank" rel="noopener noreffer ">mistral-70b</a>/yi/qwen/&hellip; in the near future.</p>
</blockquote>]]></description></item></channel></rss>