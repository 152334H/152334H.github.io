<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>openai - Tag - 152334H</title><link>https://152334H.github.io/tags/openai/</link><description>openai - Tag - 152334H</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 05 Aug 2023 04:09:15 +0800</lastBuildDate><atom:link href="https://152334H.github.io/tags/openai/" rel="self" type="application/rss+xml"/><item><title>Non-determinism in GPT-4 is caused by Sparse MoE</title><link>https://152334H.github.io/blog/non-determinism-in-gpt-4/</link><pubDate>Sat, 05 Aug 2023 04:09:15 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/non-determinism-in-gpt-4/</guid><description><![CDATA[<p>It&rsquo;s <a href="https://twitter.com/BorisMPower/status/1608522707372740609" target="_blank" rel="noopener noreffer ">well-known</a> at this point that GPT-4/GPT-3.5-turbo is non-deterministic, even at <code>temperature=0.0</code>. This is an odd behavior if you&rsquo;re used to dense decoder-only models, where temp=0 should imply <a href="https://nn.labml.ai/sampling/greedy.html" target="_blank" rel="noopener noreffer ">greedy sampling</a> which should imply full determinism, because the logits for the next token should be a pure function of the input sequence &amp; the model weights.</p>]]></description></item></channel></rss>