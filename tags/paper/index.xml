<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>paper - Tag - 152334H</title><link>https://152334H.github.io/tags/paper/</link><description>paper - Tag - 152334H</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 30 Jul 2024 00:00:00 +0800</lastBuildDate><atom:link href="https://152334H.github.io/tags/paper/" rel="self" type="application/rss+xml"/><item><title>Calculating the Cost of a Google Deepmind Paper</title><link>https://152334H.github.io/blog/scaling-exponents/</link><pubDate>Tue, 30 Jul 2024 00:00:00 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/scaling-exponents/</guid><description><![CDATA[<p>Recently, GDM released a great paper titled, <a href="https://arxiv.org/pdf/2407.05872" target="_blank" rel="noopener noreffer "><em>Scaling Exponents Across Parameterizations and Optimizers</em></a>, in which they conduct over 10,000 LLM training runs to obtain optimal hyperparameters under different regimes.</p>
<p>After reading it (it was great), I wanted to test my understanding of the paper by tallying up all experiments conducted within, calculating <strong>the total compute cost it would take to replicate the paper</strong>.</p>]]></description></item></channel></rss>