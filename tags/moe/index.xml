<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>moe - Tag - 152334H</title><link>https://152334H.github.io/tags/moe/</link><description>moe - Tag - 152334H</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 09 Aug 2023 05:15:14 +0800</lastBuildDate><atom:link href="https://152334H.github.io/tags/moe/" rel="self" type="application/rss+xml"/><item><title>Knowing Enough About MoE to Explain Dropped Tokens in GPT-4</title><link>https://152334H.github.io/blog/knowing-enough-about-moe/</link><pubDate>Wed, 09 Aug 2023 05:15:14 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/knowing-enough-about-moe/</guid><description><![CDATA[<p>In a previous blogpost, I made a <a href="/blog/non-determinism-in-gpt-4/" rel="">simple observation</a> about GPT-4 from a <a href="https://arxiv.org/pdf/2308.00951.pdf" target="_blank" rel="noopener noreffer ">paper</a> I had incidentally read on a whim. After finishing the post, I realised I didn&rsquo;t actually ever figure out how <em>token dropping</em> could occur; only learning a black-box rule that it could occur in batched MoE inference for <em>reasons</em>.</p>
<p>This post is here to fix that &ndash; to collect enough info from important MoE papers (and alleged GPT-4 leaks) to explain the full mechanism of token dropping.</p>]]></description></item><item><title>Non-determinism in GPT-4 is caused by Sparse MoE</title><link>https://152334H.github.io/blog/non-determinism-in-gpt-4/</link><pubDate>Sat, 05 Aug 2023 04:09:15 +0800</pubDate><author>152334H</author><guid>https://152334H.github.io/blog/non-determinism-in-gpt-4/</guid><description><![CDATA[<p>It&rsquo;s <a href="https://twitter.com/BorisMPower/status/1608522707372740609" target="_blank" rel="noopener noreffer ">well-known</a> at this point that GPT-4/GPT-3.5-turbo is non-deterministic, even at <code>temperature=0.0</code>. This is an odd behavior if you&rsquo;re used to dense decoder-only models, where temp=0 should imply <a href="https://nn.labml.ai/sampling/greedy.html" target="_blank" rel="noopener noreffer ">greedy sampling</a> which should imply full determinism, because the logits for the next token should be a pure function of the input sequence &amp; the model weights.</p>]]></description></item></channel></rss>