<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Calculating the Cost of a Google Deepmind Paper - 152334H</title><meta name=Description content="152334H Personal Blog"><meta property="og:title" content="Calculating the Cost of a Google Deepmind Paper"><meta property="og:description" content="Recently, GDM released a great paper titled, Scaling Exponents Across Parameterizations and Optimizers, in which they conduct over 10,000 LLM training runs to obtain optimal hyperparameters under different regimes.
After reading it (it was great), I wanted to test my understanding of the paper by tallying up all experiments conducted within, calculating the total compute cost it would take to replicate the paper."><meta property="og:type" content="article"><meta property="og:url" content="https://152334H.github.io/blog/scaling-exponents/"><meta property="og:image" content="https://152334H.github.io/undefined.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-30T00:00:00+08:00"><meta property="article:modified_time" content="2024-07-30T22:19:53+08:00"><meta property="og:site_name" content="152334H"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://152334H.github.io/undefined.png"><meta name=twitter:title content="Calculating the Cost of a Google Deepmind Paper"><meta name=twitter:description content="Recently, GDM released a great paper titled, Scaling Exponents Across Parameterizations and Optimizers, in which they conduct over 10,000 LLM training runs to obtain optimal hyperparameters under different regimes.
After reading it (it was great), I wanted to test my understanding of the paper by tallying up all experiments conducted within, calculating the total compute cost it would take to replicate the paper."><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://152334H.github.io/blog/scaling-exponents/><link rel=prev href=https://152334H.github.io/blog/deepseek-0/><link rel=next href=https://152334H.github.io/blog/time-to-think/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Calculating the Cost of a Google Deepmind Paper","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/152334H.github.io\/blog\/scaling-exponents\/"},"genre":"posts","keywords":"machine learning, paper, llm, series","wordcount":3782,"url":"https:\/\/152334H.github.io\/blog\/scaling-exponents\/","datePublished":"2024-07-30T00:00:00+08:00","dateModified":"2024-07-30T22:19:53+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"152334H"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>All Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/><b>About </b></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>All Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title><b>About</b></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Calculating the Cost of a Google Deepmind Paper</h1><h2 class=single-subtitle>How to burn US$10,000,000 on an arXiv preprint</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://152334H.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>152334H</a></span>&nbsp;<span class=post-category>included in <a href=/categories/tech/><i class="far fa-folder fa-fw" aria-hidden=true></i>tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime="July 30, 2024">July 30, 2024</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;3782 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;18 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#headline-result>Headline result</a></li><li><a href=#a-summary-of-all-experiments-tried>A summary of all experiments tried</a></li><li><a href=#transformer-information>Transformer information</a><ul><li><a href=#flops-per-token>FLOPs per token</a></li></ul></li><li><a href=#subproblem-alignment-experiments>Subproblem: Alignment experiments</a></li><li><a href=#subproblem-table-e1-experiments>Subproblem: Table E1 experiments</a></li><li><a href=#estimating-lr-sweep-damage>Estimating LR sweep damage</a><ul><li><a href=#an-arbitrary-decision>An arbitrary decision</a></li></ul></li><li><a href=#main-problem-epslion>Main problem: Epslion</a><ul><li><a href=#optimal-eps-runs>Optimal eps runs</a></li><li><a href=#epslion-heatmaps>Epslion Heatmaps</a><ul><li><a href=#these-squares-are-worth-us32-million>These squares are worth US$3.2 Million</a></li></ul></li></ul></li><li><a href=#main-problem-lr-sweep-strategies>Main problem: LR Sweep Strategies</a><ul><li><a href=#beta-only-experiments>$\beta$-only experiments</a></li><li><a href=#gamma-experiments>$\gamma$ experiments</a></li></ul></li><li><a href=#extras>Extras</a><ul><li><a href=#weight-decay>Weight Decay</a></li><li><a href=#adafactor>Adafactor</a></li><li><a href=#compute-optimal>Compute Optimal</a></li></ul></li><li><a href=#code-summary>Code summary</a></li></ul></nav></div></div><div class=content id=content><p>Recently, GDM released a great paper titled, <a href=https://arxiv.org/pdf/2407.05872 target=_blank rel="noopener noreffer"><em>Scaling Exponents Across Parameterizations and Optimizers</em></a>, in which they conduct over 10,000 LLM training runs to obtain optimal hyperparameters under different regimes.</p><p>After reading it (it was great), I wanted to test my understanding of the paper by tallying up all experiments conducted within, calculating <strong>the total compute cost it would take to replicate the paper</strong>.</p><h2 id=headline-result>Headline result</h2><table><thead><tr><th>Subset</th><th>Sources of uncertainty</th><th>FLOPs</th><th>Costs @ $3/H100/hr</th></tr></thead><tbody><tr><td>Alignment</td><td>N/A</td><td>3.7e20</td><td>$888</td></tr><tr><td>LR variants (+default)</td><td>LR-sweeps, bayes search</td><td>7.99e23</td><td>$1.90M</td></tr><tr><td>LR variants (+optimal)</td><td>LR-sweeps</td><td>1.35e24</td><td>$3.22M</td></tr><tr><td>Epslion (Heatmaps)</td><td>LR-sweeps, $D$</td><td>1.34e24</td><td>$3.19M</td></tr><tr><td>Epslion (Full Sweeps)</td><td>LR-sweeps</td><td>7.99e23</td><td>$1.90M</td></tr><tr><td>Weight Decay</td><td>LR-sweeps</td><td>1.33e23</td><td>$317K</td></tr><tr><td>Adafactor vs Adam+PS</td><td>LR-sweeps, $D$</td><td>7.92e22</td><td>$188.5K</td></tr><tr><td>Compute Optimals</td><td>LR-sweeps, $D$</td><td>7.52e23</td><td>$1.79M</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td><strong>Total</strong></td><td>too much</td><td><strong>5.42e24</strong></td><td><strong>$12.9M</strong></td></tr></tbody></table><p><strong>Any corrections on the numbers here will be appreciated.</strong></p><p>Although I have made significant efforts to vet these claims, <em>if I have made significant mistakes in mathematics, these results could be off by magnitudes.</em></p><div class="details admonition question"><div class="details-summary admonition-title"><i class="icon fas fa-question-circle fa-fw" aria-hidden=true></i>Sidenote: What's an H100 worth?<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><p>Although it&rsquo;s never stated, all experiments in the paper were almost certainly conducted with TPUs (because it&rsquo;s from Google Deepmind). Furthermore, as there is no mention of int8 usage in their paper, it is most likely that all experiments were conducted with bfloat16 compute precision, per the <a href=https://github.com/google-deepmind/nanodo/blob/10aefdeed40a63293daf112b91a5538cd24fa3a4/nanodo/configs/default.py#L41 target=_blank rel="noopener noreffer">nanodo default</a>.</p><p>However, as a GPU user, I prefer to calculate compute in terms of H100 hours. Some basic facts:</p><ul><li>The H100-SXM is <a href=https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet target=_blank rel="noopener noreffer">reported</a> as having 989.40TFLOP/s of 16-bit tensor core operations.<ul><li>Also, 66.9TFLOP/s fp32 non-tensor, but I won&rsquo;t consider non-tensor operations (such as softmax or hadamard products) in my analysis.</li></ul></li><li>Recent pytorch <a href=https://pytorch.org/blog/maximizing-training/ target=_blank rel="noopener noreffer">blogs</a> and <a href=https://github.com/pytorch/torchtitan/pull/165 target=_blank rel="noopener noreffer">torchtitan</a> both report single-node FSDP&rsquo;d bf16 H100 MFU for reasonably mid sized models at (optimistically) 40%.<ul><li>the smaller models ($D&lt;1024$) in the paper are unlikely to have MFU that high.</li><li>Although this is not hard to push higher with some manual tuning, the time spent tuning performance & engineering required to heuristically adjust for efficiency depending on setting is unlikely to be worth it.</li></ul></li><li>The cost of a H100 node (at the time of writing) is $3.5/hr/gpu on <a href=https://cloud.lambdalabs.com/instances target=_blank rel="noopener noreffer">lambdalabs</a>, $2.85/hr/gpu from <a href=https://sfcompute.com/ target=_blank rel="noopener noreffer">sfcompute</a>, and ballpark $2/hr/gpu if you get a <a href=https://gpulist.ai/ target=_blank rel="noopener noreffer">long term bulk contract</a>.</li></ul><p>If we pessimistically estimate the true average tensor FLOP/s provided by a H100 GPU on an average run as 3.5e14 (aka slightly above 35% MFU), and the cost of a H100 GPU as $3/hr, we get:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>cost_of_run</span><span class=p>(</span><span class=n>flops</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>pergpu_flops</span><span class=o>=</span><span class=mf>3.5e14</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>gpu_hours</span> <span class=o>=</span> <span class=n>flops</span> <span class=o>/</span> <span class=mi>3600</span> <span class=o>/</span> <span class=n>pergpu_flops</span>
</span></span><span class=line><span class=cl>  <span class=n>rental_cost</span> <span class=o>=</span> <span class=mi>3</span> <span class=o>*</span> <span class=n>gpu_hours</span>
</span></span><span class=line><span class=cl>  <span class=n>single_node_duration</span> <span class=o>=</span> <span class=n>gpu_hours</span> <span class=o>/</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>rental_cost</span><span class=p>,</span> <span class=n>single_node_duration</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>These numbers are fungible</strong> and you can choose to mentally halve (or double) them if you find it appropriate.</p></div></div></div><hr><h2 id=a-summary-of-all-experiments-tried>A summary of all experiments tried</h2><figure><img src=Pasted%20image%2020240722025146.png></figure><p>There are a few different types of experiments done in the paper:</p><div class="details admonition info open"><div class="details-summary admonition-title"><i class="icon fas fa-info-circle fa-fw" aria-hidden=true></i>Experiment types<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><ul><li><strong>Alignment experiments</strong>, which use a single global close-to-optimal LR, while varying<ul><li>$D \in {1024, 2048, 4096}$</li><li>4x paramterizations</li><li>3x optimizers (Adam, SGD+momentum, Adafactor)</li></ul></li><li><strong>Learning rate</strong> experiments, which vary:<ul><li><p>3x optimizers (Adam, SGD+momentum, Adam+PS)</p></li><li><p>4x paramterizations</p></li><li><p><strong>14x model widths</strong> $D \in [128, 16384]$. but this is really best described as scaling numheads $H \in {1,2,4,6,8,12,16,20,24,32,48,64,96,128}$</p></li><li><p>Global LR vs Per-layer Beta LR vs Per-layer $\beta$ Gamma LR + Per-layer $\beta\ \gamma$ No align LR</p><ul><li>The $\gamma$ experiements are particularly complex to calculate, see <a href=#Problems rel>point 3</a></li></ul></li><li><p>LR by an <strong>indeterminate range</strong> &ndash; they sweep in intervals of $2^{0.25}\text{ or }2^{0.5}$ and terminate rightwards when</p><ol><li>the LR leads to NaNs OR</li><li>the eval loss for a given LR $\mathcal{L}^\eta \gt 1.2\times \text{argmin}_\eta(\mathcal{L^\eta})$</li></ol><p>i.e. the first (larger than optimal) LR to show either of those conditions is <strong>not plotted</strong>, and the LR $\sqrt{2}$ or $\surd\surd2$ is.</p><p>&mldr;or at least, that is what the paper says is supposed to be the case. <a href=#Problems rel>I explain my contentions later</a>.</p></li></ul></li><li><strong>Adam Epslion</strong> experiments, which vary<ul><li>over 4x parameterizations,<ul><li><em>at least</em> $D\in {3072, 4096, 6144, 8192, 12288, 16384}$ over Adam, where<ul><li><em>at least</em> 6x eps is tried</li><li><em>at least</em> constant vs per-layer $\epsilon$ is compared.</li><li><em>at least</em> 13x LR is tried. Appendix F: &ldquo;<a href="https://arxiv.org/pdf/2407.05872#page=43" target=_blank rel="noopener noreffer">learning rate sweep at each model dim for each value of epsilon or base epsilon</a>&rdquo;</li></ul></li><li>according to Appendix J/K, over <strong>all 14 model dims</strong>,<ul><li>For Adam, 4x (base eps, small const, good per-layer, atan2)<ul><li>technically, we double-count base EPS from the LR experiments, but we also neglect the extra no-align per-layer eps experiments, so this cancels out</li></ul></li><li>For Adam+PS, 2x (base eps, good per-layer)<ul><li>the double-neglect accounting argument applies here too</li></ul></li></ul></li></ul></li></ul></li><li>extra <strong>weight decay</strong> experiments<ul><li>static: adam, per-layer, full alignment, decoupled 1e-4</li><li>4x parameterizations</li><li>LR experiment-like sweep across <strong>all 14 model widths</strong></li></ul></li><li>extra <strong>adafactor</strong> experiments<ul><li>2x optim (Adafactor vs adam+ps)</li><li>2x setting (globalLR+default vs perlayer+optimal)</li><li>4x parameterizations</li><li>LR experiment-like sweep across <strong>only 11x</strong> model widths up to $H=48$ due to FSDP.<ul><li>actually <a href="https://arxiv.org/pdf/2407.05872#page=52" target=_blank rel="noopener noreffer">implemented as 12x</a> but <a href="https://arxiv.org/pdf/2407.05872#page=47" target=_blank rel="noopener noreffer">final results are 11x</a> and I follow the latter.</li></ul></li></ul></li><li>extra fixed step vs <strong>compute optimal</strong><ul><li>the 50k fixed step experiments are <strong>not the same</strong> as any of the above; they use &ldquo;default constant learning rate multipliers&rdquo; and have different power laws.</li><li>3x optim (SGD+moment, adam, adafactor)</li><li>4x parameterizations</li><li>LR experiment-like sweep across model width && LR.<ul><li>width <strong>only goes up to 11x</strong>, last 3 are missing on Compute Optimal.</li></ul></li><li>compute-optimal experiments use 20x tokens of non-embedding P as a heuristic.</li></ul></li></ul></div></div></div><p>However, there are many problems with the experimental summary as given above.</p><div class="details admonition warning"><div class="details-summary admonition-title"><i class="icon fas fa-exclamation-triangle fa-fw" aria-hidden=true></i>Problems<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><ol><li><p>It is <strong>not clear whether they re-executed</strong> the per-layerLR experiments for the two edge cases where per-layer constants lead to identical behavior to globalLR (where $c_1 = c_l = c_{L+1}$):</p><ul><li>muP + SGD + full alignment, or</li><li>Adafactor + any parameterization + no alignment</li></ul><p>My expectation is that <strong>their experiments were repeated</strong>, because if you look at Table E1, you&rsquo;ll see that the muP+SGD+full columns actually have a single diverging value (presumably caused by precision differences):</p><figure><img src=VXjZe27k.jpeg></figure><p><strong>However</strong>, I was also given (private) notice that <strong>in some cases, the experiments with theoretically equivalent settings were merely executed once</strong>, with the eval losses copied twice. This makes the true extent of compute unknowable from the paper.</p></li><li><p>The LR experiments have indeterminate bounds, so I can&rsquo;t directly figure out how many experiments were executed.</p><p>You can&rsquo;t &ldquo;just read the graphs&rdquo; to figure out what the range of LRs used are either; they cut off the y/x axis:</p><figure><img src=Pasted%20image%2020240722134014.png></figure><p>Frankly, it doesn&rsquo;t even look like the steps here are guaranteed to be split in intervals of $2^{0.25}\text{ or }2^{0.5}$.</p><figure><img src=Pasted%20image%2020240730045534.png></figure><p>After further inspection, it looks an awful lot like the runs have <strong>arbitrary LR ranges even for the same $D$, optim, parameterization, and alignment</strong>. Or I just don&rsquo;t understand the selection process (what are the unshaded shapes?).</p></li><li><p>In <a href="https://arxiv.org/pdf/2407.05872#page=34" target=_blank rel="noopener noreffer">C.4</a>., they state:</p><blockquote><p>When tuning the per-layer constant multiplicative factors defined in Section 4.2, we use <a href=https://github.com/google/vizier target=_blank rel="noopener noreffer">vizier</a> to perform 3D hparam search for $(γ_1, γ_h, γ_{L+1})$ at $b = 1024$. Recall that we define the learning rate in layer $l$ as $η_l = β_n·γ_l·\frac{n}{b}^{−cl}$ and sweep one dimension at all model sizes to determine $β_n$, so these values of $(γ_1, γ_h, γ_{L+1})$ define two ratios where any common factor can be absorbed by $β_n$.</p></blockquote><p>To be clear, that last segment means: &ldquo;you can divide $(γ_1, γ_h, γ_{L+1})$ by any of the 3 values to obtain some $(\gamma_x, \gamma_y, 1)$ tuple, the sweep will bring $\beta_n$ back to the correct value&rdquo;. And so they say:</p><blockquote><p>For each optimizer × parameterization, we run 800 trials with at most 100 trials in parallel with a range set to $[1\text{e−}2, 1e2]$ for each constant. If the optimal value for any of the constants is at or near the edge of the range after this first search, we extend the range of the sweep for that constant to 0.01 and 100x the optimal value found in the original sweep and repeat the same tuning procedure.</p></blockquote><p>Upside: this gives 800 experiments as a lower bound for the $\gamma$ experiments.
Downside: We otherwise have no plotted information about the 3D experiments that were conducted. The actual plotted graphs just show final eval loss against base LR, under the assumption that the $b=1024$ base line on the Optimal Constants graphs actually hide the extra work done to sweep $\gamma$ values.</p></li><li><p>It is deeply unclear to me what is actually implemented for the fixed-step vs compute optimal runs. If we look at the <a href="https://arxiv.org/pdf/2407.05872#page=51" target=_blank rel="noopener noreffer">50k steps graph</a>:</p><figure><img src=Pasted%20image%2020240730044030.png></figure><p>It looks <em>extremely similar</em>, but <strong>not identical</strong> to the <a href="https://arxiv.org/pdf/2407.05872#page=56" target=_blank rel="noopener noreffer">original Adam+GlobalLR+default graphs</a>:</p><figure><img src=Pasted%20image%2020240730044047.png></figure><p>I have no idea what the differences are supposed to be here. However, in the interest of sticking with the paper&rsquo;s behaviour, I attempt to include the compute used for these psuedo-repeated experiments.</p></li></ol></div></div></div><p>For each of these issues, I do my best to pick an approximation that makes sense to me in the later sections.</p><hr><h2 id=transformer-information>Transformer information</h2><p>In Appendix C, the model is described as:</p><ul><li>decoder-only</li><li>no bias on weights (including layernorm, which only has learnable scale)</li><li>LPE, pre-LN, GeLU, no tied emb</li><li>T5 Sentencepiece 32k + 1BOS + 100extra, i.e. $V=32101$. This is never stated to be padded.</li><li>&ldquo;Training inputs are sequence-packed, while evaluation inputs are padded&rdquo;</li><li>$\text{batch size}=256$, $l_\text{seq}=512$, $L=8$, $D_\text{head}=128$</li><li>$D_\text{head}*H = D$, $R_\text{ffn} = 4$.</li></ul><p>with some extra details for later:</p><ul><li>no dropout</li><li>mostly FSDP</li><li>$P \approx L12D^2 + 2VD$ (this excludes the layernorm params ($2LD$) and the LPE ($Vl_\text{seq}$))</li><li>&ldquo;The compute optimal experiments include models up to $H = 32$ or $H = 48$, and the fixed (50,000) step experiments include models up to $H = 128$.&rdquo;</li></ul><h3 id=flops-per-token>FLOPs per token</h3><p>To start, we want to find $M$, the number of FLOPs required per token for a training run.</p><div class="details admonition info"><div class="details-summary admonition-title"><i class="icon fas fa-info-circle fa-fw" aria-hidden=true></i>Basic transformer math<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><p>As a reminder for any noam-like transformer, the tensor FLOPs required per token $M$ is approx:</p><p>$$V - \text{vocab size}$$
$$D - \text{hidden dim}$$
$$L - \text{xf layer count}$$</p><p>$$R_{\text{ffn}} - \text{[ffn dim : outer dim] ratio, assuming no GLU}$$
$$R_{kv} - \text{[num k or v heads : num att heads] ratio}$$
$$l_{seq} - \text{assumed average sequence length}$$</p><p>$$M = 12D^2L(1 + R_{kv} + R_{\text{ffn}}) + 6DL\cdot l_{seq} + 6DV$$</p><p>In particular, $6DL\cdot l_\text{seq}$ assumes a causal mask halves the computation required (I assume flash-attn does this)</p></div></div></div><p>The paper does not describe the usage of any GQA/MQA, so I assume $R_\text{kv} = 1$. This gives us</p><blockquote><p>$M=72D^2L + 6DLl_\text{seq} + 6DV = 6D(12DL + Ll_\text{seq} + V) = 6D(L(12D+l_\text{seq}) + V)$</p></blockquote><p>We have additional constants of $L=8$, $l_\text{seq} = 512$, and $V=32101$, so we write:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>M</span><span class=p>(</span><span class=n>d</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>L</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>l_seq</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>V</span><span class=o>=</span><span class=mi>32101</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>6</span><span class=o>*</span><span class=n>d</span> <span class=o>*</span> <span class=p>(</span><span class=n>L</span><span class=o>*</span><span class=p>(</span><span class=mi>12</span><span class=o>*</span><span class=n>d</span> <span class=o>+</span> <span class=n>l_seq</span><span class=p>)</span> <span class=o>+</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>TPE</span> <span class=o>=</span> <span class=mi>50000</span> <span class=o>*</span> <span class=mi>256</span> <span class=o>*</span> <span class=mi>512</span>
</span></span></code></pre></td></tr></table></div></div><p>For all experiments <em>except the compute-optimal series in Appendix I</em>, we also have a hardcoded number of $steps=50000$ and global $BS=256$, making the total number of tokens seen per experiment $TPE=6.5536\text{e}9$ by default.</p><hr><h2 id=subproblem-alignment-experiments>Subproblem: Alignment experiments</h2><p>I <em>assume</em> the alignment experiments got their optimal LRs from the later experiments, and didn&rsquo;t do their own sweeps, so that would make the cost simply,
$$
\sum_{d\in {1024,2048,4096}} 4\times\text{tokens per experiment}\times M(d)
$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>alignment</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=p>[</span><span class=mi>1024</span><span class=p>,</span><span class=mi>2048</span><span class=p>,</span><span class=mi>4096</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=c1># &gt;&gt;&gt; f&#39;{alignment():.3E}&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;3.733E+20&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># &gt;&gt;&gt; cost_of_run(alignment())[0]</span>
</span></span><span class=line><span class=cl><span class=c1># 888.81395400704</span>
</span></span></code></pre></td></tr></table></div></div><p>These experiments would take &lt;US$1k to execute.</p><h2 id=subproblem-table-e1-experiments>Subproblem: Table E1 experiments</h2><p><a href="https://arxiv.org/pdf/2407.05872#page=40" target=_blank rel="noopener noreffer">Table E1</a> has a neat collection of many of the runs done for obtaining the <em>best</em> eval losses under any given parameterization/optimizer/setting (some combination of global vs per-layer vs $\gamma$-optimal vs $\epsilon$-optimal).</p><p>This is an easier subproblem to tackle than the general issue of <em>all LR sweeps</em>, as the requirements are better known &ndash; though still not entirely determined, per the repetition ambiguity mentioned earlier. For that issue, I assume that all experiments were conducted, with no copied results, making the estimate here an upper bound.</p><p>We have the following schedule:</p><ul><li>$D\in {3072, 4096, 6144, 8192, 12288, 16384}$</li><li>4x parameterizations</li><li>3x optimizers, where<ul><li>SGD only receives 5 experimental settings</li><li>Adam & Adam+PS receives 7</li></ul></li></ul><p>$$
\sum_{d\in {3072,4096,6144,8192,12288,16384}} 4\times(5+7*2)\times\text{tokens per experiment}\times M(d)
$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>H</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>4</span><span class=p>,</span><span class=mi>6</span><span class=p>,</span><span class=mi>8</span><span class=p>,</span><span class=mi>12</span><span class=p>,</span><span class=mi>16</span><span class=p>,</span><span class=mi>20</span><span class=p>,</span><span class=mi>24</span><span class=p>,</span><span class=mi>32</span><span class=p>,</span><span class=mi>48</span><span class=p>,</span><span class=mi>64</span><span class=p>,</span><span class=mi>96</span><span class=p>,</span><span class=mi>128</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>D</span> <span class=o>=</span> <span class=p>[</span><span class=n>h</span> <span class=o>*</span> <span class=mi>128</span> <span class=k>for</span> <span class=n>h</span> <span class=ow>in</span> <span class=n>H</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>table_e1</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>sets_x_optims</span> <span class=o>=</span> <span class=mi>5</span> <span class=o>+</span> <span class=mi>7</span> <span class=o>+</span> <span class=mi>7</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>sets_x_optims</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>[</span><span class=o>-</span><span class=mi>6</span><span class=p>:])</span>
</span></span><span class=line><span class=cl><span class=c1># &gt;&gt;&gt; f&#39;{table_e1():.3E}&#39;;cost_of_run(table_e1())</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;1.634E+23&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># (388955.9991064986, 16206.499962770775)</span>
</span></span></code></pre></td></tr></table></div></div><p>These would&rsquo;ve taken slightly below $400k in H100 compute to execute. Reasonably speaking, this is within the bounds of SWE life savings / big academic budgets / TPU Research Cloud upper-class. Technically replicable, albeit not cheap.</p><p>But the bulk of the compute used in the paper comes from the LR sweeps, so we have to start working on that.</p><h2 id=estimating-lr-sweep-damage>Estimating LR sweep damage</h2><p>So, here&rsquo;s a a graph:
<a class=lightgallery href=/blog/scaling-exponents/Pasted%20image%2020240730050758.png title=/blog/scaling-exponents/Pasted%20image%2020240730050758.png data-thumbnail=/blog/scaling-exponents/Pasted%20image%2020240730050758.png><div style=text-align:center><img style=height:ZgotmplZpx;width:ZgotmplZpx class=lazyload src=/svg/loading.min.svg data-src=/blog/scaling-exponents/Pasted%20image%2020240730050758.png data-srcset="/blog/scaling-exponents/Pasted%20image%2020240730050758.png, /blog/scaling-exponents/Pasted%20image%2020240730050758.png 1.5x, /blog/scaling-exponents/Pasted%20image%2020240730050758.png 2x" data-sizes=auto alt=/blog/scaling-exponents/Pasted%20image%2020240730050758.png width=354 height=371></div></a>Here&rsquo;s another graph:
<a class=lightgallery href=/blog/scaling-exponents/Pasted%20image%2020240730050857.png title=/blog/scaling-exponents/Pasted%20image%2020240730050857.png data-thumbnail=/blog/scaling-exponents/Pasted%20image%2020240730050857.png><div style=text-align:center><img style=height:ZgotmplZpx;width:ZgotmplZpx class=lazyload src=/svg/loading.min.svg data-src=/blog/scaling-exponents/Pasted%20image%2020240730050857.png data-srcset="/blog/scaling-exponents/Pasted%20image%2020240730050857.png, /blog/scaling-exponents/Pasted%20image%2020240730050857.png 1.5x, /blog/scaling-exponents/Pasted%20image%2020240730050857.png 2x" data-sizes=auto alt=/blog/scaling-exponents/Pasted%20image%2020240730050857.png width=367 height=347></div></a>And here&rsquo;s a third one:
<a class=lightgallery href=/blog/scaling-exponents/Pasted%20image%2020240730050917.png title=/blog/scaling-exponents/Pasted%20image%2020240730050917.png data-thumbnail=/blog/scaling-exponents/Pasted%20image%2020240730050917.png><div style=text-align:center><img style=height:ZgotmplZpx;width:ZgotmplZpx class=lazyload src=/svg/loading.min.svg data-src=/blog/scaling-exponents/Pasted%20image%2020240730050917.png data-srcset="/blog/scaling-exponents/Pasted%20image%2020240730050917.png, /blog/scaling-exponents/Pasted%20image%2020240730050917.png 1.5x, /blog/scaling-exponents/Pasted%20image%2020240730050917.png 2x" data-sizes=auto alt=/blog/scaling-exponents/Pasted%20image%2020240730050917.png width=346 height=377></div></a>Guess what?</p><ol><li>There isn&rsquo;t a constant num. of LRs sweeped for a given $D$, or optim/parameterization/setting.<ul><li>Especially notable: number of runs seems inversely correlated with $D$; there are almost always less runs for the highest dim than the lowest.</li></ul></li><li>Neither is there an observable cutoff for when the runs stop &ndash; runs will spike up to 2x the optimal no problem.</li><li>You can&rsquo;t get the <em>exact</em> correct number of runs by graph-reading; in many cases the points are out-of-bounds.</li></ol><p>The consistencies I <em>do</em> spot are that:</p><ul><li>there is typically a &ldquo;starting LR&rdquo; (smallest base) for any given line.</li><li>the hollowed points are typically to the right &ndash; but sometimes left &ndash; of the optimal point.</li></ul><p>so <strong>I <em>think</em> the mechanism worked this way</strong>:</p><ol><li>start a sweep with a starting LR and some expected jumpsizes of $\sqrt{2}$ or $\sqrt{\surd 2}$.</li><li>terminate it by the 20% / NaN heuristic.</li><li>if the graph looks weird (optimal point somewhere odd), rerun to fill many $2^{0.25}$ intervals around the current optimal. These result in the plotted hollow points</li></ol><p>I have no means of confirming this as the experimental procedure, as the authors of the paper stopped replying to me.</p><h3 id=an-arbitrary-decision>An arbitrary decision</h3><p>Due to my desire to finish this blog post in a reasonable amount of time, I made the unprincipled decision of approximating the number of experiments-per-line in any given Eval Loss vs Base Learning Rate graph as <strong>15</strong>.</p><p>Why 15? By eyeballing, the range of runs-per-line for the highest $D=16384$ hovers around 10~15. Although the lines with smaller D tend to have far more points on average, the amount of compute spent per run scales by $O(D^2)$, so I think this is fair enough.</p><p>Feel free to suggest a more principled approach if you have one.</p><hr><h2 id=main-problem-epslion>Main problem: Epslion</h2><p>Much of the compute used up by the paper comes from <a href="https://arxiv.org/pdf/2407.05872#page=15" target=_blank rel="noopener noreffer">Section 4.3</a>, the Adam epslion experiments.</p><h3 id=optimal-eps-runs>Optimal eps runs</h3><p>Now that we have an estimate of LRs-per-line as 15, we can estimate the compute spent on the actual <a href="https://arxiv.org/pdf/2407.05872#page=44" target=_blank rel="noopener noreffer">Adam epslion varying graphs</a>:</p><p>$$
\sum_{d} 4*(2+4) \times \text{points per line}\times\text{tokens per experiment}\times M(d)
$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>PpL</span> <span class=o>=</span> <span class=mi>15</span> <span class=c1># unprincipled estimate</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>eps_variants</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>6</span> <span class=o>*</span> <span class=n>PpL</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>&gt;&gt;&gt; f&#39;{eps_variants():.3E}&#39;;cost_of_run(eps_variants())
</span></span></span><span class=line><span class=cl><span class=s1>&#39;7.988E+23&#39;
</span></span></span><span class=line><span class=cl><span class=s1>(1902022.3291813303, 79250.93038255542)
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span></code></pre></td></tr></table></div></div><p>Simple enough, right? Ignoring the ~$2M bill.</p><h3 id=epslion-heatmaps>Epslion Heatmaps</h3><p>There are two ways you could approach the expected sweep range for this problem:</p><ol><li>assume the LR experiment sweep code was reused. All 14x $D$, LR swept by arcane unknown ruleset.</li><li>Limit to the graphs. Only the last 6 values of $D$ were shown &ndash; assume only those were used. Plus, if we look at <a href="https://arxiv.org/pdf/2407.05872#page=16" target=_blank rel="noopener noreffer">Figure 6</a>:<figure><img src=Pasted%20image%2020240730044906.png></figure>Notice that the range of evaluated learning rates actually seems constant here, unlike in the normal Eval Loss vs Base LR plots.</li></ol><p>I&rsquo;m picking the latter because it&rsquo;s simpler. Would be happy to be shown evidence that this is wrong.</p><p>$$ \sum_{d\in {3072,4096,6144,8192,12288,16384}} 4\cdot 2\cdot 6\cdot 13\times \text{tokens per experiment}\times M(d) $$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>eps_heatmaps</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=c1># eps-type * eps-val * parameterizations * LR range * ...</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>2</span> <span class=o>*</span> <span class=mi>6</span> <span class=o>*</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>13</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>[</span><span class=o>-</span><span class=mi>6</span><span class=p>:])</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>&gt;&gt;&gt; f&#39;{eps_heatmaps():.3E}&#39;;cost_of_run(eps_heatmaps())
</span></span></span><span class=line><span class=cl><span class=s1>&#39;1.341E+24&#39;
</span></span></span><span class=line><span class=cl><span class=s1>(3193533.466348094, 133063.89443117057)
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=these-squares-are-worth-us32-million>These squares are worth US$3.2 Million</h4><figure><img src=Pasted%20image%2020240730060102.png></figure><p>To be clear, this is supposed to be an <strong>underestimate</strong> of the budget required, because we model the average number of unique LRs used per heatmap square as a constant $13$ instead of the (typically higher) value used in variable LR sweeps.</p><h2 id=main-problem-lr-sweep-strategies>Main problem: LR Sweep Strategies</h2><p>The other meat of the paper is in Section 4.2, the $\text{optimizer}\times\text{parameterization}\times D\times\text{LR setting}\times\text{alignment}\times\text{LR Sweeps}$ experiments.</p><h3 id=beta-only-experiments>$\beta$-only experiments</h3><p>&ldquo;$\beta$&rdquo; refers to the empirically obtained base LR constant under the equation $\eta_l = \beta_n\cdot\frac{n}{b}^{-c_l}$, also known as the <code>+default</code> experiments.</p><p>The paper sweeps this for 3x optimizers, 4x parameterizations, 14x widths, global vs per-layer $c_l$, and of course unknown LR sweep counts.</p><p>$$ \sum_{d} 3*4*2 \times \text{points per line}\times\text{tokens per experiment}\times M(d) $$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>beta_only</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>3</span><span class=o>*</span><span class=mi>4</span><span class=o>*</span><span class=mi>2</span><span class=o>*</span><span class=n>PpL</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 7.988E+23 (1902022.3291813303, 79250.93038255542)</span>
</span></span></code></pre></td></tr></table></div></div><p>Incidentally, this has an identical estimated cost to the <a href=#optimal-eps-runs rel>epslion variants</a>.</p><h3 id=gamma-experiments>$\gamma$ experiments</h3><p>So, two issues.</p><ol><li>These experiments are &ldquo;like&rdquo; the $\beta$-only experiments, but with 3x cases (GlobalLR, Perlayer-fullalign, Perlayer-nolign) instead of 2x (GlobalLR, Perlayer-fullalign).
$$ \sum_{d} 3*4*3 \times \text{points per line}\times\text{tokens per experiment}\times M(d) $$</li><li>Specifically for $d=1024=b$, we have <strong>at least</strong> 800 extra runs, due to the 3D hparam search for $(\gamma_1, \gamma_h, \gamma_{L+1})$.
$$ 3*4*3*800 \times\text{tokens per experiment}\times M(1024) $$</li></ol><p>We can combine those two as,
$$ 36\times\text{tokens per experiment}(800*M(1024) + \text{points per line}\sum_{d}\times M(d)) $$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>gamma_expts</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>36</span><span class=o>*</span><span class=n>TPE</span> <span class=o>*</span> <span class=p>(</span><span class=mi>800</span><span class=o>*</span><span class=n>M</span><span class=p>(</span><span class=mi>1024</span><span class=p>)</span> <span class=o>+</span> <span class=n>PpL</span><span class=o>*</span><span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># gamma_expts 	 1.354E+24 (3224397.534237257, 134349.8972598857)</span>
</span></span></code></pre></td></tr></table></div></div><p>This is, once again, exceedingly close to that of the Adam $\epslion$ heatmap experiments.</p><p>Sidenote: I may be understanding the per-layer aspect of the paper incorrectly; I expected the compute expenditure of this section to be larger.</p><h2 id=extras>Extras</h2><h3 id=weight-decay>Weight Decay</h3><p>The WD experiments are simple enough. We repeat 4x parameterizations && do a single base-LR sweep on all $D$</p><p>$$
\sum_{d} 4*(2+4) \times \text{points per line}\times\text{tokens per experiment}\times M(d)
$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>weight_decay</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>PpL</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>&gt;&gt;&gt; f&#39;{weight_decay():.3E}&#39;; cost_of_run(weight_decay())
</span></span></span><span class=line><span class=cl><span class=s1>&#39;1.331E+23&#39;
</span></span></span><span class=line><span class=cl><span class=s1>(317003.7215302217, 13208.488397092571)
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span></code></pre></td></tr></table></div></div><p>Incredibly cheap, I could afford that in some years.</p><h3 id=adafactor>Adafactor</h3><p>As a reminder, I only count the first 11 $D$, even though the report actually has 12 in one graph.</p><p>$$
\sum_{d\in D[:11]} 2 * 2* 4\times \text{points per line}\times\text{tokens per experiment}\times M(d)
$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>adafactor</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>2</span><span class=o>*</span><span class=mi>2</span><span class=o>*</span><span class=mi>4</span><span class=o>*</span><span class=n>PpL</span><span class=o>*</span><span class=n>TPE</span><span class=o>*</span><span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>[:</span><span class=mi>11</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>&gt;&gt;&gt; f&#39;{adafactor():.3E}&#39;; cost_of_run(adafactor())
</span></span></span><span class=line><span class=cl><span class=s1>&#39;7.918E+22&#39;
</span></span></span><span class=line><span class=cl><span class=s1>(188532.80765144504, 7855.533652143543)
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=compute-optimal>Compute Optimal</h3><p>The paper states that,</p><blockquote><p>The compute optimal experiments include models up to $H = 32$ or $H = 48$, and the fixed (50,000) step experiments include models up to $H = 128$.</p></blockquote><p>If you read the graphs in <a href="https://arxiv.org/pdf/2407.05872#page=50" target=_blank rel="noopener noreffer">Appendix I</a>, this is slightly wrong, because</p><ul><li><p>50k experiments go to $H=48$ on Adafactor, and $H=128$ otherwise</p></li><li><p>all compute optimal experiments go up to $H=32$ only.</p><p>Note that a 4B param run requires 80B tokens by chinchilla, and C4 is less than 200B tokens, so they couldn&rsquo;t have gone higher without changing the dataset.</p></li></ul><p>This is honestly a bit complex, so let&rsquo;s forgo the latex and just describe it in python:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>P</span><span class=p>(</span><span class=n>d</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>L</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>V</span><span class=o>=</span><span class=mi>32101</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>d</span> <span class=o>*</span> <span class=p>(</span><span class=mi>6</span><span class=o>*</span><span class=n>L</span><span class=o>*</span><span class=n>d</span> <span class=o>+</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_optimal</span><span class=p>():</span>
</span></span><span class=line><span class=cl>  <span class=n>indices_50k</span> <span class=o>=</span> <span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>14</span><span class=p>,</span> <span class=mi>12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>4</span><span class=o>*</span><span class=n>PpL</span><span class=o>*</span><span class=nb>sum</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span> <span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>[:</span><span class=n>i</span><span class=p>]</span> <span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>indices_50k</span><span class=p>),</span>
</span></span><span class=line><span class=cl>	<span class=mi>20</span>  <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>P</span><span class=p>(</span><span class=n>d</span><span class=p>)</span><span class=o>*</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>[:</span><span class=mi>11</span><span class=p>])</span> <span class=o>*</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=p>])</span>
</span></span><span class=line><span class=cl><span class=c1># compute_optim 	 7.518E+23 (1790104.1799513847, 74587.67416464102)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=code-summary>Code summary</h2><p>Here is the full script to get the estimates I created:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>TPE</span> <span class=o>=</span> <span class=mi>50000</span> <span class=o>*</span> <span class=mi>256</span> <span class=o>*</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=n>H</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>4</span><span class=p>,</span><span class=mi>6</span><span class=p>,</span><span class=mi>8</span><span class=p>,</span><span class=mi>12</span><span class=p>,</span><span class=mi>16</span><span class=p>,</span><span class=mi>20</span><span class=p>,</span><span class=mi>24</span><span class=p>,</span><span class=mi>32</span><span class=p>,</span><span class=mi>48</span><span class=p>,</span><span class=mi>64</span><span class=p>,</span><span class=mi>96</span><span class=p>,</span><span class=mi>128</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>D</span> <span class=o>=</span> <span class=p>[</span><span class=n>h</span> <span class=o>*</span> <span class=mi>128</span> <span class=k>for</span> <span class=n>h</span> <span class=ow>in</span> <span class=n>H</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>PpL</span> <span class=o>=</span> <span class=mi>15</span> <span class=c1># unprincipled estimate</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>cost_of_run</span><span class=p>(</span><span class=n>flops</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>pergpu_flops</span><span class=o>=</span><span class=mf>3.5e14</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>gpu_hours</span> <span class=o>=</span> <span class=n>flops</span> <span class=o>/</span> <span class=mi>3600</span> <span class=o>/</span> <span class=n>pergpu_flops</span>
</span></span><span class=line><span class=cl>  <span class=n>rental_cost</span> <span class=o>=</span> <span class=mi>3</span> <span class=o>*</span> <span class=n>gpu_hours</span>
</span></span><span class=line><span class=cl>  <span class=n>single_node_duration</span> <span class=o>=</span> <span class=n>gpu_hours</span> <span class=o>/</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>rental_cost</span><span class=p>,</span> <span class=n>single_node_duration</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>M</span><span class=p>(</span><span class=n>d</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>L</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>l_seq</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>V</span><span class=o>=</span><span class=mi>32101</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>6</span><span class=o>*</span><span class=n>d</span> <span class=o>*</span> <span class=p>(</span><span class=n>L</span><span class=o>*</span><span class=p>(</span><span class=mi>12</span><span class=o>*</span><span class=n>d</span> <span class=o>+</span> <span class=n>l_seq</span><span class=p>)</span> <span class=o>+</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>P</span><span class=p>(</span><span class=n>d</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>L</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>V</span><span class=o>=</span><span class=mi>32101</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>d</span> <span class=o>*</span> <span class=p>(</span><span class=mi>6</span><span class=o>*</span><span class=n>L</span><span class=o>*</span><span class=n>d</span> <span class=o>+</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>alignment</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=p>[</span><span class=mi>1024</span><span class=p>,</span><span class=mi>2048</span><span class=p>,</span><span class=mi>4096</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>table_e1</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>sets_x_optims</span> <span class=o>=</span> <span class=mi>5</span> <span class=o>+</span> <span class=mi>7</span> <span class=o>+</span> <span class=mi>7</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>sets_x_optims</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>[</span><span class=o>-</span><span class=mi>6</span><span class=p>:])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>eps_variants</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>6</span> <span class=o>*</span> <span class=n>PpL</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>eps_heatmaps</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>2</span> <span class=o>*</span> <span class=mi>6</span> <span class=o>*</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>13</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>[</span><span class=o>-</span><span class=mi>6</span><span class=p>:])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>beta_only</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>3</span><span class=o>*</span><span class=mi>4</span><span class=o>*</span><span class=mi>2</span><span class=o>*</span><span class=n>PpL</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>gamma_expts</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>36</span><span class=o>*</span><span class=n>TPE</span> <span class=o>*</span> <span class=p>(</span><span class=mi>800</span><span class=o>*</span><span class=n>M</span><span class=p>(</span><span class=mi>1024</span><span class=p>)</span> <span class=o>+</span> <span class=n>PpL</span><span class=o>*</span><span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>weight_decay</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>PpL</span> <span class=o>*</span> <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>adafactor</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>2</span><span class=o>*</span><span class=mi>2</span><span class=o>*</span><span class=mi>4</span><span class=o>*</span><span class=n>PpL</span><span class=o>*</span><span class=n>TPE</span><span class=o>*</span><span class=nb>sum</span><span class=p>(</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>[:</span><span class=mi>11</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_optim</span><span class=p>():</span>
</span></span><span class=line><span class=cl>  <span class=n>indices_50k</span> <span class=o>=</span> <span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>14</span><span class=p>,</span> <span class=mi>12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>4</span><span class=o>*</span><span class=n>PpL</span><span class=o>*</span><span class=nb>sum</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>TPE</span> <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span> <span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>[:</span><span class=n>i</span><span class=p>]</span> <span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>indices_50k</span><span class=p>),</span>
</span></span><span class=line><span class=cl>	<span class=mi>20</span>  <span class=o>*</span> <span class=nb>sum</span><span class=p>(</span><span class=n>P</span><span class=p>(</span><span class=n>d</span><span class=p>)</span><span class=o>*</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>[:</span><span class=mi>11</span><span class=p>])</span> <span class=o>*</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>total_flops</span><span class=p>,</span> <span class=n>total_price</span><span class=p>,</span> <span class=n>total_hours</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>f</span> <span class=ow>in</span> <span class=p>(</span><span class=n>alignment</span><span class=p>,</span><span class=n>table_e1</span><span class=p>,</span><span class=n>eps_variants</span><span class=p>,</span><span class=n>eps_heatmaps</span><span class=p>,</span><span class=n>beta_only</span><span class=p>,</span><span class=n>gamma_expts</span><span class=p>,</span><span class=n>weight_decay</span><span class=p>,</span><span class=n>adafactor</span><span class=p>,</span><span class=n>compute_optim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>flops</span> <span class=o>=</span> <span class=n>f</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>costs</span> <span class=o>=</span> <span class=n>cost_of_run</span><span class=p>(</span><span class=n>flops</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>f</span><span class=o>.</span><span class=vm>__name__</span><span class=si>:</span><span class=s1>15</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>flops</span><span class=si>:</span><span class=s1>.3E</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>,</span><span class=n>costs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>total_flops</span> <span class=o>+=</span> <span class=n>flops</span><span class=p>;</span> <span class=n>total_price</span> <span class=o>+=</span> <span class=n>costs</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span> <span class=n>total_hours</span> <span class=o>+=</span> <span class=n>costs</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>total_flops</span><span class=si>=:</span><span class=s1>.3E</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;rental price: US$</span><span class=si>{</span><span class=n>total_price</span><span class=o>/</span><span class=mf>1e6</span><span class=si>:</span><span class=s1>.3</span><span class=si>}</span><span class=s1>M&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;h100 node months required: </span><span class=si>{</span><span class=n>total_hours</span><span class=o>/</span><span class=mi>24</span><span class=o>/</span><span class=mi>30</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;(sanity check) </span><span class=si>{</span><span class=n>D</span><span class=si>=}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;(sanity check) model sizes:&#39;</span><span class=p>,</span> <span class=p>[</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>P</span><span class=p>(</span><span class=n>d</span><span class=p>)</span><span class=o>/</span><span class=mf>1e9</span><span class=si>:</span><span class=s1>.3</span><span class=si>}</span><span class=s1>B&#39;</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;(sanity check) M/6P:&#39;</span><span class=p>,</span> <span class=p>[</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=mi>100</span><span class=o>*</span><span class=n>M</span><span class=p>(</span><span class=n>d</span><span class=p>)</span><span class=o>/</span><span class=n>P</span><span class=p>(</span><span class=n>d</span><span class=p>)</span><span class=o>/</span><span class=mi>6</span><span class=si>:</span><span class=s1>.3</span><span class=si>}</span><span class=s1>%&#39;</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>D</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>This gives the following:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>alignment       3.733E+20 (888.81395400704, 37.033914750293334)
</span></span><span class=line><span class=cl>table_e1        1.634E+23 (388955.9991064986, 16206.499962770775)
</span></span><span class=line><span class=cl>eps_variants    7.988E+23 (1902022.3291813303, 79250.93038255542)
</span></span><span class=line><span class=cl>eps_heatmaps    1.341E+24 (3193533.466348094, 133063.89443117057)
</span></span><span class=line><span class=cl>beta_only       7.988E+23 (1902022.3291813303, 79250.93038255542)
</span></span><span class=line><span class=cl>gamma_expts     1.354E+24 (3224397.534237257, 134349.8972598857)
</span></span><span class=line><span class=cl>weight_decay    1.331E+23 (317003.7215302217, 13208.488397092571)
</span></span><span class=line><span class=cl>adafactor       7.918E+22 (188532.80765144504, 7855.533652143543)
</span></span><span class=line><span class=cl>compute_optim   7.518E+23 (1790104.1799513847, 74587.67416464102)
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>total_flops=5.421E+24
</span></span><span class=line><span class=cl>rental price: US$12.9M
</span></span><span class=line><span class=cl>h100 node months required: 746.9595590938408
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>(sanity check) D=[128, 256, 512, 768, 1024, 1536, 2048, 2560, 3072, 4096, 6144, 8192, 12288, 16384]
</span></span><span class=line><span class=cl>(sanity check) model sizes: [&#39;0.00979B&#39;, &#39;0.0227B&#39;, &#39;0.058B&#39;, &#39;0.106B&#39;, &#39;0.166B&#39;, &#39;0.325B&#39;, &#39;0.534B&#39;, &#39;0.794B&#39;, &#39;1.1B&#39;, &#39;1.87B&#39;, &#39;4.02B&#39;, &#39;6.97B&#39;, &#39;15.3B&#39;, &#39;26.8B&#39;]
</span></span><span class=line><span class=cl>(sanity check) M/6P: [&#39;63.4%&#39;, &#39;68.5%&#39;, &#39;75.3%&#39;, &#39;79.7%&#39;, &#39;82.8%&#39;, &#39;86.8%&#39;, &#39;89.3%&#39;, &#39;91.0%&#39;, &#39;92.2%&#39;, &#39;93.9%&#39;, &#39;95.7%&#39;, &#39;96.7%&#39;, &#39;97.7%&#39;, &#39;98.3%&#39;]
</span></span></code></pre></td></tr></table></div></div><p>In the grand scheme of things, 5.42e24 is &ldquo;not that big&rdquo;. After all, that&rsquo;s not even 15% of the <a href=https://ai.meta.com/research/publications/the-llama-3-herd-of-models/ target=_blank rel="noopener noreffer">compute used for Llama 3</a>; a <a href=https://www.semianalysis.com/p/100000-h100-clusters-power-network target=_blank rel="noopener noreffer">100k H100 cluster</a> could accomplish all of these experiments in just 2 days.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on July 30, 2024&nbsp;<a class=git-hash href=https://github.com/152334H/blog/commit/a61ab43a580feed3284628a9a3e977219e6a4d35 target=_blank title="commit by 152334H(54623771+152334H@users.noreply.github.com) a61ab43a580feed3284628a9a3e977219e6a4d35: Update index.md">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>a61ab43</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://152334H.github.io/blog/scaling-exponents/ data-title="Calculating the Cost of a Google Deepmind Paper" data-hashtags="machine learning,paper,llm,series"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://152334H.github.io/blog/scaling-exponents/ data-hashtag="machine learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://152334H.github.io/blog/scaling-exponents/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://152334H.github.io/blog/scaling-exponents/ data-title="Calculating the Cost of a Google Deepmind Paper"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://152334H.github.io/blog/scaling-exponents/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://152334H.github.io/blog/scaling-exponents/ data-title="Calculating the Cost of a Google Deepmind Paper"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://152334H.github.io/blog/scaling-exponents/ data-title="Calculating the Cost of a Google Deepmind Paper"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/machine-learning/>machine learning</a>,&nbsp;<a href=/tags/paper/>paper</a>,&nbsp;<a href=/tags/llm/>llm</a>,&nbsp;<a href=/tags/series/>series</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/blog/deepseek-0/ class=prev rel=prev title="DeepSeek Core Readings 0 - Coder"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>DeepSeek Core Readings 0 - Coder</a>
<a href=/blog/time-to-think/ class=next rel=next title="Time To Think">Time To Think<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=giscus class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app>Giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://152334H.github.io/ target=_blank>152334H</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/twemoji@14.0.2/dist/twemoji.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{giscus:{category:"Announcements",categoryId:"DIC_kwDOH03r284CQ2Tu",darkTheme:"dark_dimmed",emitMetadata:"0",inputPosition:"bottom",lang:"en",lazyLoading:!1,lightTheme:"light",mapping:"pathname",reactionsEnabled:"1",repo:"152334H/152334h.github.io",repoId:"R_kgDOH03r2w"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"},twemoji:!0}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W0STJ4D3N3",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-W0STJ4D3N3" async></script></body></html>