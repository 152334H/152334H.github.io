<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Why can't TorToiSe be fine-tuned? - 152334H</title><meta name=Description content="152334H Personal Blog"><meta property="og:title" content="Why can't TorToiSe be fine-tuned?"><meta property="og:description" content="TorToiSe üê¢ is an open-source Text-To-Speech (TTS) neural network that creates fairly authentic & realistic voices. Checkpoints for local inference have been available since April last year, but its users are seemingly unable to fine-tune the model with additional voice data.
Why is this the case, and how could it be fixed?"><meta property="og:type" content="article"><meta property="og:url" content="https://152334H.github.io/blog/tortoise-fine-tuning/"><meta property="og:image" content="https://152334H.github.io/undefined.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-11T08:13:30+08:00"><meta property="article:modified_time" content="2023-02-11T23:01:26+08:00"><meta property="og:site_name" content="152334H"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://152334H.github.io/undefined.png"><meta name=twitter:title content="Why can't TorToiSe be fine-tuned?"><meta name=twitter:description content="TorToiSe üê¢ is an open-source Text-To-Speech (TTS) neural network that creates fairly authentic & realistic voices. Checkpoints for local inference have been available since April last year, but its users are seemingly unable to fine-tune the model with additional voice data.
Why is this the case, and how could it be fixed?"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://152334H.github.io/blog/tortoise-fine-tuning/><link rel=prev href=https://152334H.github.io/blog/nus-mistake/><link rel=next href=https://152334H.github.io/blog/blog-refurbishment/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Why can't TorToiSe be fine-tuned?","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/152334H.github.io\/blog\/tortoise-fine-tuning\/"},"genre":"posts","keywords":"machine learning, TTS","wordcount":1379,"url":"https:\/\/152334H.github.io\/blog\/tortoise-fine-tuning\/","datePublished":"2023-02-11T08:13:30+08:00","dateModified":"2023-02-11T23:01:26+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"152334H"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>All Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/><b>About </b></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>All Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title><b>About</b></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Why can't TorToiSe be fine-tuned?</h1><h2 class=single-subtitle>A surprisingly cunning move</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://152334H.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>152334H</a></span>&nbsp;<span class=post-category>included in <a href=/categories/tech/><i class="far fa-folder fa-fw" aria-hidden=true></i>tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime="February 11, 2023">February 11, 2023</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1379 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;7 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#preamble>Preamble</a></li><li><a href=#premise>Premise</a></li><li><a href=#problem>Problem</a></li><li><a href=#proposal>Proposal</a></li></ul></nav></div></div><div class=content id=content><p><a href=http://nonint.com/static/tortoise_v2_examples.html target=_blank rel="noopener noreffer">TorToiSe üê¢</a> is an open-source Text-To-Speech (TTS) neural network that <abbr tabindex=0 title="It is also notable for its alleged similiarity to 11labs's TTS stack." style=color:#bbb;text-decoration-style:dotted;text-decoration-line:underline>creates fairly authentic & realistic voices</abbr>. Checkpoints for local inference have been available since <a href=https://huggingface.co/jbetker/tortoise-tts-v2/tree/main/.models target=_blank rel="noopener noreffer">April last year</a>, but its users are seemingly unable to fine-tune the model with additional voice data.</p><p>Why is this the case, and how could it be fixed?</p><h2 id=preamble>Preamble</h2><p>First, let&rsquo;s understand the situation.</p><p>The code, <a href=https://nonint.com/2022/04/25/tortoise-architectural-design-doc/ target=_blank rel="noopener noreffer">design</a>, and training details for TorToiSe are well-documented. The <a href=https://github.com/neonbjb/tortoise-tts target=_blank rel="noopener noreffer">main repo</a> contains all of the code needed to download and run the TTS models (which are hosted on <a href=https://huggingface.co/jbetker/tortoise-tts-v2 target=_blank rel="noopener noreffer">huggingface</a>). The code for training TorToiSe from scratch is covered under the <a href=https://github.com/neonbjb/DL-Art-School/tree/master/codes/models/audio/tts target=_blank rel="noopener noreffer">DL-Art-School</a> project, and the overall process is well-documented in their <a href=https://docs.google.com/document/d/13O_eyY65i6AkNrN_LdPhpUjGhyTNKYHvDrIvHnHe1GA/edit target=_blank rel="noopener noreffer">draft paper</a>.</p><p>With all of this available, it would be incredibly surprising if no one had successfully fine-tuned the model for their own purposes. As it turns out, <a href=https://twitter.com/lexman_ai target=_blank rel="noopener noreffer">someone</a> has:</p><blockquote><p>What I ended up doing instead was fine-tuning the TorToiSe models on several hundred hours of clips from the Lex Fridman podcast. I picked Lex&rsquo;s podcast because it&rsquo;s one of my favourites and he frequently talks about the concepts I want to explore with this project. His podcast was initially called &ldquo;the Artificial Intelligence Podcast&rdquo; which I also find very fitting.</p><p>I won&rsquo;t go into the details of how I fine-tuned the model since there are some concerns with this being used maliciously. If you&rsquo;re interested in fine-tuning your own model, to save you some time <strong>I will say that a critical component needed to do it has since been removed from the public release. So you won&rsquo;t be able to do it without re-training some of the models needed from scratch.</strong></p><p>&ndash; <a href=https://lexman.rocks/about target=_blank rel="noopener noreffer">@lexman_ai</a></p></blockquote><p>A few searches on the GitHub repo later, and the &lsquo;critical component&rsquo; is revealed:</p><blockquote><p>This model is extremely potent when fine-tuned. It can be easily used to make deep-fake voiceovers of people. I&rsquo;m sure you&rsquo;ve heard the Joe Rogan and Steve Jobs podcast - that was Tortoise. <strong>Withholding the VQVAE which is required to perform that fine-tuning is my way of de-weaponizing it</strong>, to an extent.</p><p>&ndash; <a href=https://github.com/neonbjb/tortoise-tts/issues/177#issuecomment-1287555902 target=_blank rel="noopener noreffer">@neonbjb</a></p></blockquote><h2 id=premise>Premise</h2><p>The &lsquo;critical component&rsquo; that has not been released is the <a href="https://docs.google.com/document/d/13O_eyY65i6AkNrN_LdPhpUjGhyTNKYHvDrIvHnHe1GA/edit#heading=h.121azdpfv4uv" target=_blank rel="noopener noreffer"><strong>&ldquo;VQVAE&rdquo;</strong></a> (or discrete-<a href=https://jaan.io/what-is-variational-autoencoder-vae-tutorial/ target=_blank rel="noopener noreffer">VAE</a>), a model which (in this case) can encode/decode spectrograms into/from a list of integers (&ldquo;MEL tokens&rdquo;) in the range <code>[0,8192)</code>.</p><p>The VQVAE is <strong>necessary for training</strong>, but also <strong>irrelevant for inference</strong>.</p><p>During training, the model is used to convert training data (the spectrograms of raw audio files) to discrete tokens that can be used by the GPT model (as predicted outputs) and the CLVP model (as inputs):</p><div class=mermaid id=id-1></div><p>During inference, the MEL tokens are simply generated by the GPT model:</p><div class=mermaid id=id-2></div><p>If none of that makes sense to you, well shit, I tried. I recommend reading the <a href=https://nonint.com/2022/04/25/tortoise-architectural-design-doc/ target=_blank rel="noopener noreffer">architectural design doc</a> and the tortoise source code, especially <a href=https://github.com/152334H/tortoise-tts-fast/blob/main/tortoise/api.py target=_blank rel="noopener noreffer"><code>api.py</code></a> and <a href=https://github.com/152334H/tortoise-tts-fast/blob/main/tortoise/models/autoregressive.py target=_blank rel="noopener noreffer"><code>models/autoregressive.py</code></a>.</p><hr><p>So, you need the VQVAE for fine-tuning. Okay, the code for it is <a href=https://github.com/neonbjb/DL-Art-School/blob/master/codes/models/audio/tts/lucidrains_dvae.py target=_blank rel="noopener noreffer">here</a>, the training hyperparameters are written <a href=https://github.com/neonbjb/tortoise-tts/issues/92#issuecomment-1149023261 target=_blank rel="noopener noreffer">here</a>, and the dataset is&mldr;</p><blockquote><p>I independently built an extended TTS dataset composed of audiobooks and podcasts scraped from the web. This data was split on 500ms silences, and any audio clip between 5-20 seconds was kept. I then fed the resulting clips through a pipeline of classifiers that I trained which remove any audio with background noise, music, poor quality (such as phone calls), multiple voices speaking at once and reverb. Due to disk space limitations, I was forced to limit the amount of scraping. The end result was 49,000 hours of cleaned audio clips.</p><p>I transcribed this dataset using a wav2vec2-large model. I personally fine-tuned this model to predict punctuation, as quotation marks, commas and exclamation marks are important for the purposes of generating speech but are not generally included in the training of speech recognition models. Fine-tuning was performed on LibriTTS and HiFiTTS and the pretrained model weights and transcription scripts can be found here.</p><p>&ndash; <a href="https://docs.google.com/document/d/13O_eyY65i6AkNrN_LdPhpUjGhyTNKYHvDrIvHnHe1GA/edit#heading=h.wh7r70k3i1jl" target=_blank rel="noopener noreffer">Appendix I - Extended Dataset Collection</a></p></blockquote><p>&mldr;absent, but probably not too difficult to emulate with a combination of <a href=https://github.com/OpenAI/whisper target=_blank rel="noopener noreffer">Whisper</a> on <a href=https://github.com/facebookresearch/libri-light target=_blank rel="noopener noreffer">LibriLight</a>, plus some additional scraping of YouTube.</p><p>Is that it?</p><h2 id=problem>Problem</h2><p>Unfortunately, this is not the end. Quoting from <a href=https://github.com/neonbjb/tortoise-tts/issues/33#issuecomment-1120576446 target=_blank rel="noopener noreffer">yet another issue</a>:</p><blockquote><p>If you are interested in the VQVAE model itself, I would be glad to divulge training details for it so you can build your own. <strong>It will not be compatible with the Tortoise codes, though</strong> (unless you are really lucky. :) )</p></blockquote><p>To understand what this means, it is necessary to describe how the VQVAE works.</p><figure><img src=R9VMWD6.0.png></figure><p>If you&rsquo;d like a more accurate description with more detail, I recommend reading <a href=https://ml.berkeley.edu/blog/posts/vq-vae/ target=_blank rel="noopener noreffer">this blog</a>.</p><p>But in simple terms:</p><ul><li>a <strong>codebook</strong> (list) of <em>K</em> arrays, each filled with <code>dim</code> random floating point values, is created for the model</li><li>the encoder converts a spectrogram to a series of arrays of length <code>dim</code>. for each array produced,<ul><li>the <strong>codebook</strong> array with the closest euclidean distance is picked.</li><li>the <strong>index</strong> of each codebook array picked corresponds to the <strong>MEL token</strong></li></ul></li><li>the decoder converts the series of codebook arrays back into a spectrogram; the whole network is trained to minimise the reconstruction loss of that spectrogram</li></ul><p>The <strong>codebook</strong> here is the problem. A fresh VQVAE, trained from scratch, would have a <strong>completely different codebook</strong>, and would fail to learn the same internal representations for audio.</p><p>Or, to be more concrete: if, given a short audio clip, the <strong>original VQVAE</strong> produces the MEL tokens <code>[39, 3258, 1892, 8101]</code>, the <strong>new VQVAE</strong> is extremely likely to produce a <strong>completely different list</strong> of tokens, like <code>[2814, 3250, 982, 5832]</code>, even if trained perfectly to negligible loss.</p><p>So, if you trained a fresh VQVAE from scratch, you would have to train <strong>the entirety of TorToiSe</strong> from scratch (ignoring the vocoder) as well to get it to work. Not ideal.</p><p>Surely there has to be some way to recreate the original VQVAE?</p><h2 id=proposal>Proposal</h2><p>When I quoted @lexman_ai above, he said:</p><blockquote><p>So you won&rsquo;t be able to do it without re-training <strong>some</strong> of the models needed from scratch.</p></blockquote><p><em>Some</em> of the models doesn&rsquo;t sound like <em>all</em> of the models. Could it be possible to train a new VQVAE, while avoiding the problems I mentioned above?</p><hr><p><em>Everything below this line is baseless speculation, so take it with a heap of salt</em></p><div class="details admonition note"><div class="details-summary admonition-title"><i class="icon fas fa-pencil-alt fa-fw" aria-hidden=true></i>Baseless Speculation<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><p>Formally, we can define the encoder/decoder pair of the VQVAE as functions $f(x) = z, g(z) = \hat x$, where</p><p>$$g(f(x)) \approx x$$</p><p><strong>Informally</strong>, <code>x</code> is a mel spectrogram, <code>z</code> is a list of MEL tokens, and <code>f</code> is a 1D resnet.</p><p>Formally, our goal is to create a new encoder, $f&rsquo;(x)$, such that</p><p>$$f&rsquo;(x) \approx f(x)$$</p><p>for a varied distribution of values of $x$ and $z$.</p><p><strong>Informally</strong>, you <em>could</em> generate a large dataset of synthetic <code>[x,z]</code> pairs by running inference on TorToiSe&rsquo;s GPT model a lot of times, and train an encoder directly to try to predict <code>z</code> from <code>x</code>. You would have to replace the overall reconstruction loss function of the VQVAE with a different loss function (MSE?) that minimises the difference between the expected cookbook array vs the output array of the encoder, and also change up all of the training hyperparameters.</p><p>Would it work? I don&rsquo;t know, but I can imagine a number of ways this could go wrong:</p><ul><li>The synthetic <code>[x,z]</code> pairs are simply too low quality for a model to converge,</li><li>The specific random cookbook arrays used in the original VQVAE are important in some way that prevents a resnet with a different cookbook from learning to predict the right tokens</li><li>i missed something obvious (it happens)</li></ul><p>If I was really going to try this idea, I might do something like:</p><ul><li>using random lines + random conditioning latents from a large tts dataset, generate (with the GPT model) a low-quality unfiltered large dataset, plus a higher quality smaller dataset filtered with CLVP</li><li>pre-train an encoder & decoder (separately) on the large dataset, before fine-tuning on the smaller dataset</li><li>switch back to the original VQVAE training method with reconstruction loss, and fine-tune the entire VQVAE on librilight or some other dataset</li><li>fine-tune the rest of tortoise to account for the differences between this reconstructed model && the original VQVAE</li></ul></div></div></div><p>But it might not work. And if it doesn&rsquo;t work, and it is truly impossible to fine-tune tortoise without training from scratch, then I can only congratulate the creator of TorToiSe for his ingenuity.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on February 11, 2023&nbsp;<a class=git-hash href=https://github.com/152334H/blog/commit/f67b4c1b9addf5faa65a10b55ba528493b1cd444 target=_blank title="commit by 152334H(54623771+152334H@users.noreply.github.com) f67b4c1b9addf5faa65a10b55ba528493b1cd444: make the tooltip work on mobile">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>f67b4c1</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://152334H.github.io/blog/tortoise-fine-tuning/ data-title="Why can't TorToiSe be fine-tuned?" data-hashtags="machine learning,TTS"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://152334H.github.io/blog/tortoise-fine-tuning/ data-hashtag="machine learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://152334H.github.io/blog/tortoise-fine-tuning/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://152334H.github.io/blog/tortoise-fine-tuning/ data-title="Why can't TorToiSe be fine-tuned?"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://152334H.github.io/blog/tortoise-fine-tuning/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://152334H.github.io/blog/tortoise-fine-tuning/ data-title="Why can't TorToiSe be fine-tuned?"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on ÂæÆÂçö" data-sharer=weibo data-url=https://152334H.github.io/blog/tortoise-fine-tuning/ data-title="Why can't TorToiSe be fine-tuned?"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/machine-learning/>machine learning</a>,&nbsp;<a href=/tags/tts/>TTS</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/blog/nus-mistake/ class=prev rel=prev title="The bleak future of Artificial Intelligence in Singapore"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>The bleak future of Artificial Intelligence in Singapore</a>
<a href=/blog/blog-refurbishment/ class=next rel=next title="Blog Refurbishment">Blog Refurbishment<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=giscus class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app>Giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://152334H.github.io/ target=_blank>152334H</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/twemoji@14.0.2/dist/twemoji.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/mermaid@9.1.3/dist/mermaid.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{giscus:{category:"Announcements",categoryId:"DIC_kwDOH03r284CQ2Tu",darkTheme:"dark_dimmed",emitMetadata:"0",inputPosition:"bottom",lang:"en",lazyLoading:!1,lightTheme:"light",mapping:"pathname",reactionsEnabled:"1",repo:"152334H/152334h.github.io",repoId:"R_kgDOH03r2w"}},data:{"id-1":`graph LR;
    Audio[Training Spectrogram] --\u003e|ConditioningEncoder| ConditioningEncoding
    Audio[Training Spectrogram] --\u003e|VQVAE| MELTokens
    Text[Training Text] --\u003e|VocabBpeTokenizer| TextTokens
    subgraph A [Training]
    ConditioningEncoding --\u003e GPT
    TextTokens --\u003e GPT
    MELTokens --\u003e GPT
    MELTokens --\u003e CLVP
    TextTokens --\u003e CLVP
    end`,"id-2":`graph LR;
    Audio[Input reference audio] --\u003e|ConditioningEncoder| ConditioningEncoding
    Text[Input Text] --\u003e|VocabBpeTokenizer| TextTokens
    ConditioningEncoding --\u003e GPT
    TextTokens --\u003e GPT
    subgraph autoregression
    GPT --\u003e MELTokens
    end
    subgraph A [top-k ranking]
    MELTokens --\u003e CLVP
    TextTokens --\u003e CLVP
    end`},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"},twemoji:!0}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W0STJ4D3N3",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-W0STJ4D3N3" async></script></body></html>