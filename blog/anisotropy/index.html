<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Contrastive Search might-not-be What You Need - 152334H</title><meta name=Description content="152334H Personal Blog"><meta property="og:title" content="Contrastive Search might-not-be What You Need"><meta property="og:description" content="TL;DR

GPT-NeoX-20B appears to be anisotropic (Isotropy: 0.197)
Int8 quantisation appears to have ~no effect on isotropy
I am new to ML so the above could be false, feel free to poke at my findings here
"><meta property="og:type" content="article"><meta property="og:url" content="https://152334H.github.io/blog/anisotropy/"><meta property="og:image" content="https://152334H.github.io/undefined.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-12T19:08:08+08:00"><meta property="article:modified_time" content="2022-12-12T21:07:39+08:00"><meta property="og:site_name" content="152334H"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://152334H.github.io/undefined.png"><meta name=twitter:title content="Contrastive Search might-not-be What You Need"><meta name=twitter:description content="TL;DR

GPT-NeoX-20B appears to be anisotropic (Isotropy: 0.197)
Int8 quantisation appears to have ~no effect on isotropy
I am new to ML so the above could be false, feel free to poke at my findings here
"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://152334H.github.io/blog/anisotropy/><link rel=prev href=https://152334H.github.io/blog/an-informal-reminder/><link rel=next href=https://152334H.github.io/blog/tortoise-tts-fast/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Contrastive Search might-not-be What You Need","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/152334H.github.io\/blog\/anisotropy\/"},"genre":"posts","keywords":"machine learning, language models, research","wordcount":799,"url":"https:\/\/152334H.github.io\/blog\/anisotropy\/","datePublished":"2022-12-12T19:08:08+08:00","dateModified":"2022-12-12T21:07:39+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"152334H"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>All Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/><b>About </b></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>All Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title><b>About</b></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Contrastive Search might-not-be What You Need</h1><h2 class=single-subtitle><span style=color:grey>Alt title: <i>Please help me fact-check my code</i></span></h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://152334H.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>152334H</a></span>&nbsp;<span class=post-category>included in <a href=/categories/tech/><i class="far fa-folder fa-fw" aria-hidden=true></i>tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime="December 12, 2022">December 12, 2022</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;799 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;4 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#gpt-j-6b>GPT-J-6B</a></li><li><a href=#gpt-neox-20b>GPT-NeoX-20B</a></li><li><a href=#is-int8-the-problem>Is Int8 the problem?</a></li></ul></nav></div></div><div class=content id=content><h4>TL;DR</h4><ul><li><u><strong>GPT-NeoX-20B appears to be anisotropic</strong></u> (Isotropy: 0.197)</li><li>Int8 quantisation appears to have ~no effect on isotropy</li><li>I am new to ML so the above could be false, feel free to poke at my findings <a href=https://github.com/152334H/Contrastive_Search_Is_What_You_Need/tree/main/isotropy_analysis target=_blank rel="noopener noreffer">here</a></li></ul><h1 id=evaluating-the-isotropy-of-more-language-models>Evaluating the isotropy of more language models</h1><p>In <a href=https://arxiv.org/abs/2210.14140 target=_blank rel="noopener noreffer"><em>Contrastive Search Is What You Need For Neural Text Generation</em></a>, it is argued that</p><blockquote><p>through extensive evaluations on a wide range of LMs with different scales, we empirically show that the English autoregressive LMs are naturally isotropic,</p></blockquote><p align=center><img width=299 src=cool_graph.png><p align=center style=color:grey>They also show a cool graph representing this finding</p></p><p>Do I have any idea what that means? <em>Nope.</em> I know that it makes <a href=https://huggingface.co/blog/introducing-csearch target=_blank rel="noopener noreffer">contrastive search</a> useful, but I don&rsquo;t grasp any of the mathematics.</p><p>What I did find out: the evaluation harness for isotropy used in the paper is <a href=https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need/tree/main/isotropy_analysis target=_blank rel="noopener noreffer">open source</a> and very easy to install! So I decided to try running it on a few models that were absent in the paper.</p><h2 id=gpt-j-6b>GPT-J-6B</h2><p>I&rsquo;m already working on a <a href=https://github.com/152334H/gpt-j-editor target=_blank rel="noopener noreffer">project</a> with GPT-J, so I started with it.</p><p>Getting the isotropy evaluation code installed is simple.</p><div class="details admonition note"><div class="details-summary admonition-title"><i class="icon fas fa-pencil-alt fa-fw" aria-hidden=true></i>Installation and setup<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><h4 id=setup-conda-env>Setup conda env</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>conda create -n csearch <span class=nv>python</span><span class=o>=</span>3.9
</span></span><span class=line><span class=cl>conda activate csearch
</span></span><span class=line><span class=cl>conda install pytorch torchvision torchaudio pytorch-cuda<span class=o>=</span>11.7 -c pytorch -c nvidia
</span></span><span class=line><span class=cl>pip install simctg
</span></span></code></pre></td></tr></table></div></div><h4 id=clone-repo--extract-dataset>Clone repo & extract dataset</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need
</span></span><span class=line><span class=cl><span class=nb>cd</span> Contrastive_Search_Is_What_You_Need/data
</span></span><span class=line><span class=cl>unzip wit.zip
</span></span></code></pre></td></tr></table></div></div><h4 id=add-code-for-new-gpt-model>Add code for new GPT model</h4><p>After getting the evaluation code installed, I edited the code to work for GPT-J, as shown in <a href=https://github.com/152334H/Contrastive_Search_Is_What_You_Need/commit/f1d609201aff2936a4fd56f12a22ed27b5633a34 target=_blank rel="noopener noreffer">this commit</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-diff data-lang=diff><span class=line><span class=cl><span class=gh>diff --git a/isotropy_analysis/compute_language_model_isotropy.py b/isotropy_analysis/compute_language_model_isotropy.py
</span></span></span><span class=line><span class=cl><span class=gh>index 44b6ed1..6c83641 100644
</span></span></span><span class=line><span class=cl><span class=gh></span><span class=gd>--- a/isotropy_analysis/compute_language_model_isotropy.py
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>+++ b/isotropy_analysis/compute_language_model_isotropy.py
</span></span></span><span class=line><span class=cl><span class=gi></span><span class=gu>@@ -25,7 +25,7 @@ def parse_text(text, tokenizer, max_len, bos_token_id=None):
</span></span></span><span class=line><span class=cl><span class=gu></span><span class=gi>+    if &#39;gpt-neo&#39; in model_name or &#39;gpt-j&#39; in model_name:
</span></span></span><span class=line><span class=cl><span class=gi></span><span class=gd>-    if &#39;gpt-neo&#39; in model_name:
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gu>@@ -63,24 +63,12 @@ if __name__ == &#39;__main__&#39;:
</span></span></span><span class=line><span class=cl><span class=gu></span><span class=gi>+    elif &#39;gpt-j&#39; in model_name:
</span></span></span><span class=line><span class=cl><span class=gi>+        print(&#39;Evaluating GPT-J model&#39;)
</span></span></span><span class=line><span class=cl><span class=gi>+        from transformers import GPTJForCausalLM, AutoTokenizer
</span></span></span><span class=line><span class=cl><span class=gi>+        model = GPTJForCausalLM.from_pretrained(model_name, revision=&#34;float16&#34;, torch_dtype=torch.float16, low_cpu_mem_usage=True)
</span></span></span><span class=line><span class=cl><span class=gi>+        tokenizer = AutoTokenizer.from_pretrained(model_name)
</span></span></span><span class=line><span class=cl><span class=gi>+        bos_token_id = None
</span></span></span></code></pre></td></tr></table></div></div></div></div></div><p>Result:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ ./inference.sh
</span></span><span class=line><span class=cl>Evaluating GPT-J model
</span></span><span class=line><span class=cl>Model loaded!
</span></span><span class=line><span class=cl>Loading data...
</span></span><span class=line><span class=cl>Data loaded!
</span></span><span class=line><span class=cl>Performing inference...
</span></span><span class=line><span class=cl>100% <span class=p>|</span><span class=c1>#######################################################|</span>
</span></span><span class=line><span class=cl>Inference completed!
</span></span><span class=line><span class=cl>Language Code:en, Model:EleutherAI/gpt-j-6B, Isotropy:0.69
</span></span></code></pre></td></tr></table></div></div><p><code>0.69</code> is about what you&rsquo;d expect from the paper, no surprise there.</p><h2 id=gpt-neox-20b>GPT-NeoX-20B</h2><p>My hardware setup is limited to an RTX 3090, so the only way I can run NeoX-20B (without more money) is to load it with <a href=https://huggingface.co/blog/hf-bitsandbytes-integration target=_blank rel="noopener noreffer">LLM.int8()</a>.</p><p>I eventually achieve that after some debugging:</p><div class="details admonition note"><div class="details-summary admonition-title"><i class="icon fas fa-pencil-alt fa-fw" aria-hidden=true></i>Debugging problems<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><p>Working from <a href=https://github.com/152334H/Contrastive_Search_Is_What_You_Need/commit/f1d609201aff2936a4fd56f12a22ed27b5633a34 target=_blank rel="noopener noreffer">the same commit</a>, I get a strange result:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ ./inference.sh
</span></span><span class=line><span class=cl>Evaluating GPT-NeoX model
</span></span><span class=line><span class=cl>Model loaded!
</span></span><span class=line><span class=cl>Loading data...
</span></span><span class=line><span class=cl>Data loaded!
</span></span><span class=line><span class=cl>Performing inference...
</span></span><span class=line><span class=cl>100% <span class=p>|</span><span class=c1>#######################################################|</span>
</span></span><span class=line><span class=cl>Inference completed!
</span></span><span class=line><span class=cl>Language Code:en, Model:EleutherAI/gpt-j-6B, Isotropy:nan
</span></span></code></pre></td></tr></table></div></div><p><code>nan</code>. The bane of Machine Learning. I&rsquo;m 99% certain this is wrong in some way, so I add print statements everywhere.</p><p>Eventually, I notice that the <code>last_hidden_states</code> of the model&rsquo;s forward pass looks like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>tensor<span class=o>([[[</span>nan, nan, nan,  ..., nan, nan, nan<span class=o>]</span>,                                                             <span class=p>|</span>
</span></span><span class=line><span class=cl>         <span class=o>[</span>nan, nan, nan,  ..., nan, nan, nan<span class=o>]</span>,
</span></span><span class=line><span class=cl>         <span class=o>[</span>nan, nan, nan,  ..., nan, nan, nan<span class=o>]</span>,
</span></span><span class=line><span class=cl>         <span class=o>[</span>nan, nan, nan,  ..., nan, nan, nan<span class=o>]</span>,
</span></span><span class=line><span class=cl>         <span class=o>[</span>nan, nan, nan,  ..., nan, nan, nan<span class=o>]</span>,
</span></span><span class=line><span class=cl>         <span class=o>[</span>nan, nan, nan,  ..., nan, nan, nan<span class=o>]]]</span>, <span class=nv>device</span><span class=o>=</span><span class=s1>&#39;cuda:0&#39;</span>,
</span></span><span class=line><span class=cl>       <span class=nv>dtype</span><span class=o>=</span>torch.float16<span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>And that if I call the model right after <code>model.eval()</code>, I get this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>tensor<span class=o>([[[</span>-0.1514,  1.2617, -0.0116,  ..., -0.1769, -1.2178,  0.0608<span class=o>]</span>,
</span></span><span class=line><span class=cl>         <span class=o>[</span>-0.8823, -0.5493, -0.0648,  ...,  0.1132, -3.9043,  0.0745<span class=o>]</span>,
</span></span><span class=line><span class=cl>         <span class=o>[</span> 0.6362,  0.8086,  0.0204,  ...,  0.0682, -5.6445,  0.2147<span class=o>]</span>,
</span></span><span class=line><span class=cl>         <span class=o>[</span>-0.1072,  1.6904, -1.4424,  ...,  0.1148,  0.1061, -1.3096<span class=o>]]]</span>,
</span></span><span class=line><span class=cl>       <span class=nv>device</span><span class=o>=</span><span class=s1>&#39;cuda:0&#39;</span>, <span class=nv>dtype</span><span class=o>=</span>torch.float16, <span class=nv>grad_fn</span><span class=o>=</span>&lt;NativeLayerNormBackward0&gt;<span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Eventually, I figure out the <a href=https://github.com/152334H/Contrastive_Search_Is_What_You_Need/blob/f1d609201aff2936a4fd56f12a22ed27b5633a34/isotropy_analysis/compute_language_model_isotropy.py#L92 target=_blank rel="noopener noreffer">line of code</a> that&rsquo;s responsible:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>if</span> <span class=n>cuda_available</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>cuda</span><span class=p>(</span><span class=n>device</span><span class=p>)</span> <span class=c1># model() produces NaN tensor after this line</span>
</span></span></code></pre></td></tr></table></div></div><p>The model is initialised with <code>device_map='auto'</code>, which puts the model on <code>device='cuda:0'</code> on my machine, which should be equivalent to the <code>device</code> in the above line.</p><p><strong>I do not understand</strong> why the above line causes everything to go to <code>nan</code>, but I also do not think it is related to the result later on, because this problem occurs in other models (e.g. GPT-J-6B) as well.</p><p>I&rsquo;m leaving this information here in case I happen to be wrong.</p></div></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ./inference.sh
</span></span><span class=line><span class=cl>Evaluating EleutherAI/gpt-neox-20b
</span></span><span class=line><span class=cl>Model loaded!
</span></span><span class=line><span class=cl>Loading data...
</span></span><span class=line><span class=cl>Data loaded!
</span></span><span class=line><span class=cl>Performing inference...
</span></span><span class=line><span class=cl>100% <span class=p>|</span><span class=c1>##############################################|</span>
</span></span><span class=line><span class=cl>Inference completed!
</span></span><span class=line><span class=cl>Language Code:en, Model:EleutherAI/gpt-neox-20b, Isotropy:0.1973011465713972
</span></span></code></pre></td></tr></table></div></div><p>And this is the reason why I bothered to write this post at all. <strong>8bit GPT-NeoX-20B</strong>, as evaluated on my machine, appears to be <strong>anisotropic</strong>.</p><h2 id=is-int8-the-problem>Is Int8 the problem?</h2><p>Probably not.</p><p align=center><img src=results.png></p><p>I tested int8 quantisised vs fp16/fp32 models, and got mostly identical results.</p><p>I can&rsquo;t rule out the possibility that my code/hardware is broken specifically for NeoX-20B, but I think the experiments I&rsquo;ve done are strong indicators that bugged code is probably not the cause of the observed anisotropy.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on December 12, 2022&nbsp;<a class=git-hash href=https://github.com/152334H/blog/commit/808e9df60e830becddedecb04e712705f2993315 target=_blank title="commit by 152334H(54623771+152334H@users.noreply.github.com) 808e9df60e830becddedecb04e712705f2993315: add date">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>808e9df</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://152334H.github.io/blog/anisotropy/ data-title="Contrastive Search might-not-be What You Need" data-hashtags="machine learning,language models,research"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://152334H.github.io/blog/anisotropy/ data-hashtag="machine learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://152334H.github.io/blog/anisotropy/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://152334H.github.io/blog/anisotropy/ data-title="Contrastive Search might-not-be What You Need"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://152334H.github.io/blog/anisotropy/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://152334H.github.io/blog/anisotropy/ data-title="Contrastive Search might-not-be What You Need"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://152334H.github.io/blog/anisotropy/ data-title="Contrastive Search might-not-be What You Need"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/machine-learning/>machine learning</a>,&nbsp;<a href=/tags/language-models/>language models</a>,&nbsp;<a href=/tags/research/>research</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/blog/an-informal-reminder/ class=prev rel=prev title="An Informal Reminder"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>An Informal Reminder</a>
<a href=/blog/tortoise-tts-fast/ class=next rel=next title="Fast (5x) Inference with TorToiSe-TTS">Fast (5x) Inference with TorToiSe-TTS<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=giscus class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app>Giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2024</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://152334H.github.io/ target=_blank>152334H</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/twemoji@14.0.2/dist/twemoji.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{giscus:{category:"Announcements",categoryId:"DIC_kwDOH03r284CQ2Tu",darkTheme:"dark_dimmed",emitMetadata:"0",inputPosition:"bottom",lang:"en",lazyLoading:!1,lightTheme:"light",mapping:"pathname",reactionsEnabled:"1",repo:"152334H/152334h.github.io",repoId:"R_kgDOH03r2w"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"},twemoji:!0}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W0STJ4D3N3",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-W0STJ4D3N3" async></script></body></html>