<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>DeepSeek Core Readings 0 - Coder - 152334H</title><meta name=Description content="152334H Personal Blog"><meta property="og:title" content="DeepSeek Core Readings 0 - Coder"><meta property="og:description" content="Paper summary: 1.3B to 33B LLMs on 1/2T code tokens (87 langs) w/ FiM and 16K seqlen. Strong effort in constructing pretraining data from Github from scratch, with repository-level samples. Evals beat OSS code models solidly + GPT-3.5 a bit; Coder-7B > CodeLlama-33B often.
They don&rsquo;t spend much effort on Instruction tuning. They commit a continued pretrain of DeepSeek LLM -> Coder: I believe it underperforms; they don&rsquo;t."><meta property="og:type" content="article"><meta property="og:url" content="https://152334H.github.io/blog/deepseek-0/"><meta property="og:image" content="https://152334H.github.io/undefined.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-30T00:00:00+08:00"><meta property="article:modified_time" content="2024-06-29T22:46:32+08:00"><meta property="og:site_name" content="152334H"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://152334H.github.io/undefined.png"><meta name=twitter:title content="DeepSeek Core Readings 0 - Coder"><meta name=twitter:description content="Paper summary: 1.3B to 33B LLMs on 1/2T code tokens (87 langs) w/ FiM and 16K seqlen. Strong effort in constructing pretraining data from Github from scratch, with repository-level samples. Evals beat OSS code models solidly + GPT-3.5 a bit; Coder-7B > CodeLlama-33B often.
They don&rsquo;t spend much effort on Instruction tuning. They commit a continued pretrain of DeepSeek LLM -> Coder: I believe it underperforms; they don&rsquo;t."><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://152334H.github.io/blog/deepseek-0/><link rel=prev href=https://152334H.github.io/blog/deepseek-1/><link rel=next href=https://152334H.github.io/blog/scaling-exponents/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"DeepSeek Core Readings 0 - Coder","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/152334H.github.io\/blog\/deepseek-0\/"},"genre":"posts","keywords":"machine learning, DeepSeek, llm, series, codegen","wordcount":1236,"url":"https:\/\/152334H.github.io\/blog\/deepseek-0\/","datePublished":"2024-06-30T00:00:00+08:00","dateModified":"2024-06-29T22:46:32+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"152334H"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>All Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/><b>About </b></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>All Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title><b>About</b></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">DeepSeek Core Readings 0 - Coder</h1><h2 class=single-subtitle>Part 0/? of a <a href=/deepseek-core-readings>series</a></h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://152334H.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>152334H</a></span>&nbsp;<span class=post-category>included in <a href=/categories/tech/><i class="far fa-folder fa-fw" aria-hidden=true></i>tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime="June 30, 2024">June 30, 2024</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1236 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;6 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#dataset>Dataset</a></li><li><a href=#training>Training</a><ul><li><a href=#objectives>Objectives</a></li><li><a href=#arch>Arch</a></li></ul></li><li><a href=#evaluation>Evaluation</a><ul><li><a href=#code-generation>Code generation</a></li><li><a href=#others>Others</a></li></ul></li><li><a href=#continued-pre-training>Continued Pre-training</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></div><div class=content id=content><p><a href=https://arxiv.org/pdf/2401.14196 target=_blank rel="noopener noreffer">Paper</a> summary: 1.3B to 33B LLMs on 1/2T code tokens (87 langs) w/ FiM and 16K seqlen. Strong effort in constructing pretraining data from Github from scratch, with repository-level samples. Evals beat OSS code models solidly + GPT-3.5 a bit; Coder-7B > CodeLlama-33B often.</p><p>They don&rsquo;t spend much effort on Instruction tuning. They commit a continued pretrain of DeepSeek LLM -> Coder: I believe it underperforms; they don&rsquo;t.</p><h2 id=dataset>Dataset</h2><p>2T tokens: 87% source code, 10%/3% code-related natural English/Chinese &ndash; English from github markdown / StackExchange, Chinese from selected articles.</p><p>To scrape github, they:</p><ol><li>crawl all repositories created before Feb 2023, keeping only top87 langs. Inspired by StarCoder, they apply filters against<ul><li>files with average length > 100chars or maxlen > 1000chars</li><li>files with &lt;25% alphabetic chars</li><li>files with <code>&lt;?xml version=</code> in first 100chars, except files tagged as XSLT</li><li>HTML files that have &lt;20% visible text, or that are more &ldquo;than 100 characters&rdquo; (???)</li><li>JSON/YAML files that aren&rsquo;t in length $[50,5000]$.</li></ul></li><li>They attempt to <strong>analyse dependencies between files</strong> within the same repository,<ul><li>by using a basic regexep to look for import keywords,</li><li>they create a <strong>potentially circular</strong> dependency graph between different files,</li><li>use a modified toposort that selects nodes with the least in-degrees rather than none,</li><li>to create a linear sorted topology of files that are then concatentated (with added comment about filepath) to form a singlular training example.</li></ul></li><li>They do <strong>repo-level deduplication</strong>, i.e. they compare concatentated repo examples for near-duplicates and prune repos when appropriate.</li><li>They use a compiler & quality model & heuristics to filter out garbage. This is supposed to get rid of code with syntax errors / poor readability/modularity.</li><li>They use an n-gram filter to get rid of test data from the train set. Specifically they look for 10-grams identical to any string in the test data, and also check for identical match on test examples that are shorter than 10-grams.</li></ol><figure><img src=Pasted%20image%2020240624022622.png></figure><h2 id=training>Training</h2><h3 id=objectives>Objectives</h3><p>By default, models are assumed to be trained with basic CausalLM.</p><p>Then, they consider applying the <a href=https://openai.com/index/efficient-training-of-language-models-to-fill-in-the-middle/ target=_blank rel="noopener noreffer">FIM</a> objective. DeepSeek considered:</p><ul><li>using T5-like masked-span prediction (&ldquo;MSP&rdquo;) to infill 50% of the time</li><li>using varying (0/50/100%) levels of FIM, using the traditional Prefix-Suffix-Middle (PSM) format.
<code>&lt;|fim_start|>{pre}&lt;|fim_hole|>{suf}&lt;|fim_end|>{mid}&lt;|eos_token|></code></li></ul><p>On 1.3B experiments, they observe that FIM 50% generally does better than MSP 50% on both infilling && code completion benchmarks.</p><p>They mention possibly using Suffix-Prefix-Middle (SPM) at the start of Section 3, but it is not clear to me whether they actually used it for their models or not.</p><h3 id=arch>Arch</h3><p>They use a 32k+ BPE vocab. This might sound like they just copied the LLaMA tokenizer, but their BOS/EOS token IDs are different (32013/32014), and they pad their embedding to 32256.</p><p>They do 1.3B/6.7B/33B. Only 33B is GQA ($R_\text{kv}=1/8$).</p><ul><li>1.3B: <code>D=2048 R_ffn=2.6875 L=24 H=16 BS=1024 LR=5.3e-4</code></li><li>6.7B: <code>D=4096 R_ffn=2.6875 L=32 H=32 BS=2304 LR=4.2e-4</code></li><li>33B: <code>D=7168 R_ffn=19/28 L=64 H=56 BS=3840 LR=3.5e-4</code></li></ul><p>Optim/LR follows <a href=deepseek-1 rel>Deepseek LLM</a>. RoPE $\theta=100000$ and 4x linear scaling, with 1k steps of 16k seqlen training. 64k extrapolation not reliable here.</p><p>I find their cluster description odd:</p><blockquote><p>&mldr;utilize clusters outfitted with NVIDIA A100 and H800 GPUs. In the A100 cluster, each node is configured with 8 GPUs, interconnected in pairs using NVLink bridges. The H800 cluster is similarly arranged, with each node containing 8 GPUs. These GPUs are interconnected using a combination of NVLink and NVSwitch technologies, ensuring efficient data transfer within nodes. To facilitate seamless communication between nodes in both A100 and H800 clusters, we employ InfiniBand interconnects, known for their high throughput and low latency.</p></blockquote><p>I don&rsquo;t get &ldquo;interconnected in pairs.&rdquo; An SXM A100 node should have 8 GPUs connected all-to-all over an NVSwitch.</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://docs.nvidia.com/dgx/dgxa100-user-guide/_images/dgxa100-system-topology.png data-srcset="https://docs.nvidia.com/dgx/dgxa100-user-guide/_images/dgxa100-system-topology.png, https://docs.nvidia.com/dgx/dgxa100-user-guide/_images/dgxa100-system-topology.png 1.5x, https://docs.nvidia.com/dgx/dgxa100-user-guide/_images/dgxa100-system-topology.png 2x" data-sizes=auto alt=https://docs.nvidia.com/dgx/dgxa100-user-guide/_images/dgxa100-system-topology.png title=https://docs.nvidia.com/dgx/dgxa100-user-guide/_images/dgxa100-system-topology.png></p><p>Direct pairing should only apply for PCIe A100s. It is technically possible that they had NVL bridges across PCIe pairs, and used some CX-6 PCIe connectors, and had a smart parallelism strategy to reduce cross-pair comms maximally. But my bet is &ldquo;typing/translation error&rdquo;.</p><p>Anyway,</p><p>They do a <em>lot less</em> for post-training alignment here than they do for Deepseek LLM. They have only a single small section for SFT, where they use 100 step warmup cosine over 2B tokens on 1e-5 lr with 4M batch size. Their dataset is unknown alpaca-like with <code>&lt;|EOT|></code> at chat turn ends.</p><h2 id=evaluation>Evaluation</h2><p>They compare against <strong>CodeGeeX2</strong>, <strong>StarCoder</strong>, <strong>CodeLlama</strong>, <strong>code-cushman-001</strong>, and <strong>GPT-3.5/4</strong> (of course). God these names bring back memories.</p><h3 id=code-generation>Code generation</h3><p>On <strong>HumanEval/MBPP</strong> (and, note: both of these are normally Python only), 1.3B-instruct trounces all non-GPT models, and 33B beats 3.5 Turbo by about 3%.</p><p>Deepseek kindly makes the effort to use the <a href=https://arxiv.org/pdf/2208.08227 target=_blank rel="noopener noreffer">MultiPL-E</a> strategy to convert the Python problems in HumanEval to C++/Java/PHP/TS/JS/C#/Bash, and finds similar results for the rest of the languages.</p><p>Because HumanEval/MBPP is too simple (basically no libraries), they also test with DS-1000. They do not compare with GPT3.5/4 here, so deepseek-coder wins by default.</p><p>Like Deepseek-LLM, they use <strong>LeetCode contests</strong> as a benchmark, where 33B achieves a Pass@1 of 27.8%, better than 3.5 again. They notice that their model improves on Medium/Hard problems with CoT, but worsens slightly on Easy problems. They also notice evidence of data contamination, as their model (and GPT-4) performs better on problems from July/August.</p><h3 id=others>Others</h3><p>On SantaCoder&rsquo;s <strong>Single-Line Infilling</strong> benchmark, Codellama-13B-base beats Deepseek-33B-base (!) for Python (but not for java/javascript). Despite bolding this information in the table, they do not describe this phenomeon at all in the prose, stating:</p><blockquote><p>Despite being the smallest model with a capacity of 1.3 billion parameters, DeepSeek-Coder outperforms its larger counterparts, StarCoder and CodeLlama, in these benchmarks.</p></blockquote><p>This is a bit weird. Usually Deepseek is more dignified than this.</p><p>Anyway, on <strong>CrossCodeEval</strong> &ndash; <a href=https://crosscodeeval.github.io/ target=_blank rel="noopener noreffer">a benchmark</a> where every code completion problem requires context from extra files, that was also created with data <em>after</em> DeepSeek&rsquo;s pretrianing cutoff &ndash; they once again note that their model performs good compared to non-OpenAI models (with the Retrieval strategy)</p><p>Finally, Deepseek groups GSM8k/MATH/GSM-Hard/SVAMP/TabMWP/ASDiv/MAWPS under the general category of &ldquo;<strong>Program-based Math Reasoning</strong>&rdquo; benchmarks (and yeah they win against <code>$baseline</code>). For these benchmarks,</p><blockquote><p>&ldquo;the model is prompted to alternately describe a solution step in natural language and then execute that step with code&rdquo;.</p></blockquote><p>I&rsquo;m not sure what this means. Do they do step-by-step reasoning? Do they <em>actually</em> execute the code, ala Code Interpreter, or just tell the model to hallucinate an execution? I&rsquo;d guess the latter, since code environments aren&rsquo;t that simple to setup.</p><h2 id=continued-pre-training>Continued Pre-training</h2><p>Deepseek attempts training DeekSeek-LLM-7B-Base on 2 Trillion more code tokens to create <strong>DeepSeek-Coder-v1.5 7B</strong>. Their training process is completely different from the prior DeepSeek models described &ndash; they purely use CausalLM, and they use different data sources:</p><figure><img src=Pasted%20image%2020240629215736.png></figure><p>They want to compare pure pretrained Coder vs continual pretrained Coder-v1.5, and test HumanEval/MBPP + GSM8K/MATH + standard GPT4All bench mix. They find that&mldr;</p><blockquote><p>the DeepSeek-Coder-Base-v1.5 model, despite a slight decrease in coding performance,
shows marked improvements across most tasks when compared to the DeepSeek-Coder-Base
model.</p></blockquote><p>Yes, you read that right. Despite being worse at coding, they state that DeepSeek-Coder-v1.5 is better. Because it performs better than Coder v1 && LLM v1 at NLP / Math benchmarks. After having 2T more tokens than both.</p><p>&mldr;</p><h2 id=conclusion>Conclusion</h2><p>Other non-openai code models at the time sucked compared to DeepSeek-Coder on the tested regime (basic problems, library usage, leetcode, infilling, small cross-context, math reasoning), and especially suck to their basic instruct FT.</p><p>They use motivated reasoning to conclude that continued pretraining on general LLMs -> code LLMs is good.</p><p>Their appendix contains example usages, benchmark curves over pretraining tokens (their 1.3B was only trained to 1T!)</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on June 29, 2024&nbsp;<a class=git-hash href=https://github.com/152334H/blog/commit/393c2ddac9788860aa589c8dd0504ab144833a8e target=_blank title="commit by 152334H(54623771+152334H@users.noreply.github.com) 393c2ddac9788860aa589c8dd0504ab144833a8e: deepseek coder">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>393c2dd</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://152334H.github.io/blog/deepseek-0/ data-title="DeepSeek Core Readings 0 - Coder" data-hashtags="machine learning,DeepSeek,llm,series,codegen"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://152334H.github.io/blog/deepseek-0/ data-hashtag="machine learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://152334H.github.io/blog/deepseek-0/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://152334H.github.io/blog/deepseek-0/ data-title="DeepSeek Core Readings 0 - Coder"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://152334H.github.io/blog/deepseek-0/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://152334H.github.io/blog/deepseek-0/ data-title="DeepSeek Core Readings 0 - Coder"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://152334H.github.io/blog/deepseek-0/ data-title="DeepSeek Core Readings 0 - Coder"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/machine-learning/>machine learning</a>,&nbsp;<a href=/tags/deepseek/>DeepSeek</a>,&nbsp;<a href=/tags/llm/>llm</a>,&nbsp;<a href=/tags/series/>series</a>,&nbsp;<a href=/tags/codegen/>codegen</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/blog/deepseek-1/ class=prev rel=prev title="DeepSeek Core Readings 1 - LLM"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>DeepSeek Core Readings 1 - LLM</a>
<a href=/blog/scaling-exponents/ class=next rel=next title="Calculating the Cost of a Google Deepmind Paper">Calculating the Cost of a Google Deepmind Paper<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=giscus class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app>Giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2024</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://152334H.github.io/ target=_blank>152334H</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/twemoji@14.0.2/dist/twemoji.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{giscus:{category:"Announcements",categoryId:"DIC_kwDOH03r284CQ2Tu",darkTheme:"dark_dimmed",emitMetadata:"0",inputPosition:"bottom",lang:"en",lazyLoading:!1,lightTheme:"light",mapping:"pathname",reactionsEnabled:"1",repo:"152334H/152334h.github.io",repoId:"R_kgDOH03r2w"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"},twemoji:!0}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W0STJ4D3N3",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-W0STJ4D3N3" async></script></body></html>