<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Non-determinism in GPT-4 is caused by Sparse MoE - 152334H</title><meta name=Description content="152334H Personal Blog"><meta property="og:title" content="Non-determinism in GPT-4 is caused by Sparse MoE"><meta property="og:description" content="It&rsquo;s well-known at this point that GPT-4/GPT-3.5-turbo is non-deterministic, even at temperature=0.0. This is an odd behavior if you&rsquo;re used to dense decoder-only models, where temp=0 should imply greedy sampling which should imply full determinism, because the logits for the next token should be a pure function of the input sequence & the model weights."><meta property="og:type" content="article"><meta property="og:url" content="https://152334H.github.io/blog/non-determinism-in-gpt-4/"><meta property="og:image" content="https://152334H.github.io/undefined.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-05T04:09:15+08:00"><meta property="article:modified_time" content="2023-08-05T04:31:56+08:00"><meta property="og:site_name" content="152334H"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://152334H.github.io/undefined.png"><meta name=twitter:title content="Non-determinism in GPT-4 is caused by Sparse MoE"><meta name=twitter:description content="It&rsquo;s well-known at this point that GPT-4/GPT-3.5-turbo is non-deterministic, even at temperature=0.0. This is an odd behavior if you&rsquo;re used to dense decoder-only models, where temp=0 should imply greedy sampling which should imply full determinism, because the logits for the next token should be a pure function of the input sequence & the model weights."><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://152334H.github.io/blog/non-determinism-in-gpt-4/><link rel=prev href=https://152334H.github.io/blog/dumped-blog-ideas/><link rel=next href=https://152334H.github.io/blog/knowing-enough-about-moe/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Non-determinism in GPT-4 is caused by Sparse MoE","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/152334H.github.io\/blog\/non-determinism-in-gpt-4\/"},"genre":"posts","keywords":"openai, machine learning, moe","wordcount":1701,"url":"https:\/\/152334H.github.io\/blog\/non-determinism-in-gpt-4\/","datePublished":"2023-08-05T04:09:15+08:00","dateModified":"2023-08-05T04:31:56+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"152334H"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>All Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/><b>About </b></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>All Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title><b>About</b></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Non-determinism in GPT-4 is caused by Sparse MoE</h1><h2 class=single-subtitle>What the title says</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://152334H.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>152334H</a></span>&nbsp;<span class=post-category>included in <a href=/categories/tech/><i class="far fa-folder fa-fw" aria-hidden=true></i>tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime="August 5, 2023">August 5, 2023</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1701 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;8 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#are-you-really-sure-it-isnt-hardware>Are you really sure it isn&rsquo;t hardware?</a><ul><li><a href=#empirical-results>Empirical Results</a></li><li><a href=#yes-im-sure>Yes, I&rsquo;m sure</a></li></ul></li><li><a href=#implications>Implications</a><ul><li><a href=#were-so-far-behind>We&rsquo;re so far behind</a></li><li><a href=#gpt-35-turbo-may-be-moe-too>GPT-3.5-Turbo may be MoE too</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></div><div class=content id=content><p>It&rsquo;s <a href=https://twitter.com/BorisMPower/status/1608522707372740609 target=_blank rel="noopener noreffer">well-known</a> at this point that GPT-4/GPT-3.5-turbo is non-deterministic, even at <code>temperature=0.0</code>. This is an odd behavior if you&rsquo;re used to dense decoder-only models, where temp=0 should imply <a href=https://nn.labml.ai/sampling/greedy.html target=_blank rel="noopener noreffer">greedy sampling</a> which should imply full determinism, because the logits for the next token should be a pure function of the input sequence & the model weights.</p><iframe id=AudioNativeElevenLabsPlayer width=100% height=180 frameborder=no scrolling=no seamless src="https://beta.elevenlabs.io/player/index.html?publicUserId=01e30727d011cacbba870a0524d6020b350e4855e45a2854e421a4f6152a9f30"></iframe>
<script src=https://beta.elevenlabs.io/player/audioNativeHelper.js type=text/javascript></script><p>When asked about this behaviour at the <a href=https://humanloop.com/blog/openai-plans target=_blank rel="noopener noreffer">developer roundtables</a> during OpenAI&rsquo;s World Tour, the responses of the members of technical staff were something along the lines of,</p><blockquote><p>Honestly, we&rsquo;re confused as well. We think there might be some bug in our systems, or some <a href=https://twitter.com/taliaringer/status/1511411984398487564 target=_blank rel="noopener noreffer">non-determinism in optimized floating point calculations</a>&mldr;</p></blockquote><p>And internally, I was thinking &ndash; okay, I know the latter point is true sometimes, and maybe OpenAI doesn&rsquo;t have enough engineers to look into a problem as small as this. I felt a little bit <a href=https://www.lesswrong.com/s/zpCiuR4T343j9WkcK target=_blank rel="noopener noreffer">confused</a> when I noticed a reference to this behavior <a href=https://community.openai.com/t/a-question-on-determinism/8185/2 target=_blank rel="noopener noreffer">over 3 years ago</a> &ndash; 3 years, and this couldn&rsquo;t be fixed?</p><p>But I didn&rsquo;t have a meaningful alternative explanation for the phenomenon. After all, why would you <em>want</em> to keep things random? Ilya&rsquo;s always going on about <a href="https://www.youtube.com/watch?v=Yf1o0TQzry8" target=_blank rel="noopener noreffer">reliability</a>, right? There was no way OpenAI <em>wanted</em> to keep determinism bugged, so an unresolvable hardware limitation was the best explanation.</p><hr><p>3 months later, reading a paper while on board a <a href=https://www.flightradar24.com/data/flights/ca969#316fde52 target=_blank rel="noopener noreffer">boring flight</a> home, I have my answer.</p><p>In the recent <a href=https://arxiv.org/abs/2308.00951 target=_blank rel="noopener noreffer">Soft MoE</a> paper, there was an interesting blurb in Section 2.2 that sparked a connection:</p><figure><img src=Pasted%20image%2020230804191515.png></figure><blockquote><p>Under capacity constraints, all Sparse MoE approaches route tokens in groups of a fixed size and enforce (or encourage) balance within the group. When groups contain tokens from different sequences or inputs, these tokens often compete against each other for available spots in expert buffers. <strong>As a consequence, the model is no longer deterministic at the sequence-level, but only at the batch-level</strong>, as some input sequences may affect the final prediction for other inputs</p></blockquote><p>It is currently <a href=https://www.semianalysis.com/p/gpt-4-architecture-infrastructure target=_blank rel="noopener noreffer">public knowledge</a> that GPT-4 is a Mixture of Experts model. Given that GPT-4 was <a href=https://cdn.openai.com/papers/gpt-4.pdf target=_blank rel="noopener noreffer">trained before Q2 2022</a>, and that Sparse Mixture-of-Experts have existed <a href=https://arxiv.org/abs/2101.03961 target=_blank rel="noopener noreffer">long before that</a>, I think the following hypothesis is justified:</p><blockquote><p>The GPT-4 API is hosted with a backend that does <a href=https://www.anyscale.com/blog/continuous-batching-llm-inference target=_blank rel="noopener noreffer">batched inference</a>. Although some of the randomness may be explained by other factors, <strong>the vast majority</strong> of non-determinism in the API is explainable by its Sparse MoE architecture failing to enforce per-sequence determinism.</p></blockquote><p>This is either completely wrong, or something that was already obvious and well-known to people developing MoE models. How can we verify this?</p><h2 id=are-you-really-sure-it-isnt-hardware>Are you really sure it isn&rsquo;t hardware?</h2><p>Not yet. Let&rsquo;s <a href=https://chat.openai.com/share/4a60bf97-0058-41d1-8a93-983230b02169 target=_blank rel="noopener noreffer">ask GPT-4</a> to write a script to test our hypothesis:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tqdm</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>openai</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>time</span> <span class=kn>import</span> <span class=n>sleep</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pathlib</span> <span class=kn>import</span> <span class=n>Path</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>chat_models</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;gpt-4&#34;</span><span class=p>,</span> <span class=s2>&#34;gpt-3.5-turbo&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>message_history</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;You are a helpful assistant.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;Write a unique, surprising, extremely randomized story with highly unpredictable changes of events.&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion_models</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;text-davinci-003&#34;</span><span class=p>,</span> <span class=s2>&#34;text-davinci-001&#34;</span><span class=p>,</span> <span class=s2>&#34;davinci-instruct-beta&#34;</span><span class=p>,</span> <span class=s2>&#34;davinci&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;[System: You are a helpful assistant]</span><span class=se>\n\n</span><span class=s2>User: Write a unique, surprising, extremely randomized story with highly unpredictable changes of events.</span><span class=se>\n\n</span><span class=s2>AI:&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TimeIt</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>name</span><span class=p>):</span> <span class=bp>self</span><span class=o>.</span><span class=n>name</span> <span class=o>=</span> <span class=n>name</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__enter__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span> <span class=bp>self</span><span class=o>.</span><span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__exit__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>):</span> <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>name</span><span class=si>}</span><span class=s2> took </span><span class=si>{</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>start</span><span class=si>}</span><span class=s2> seconds&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>C</span> <span class=o>=</span> <span class=mi>30</span>  <span class=c1># number of completions to make per model</span>
</span></span><span class=line><span class=cl><span class=n>N</span> <span class=o>=</span> <span class=mi>128</span> <span class=c1># max_tokens</span>
</span></span><span class=line><span class=cl><span class=c1># Testing chat models</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>model</span> <span class=ow>in</span> <span class=n>chat_models</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>sequences</span> <span class=o>=</span> <span class=nb>set</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>errors</span> <span class=o>=</span> <span class=mi>0</span> <span class=c1># although I track errors, at no point were any errors ever emitted</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>TimeIt</span><span class=p>(</span><span class=n>model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>C</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>completion</span> <span class=o>=</span> <span class=n>openai</span><span class=o>.</span><span class=n>ChatCompletion</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>messages</span><span class=o>=</span><span class=n>message_history</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>max_tokens</span><span class=o>=</span><span class=n>N</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>logit_bias</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;100257&#34;</span><span class=p>:</span> <span class=o>-</span><span class=mf>100.0</span><span class=p>},</span> <span class=c1># this doesn&#39;t really do anything, because chat models don&#39;t do &lt;|endoftext|&gt; much</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>sequences</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>completion</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>                <span class=n>sleep</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=c1># cheaply avoid rate limiting</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;something went wrong for&#39;</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>e</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>errors</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Model </span><span class=si>{</span><span class=n>model</span><span class=si>}</span><span class=s2> created </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span><span class=si>}</span><span class=s2> (</span><span class=si>{</span><span class=n>errors</span><span class=si>=}</span><span class=s2>) unique sequences:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>sequences</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=n>results</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=nb>len</span><span class=p>(</span><span class=n>sequences</span><span class=p>),</span> <span class=n>model</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># Testing completion models</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>model</span> <span class=ow>in</span> <span class=n>completion_models</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>sequences</span> <span class=o>=</span> <span class=nb>set</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>errors</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>TimeIt</span><span class=p>(</span><span class=n>model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>C</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>completion</span> <span class=o>=</span> <span class=n>openai</span><span class=o>.</span><span class=n>Completion</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>prompt</span><span class=o>=</span><span class=n>prompt</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>max_tokens</span><span class=o>=</span><span class=n>N</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>logit_bias</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;50256&#34;</span><span class=p>:</span> <span class=o>-</span><span class=mf>100.0</span><span class=p>},</span> <span class=c1># prevent EOS</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>sequences</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>completion</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>sleep</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;something went wrong for&#39;</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>e</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>errors</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Model </span><span class=si>{</span><span class=n>model</span><span class=si>}</span><span class=s2> created </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span><span class=si>}</span><span class=s2> (</span><span class=si>{</span><span class=n>errors</span><span class=si>=}</span><span class=s2>) unique sequences:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>sequences</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=n>results</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=nb>len</span><span class=p>(</span><span class=n>sequences</span><span class=p>),</span> <span class=n>model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Printing table of results</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Table of Results:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Num_Sequences</span><span class=se>\t</span><span class=s2>Model_Name&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>num_sequences</span><span class=p>,</span> <span class=n>model_name</span> <span class=ow>in</span> <span class=n>results</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>num_sequences</span><span class=si>}</span><span class=se>\t</span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>This final script is a little different from what you&rsquo;d see if you clicked on the share link. I had to redo the script a few times along the way, because of a few problems:</p><ul><li><p>the OpenAI API was taking very long to respond. I had to add timestamp logging to check I wasn&rsquo;t doing something wrong &ndash; I wasn&rsquo;t, the API was simply <em>really</em> slow, with nearly 10 seconds of delay to call even 3.5 turbo. I wonder why?</p></li><li><p>some completion models were truncating their responses very early. I added a logit bias against EOS to try to fix this.</p></li><li><p>Related: there is no equivalent bias against the <code>&lt;|im_end|></code> token; the API returns, <code>Invalid key in 'logit_bias': 100265. Maximum value is 100257.</code> 100265 is the accurate value for <code>&lt;|im_end|></code>:</p><figure><img src=Pasted%20image%2020230805031539.png></figure><p>I figured this lack-of-logit-bias problem for the chat models was a non-issue &ndash; most completions reached the max token length, and they were absurdly more non-deterministic anyway (adding the logit bias would realistically only increase the number of unique sequences)</p></li></ul><p>An hour of waiting and scripting later, and I got <a href=https://gist.github.com/152334H/047827ad3740627f4d37826c867a196e#comment---this-is-an-example-output-from-the-script target=_blank rel="noopener noreffer">confirmation</a>:</p><h3 id=empirical-results>Empirical Results</h3><p>Here are the results (3 attempts, <code>N=30</code>, <code>max_tokens=128</code>):</p><table><thead><tr><th>Model Name</th><th>Unique Completions (/30)</th><th>Average (/30)</th><th>Notes</th></tr></thead><tbody><tr><td>gpt-4</td><td>12,11,12</td><td>11.67</td><td></td></tr><tr><td>gpt-3.5-turbo</td><td>4,4,3</td><td>3.67</td><td></td></tr><tr><td>text-davinci-003</td><td>3,2,4</td><td>3.00</td><td></td></tr><tr><td>text-davinci-001</td><td>2,2,2</td><td>2.00</td><td></td></tr><tr><td>davinci-instruct-beta</td><td>1,1,1</td><td>deterministic</td><td>Outputs deteriorated into repeated loop</td></tr><tr><td>davinci</td><td>1,1,1</td><td>deterministic</td><td>Outputs deteriorated into repeated loop</td></tr></tbody></table><p>Before I noticed the <code>logit_bias</code> problem, I also obtained the following results (<code>max_tokens=256</code>):</p><table><thead><tr><th>Model Name</th><th>Unique Completions (/30)</th><th>Notes</th></tr></thead><tbody><tr><td>gpt-4</td><td>30</td><td></td></tr><tr><td>gpt-3.5-turbo</td><td>9</td><td></td></tr><tr><td>text-davinci-003</td><td>5</td><td></td></tr><tr><td>text-davinci-001</td><td>2</td><td>Noticed the logit bias problem at this point</td></tr></tbody></table><h3 id=yes-im-sure>Yes, I&rsquo;m sure</h3><p>The number of unique completions from GPT-4 is <strong>ridiculously</strong> high &ndash; practically <em>always</em> non-deterministic with longer outputs. This almost certainly confirms that something is up with GPT-4.</p><p>Additionally, all other models that do not collapse into a repetitive useless loop experience some degree of non-determinism as well. This lines up with the public claim that unreliable GPU calculations are responsible for some degree of randomness. However,</p><ol><li>I&rsquo;m still partially confused by the gradual increase in randomness from text-davinci-001 up to gpt-3.5-turbo. I don&rsquo;t have a neat explanation for why 003 is reliably more random than 001, or turbo more so than 003. Although I expect only the chat models to be MoE models, and not any of the 3.5 completion models, I don&rsquo;t feel confident based on the current evidence available.</li><li>This is only evidence that <em>something</em> is causing GPT-4 to be much, much more non-deterministic than other models. Maybe I&rsquo;m still completely wrong about the MoE part. Maybe it&rsquo;s just because of parameter count. (but then &ndash; why would Turbo be more unpredictable than davinci? Turbo&rsquo;s faster; if you assumed the same architecture, Turbo ought to be smaller)</li></ol><h2 id=implications>Implications</h2><p>It&rsquo;s actually pretty crazy to me, that this looks true. For a few reasons:</p><h3 id=were-so-far-behind>We&rsquo;re so far behind</h3><p><em>If</em> the non-determinism is an inherent feature of batched inference with Sparse MoE, then this fact should be visibly obvious to anyone that works with models in that vein.</p><p>Given that the vast majority of GPT-4 users still have no idea what is causing their API calls to be unreliable, it should be concluded that (I am completely wrong, OR) too few people know anything about MoE models to launch this explanation into the public consciousness.</p><p>It implies that Google Deepmind knew, and found it trivial enough to write as a throwaway sentence in a paper. It implies that I should be a lot more bullish on them, and a lot more bearish against every other wannabe foundation model org that&rsquo;s still working on dense models only.</p><h3 id=gpt-35-turbo-may-be-moe-too>GPT-3.5-Turbo may be MoE too</h3><p>I heard a <a href=https://sorry-not.leaking.sources.of.info target=_blank rel="noopener noreffer">rumour, once</a>, about 3.5-turbo sharing the <em>same architecture</em> as GPT-4; just with much much less parameters than it, or even GPT-3.</p><p>And, when I heard it, I was thinking: <em>Nah, that sounds too complicated for a small public model. Why wouldn&rsquo;t they just use a dense one? Fits on one GPU, no complexity overhead, really simple to optimise&mldr;</em></p><p>Fast forward to now, and we&rsquo;re still suffering a regime where it takes <a href=https://arxiv.org/abs/2307.09288 target=_blank rel="noopener noreffer">70B parameters to meet Turbo&rsquo;s performance</a> &ndash; a number which just <em>doesn&rsquo;t make sense</em> for how much traffic OpenAI&rsquo;s handling, and how much speed they get.</p><p>It&rsquo;s also easy to notice that Turbo is the only other model in the API that has its <a href=https://platform.openai.com/docs/api-reference/completions/create#completions/create-logprobs target=_blank rel="noopener noreffer">logprobs</a> restricted from public view. The common explanation was that they were restricted to <a href=https://arxiv.org/abs/2203.10163 target=_blank rel="noopener noreffer">prevent increased accuracy in distillation</a>, something which sounds a little bit naive today given <a href=https://arxiv.org/abs/2306.02707 target=_blank rel="noopener noreffer">Orca</a> and <a href=https://huggingface.co/datasets/Open-Orca/OpenOrca target=_blank rel="noopener noreffer">others</a>. OpenAI has also <a href=https://openai.com/blog/gpt-4-api-general-availability target=_blank rel="noopener noreffer">publicly stated</a> that they&rsquo;re working on getting the logprobs integrated into ChatCompletions, making &ldquo;prevent distillation&rdquo; less likely than &ldquo;this is hard to engineer reliably because they&rsquo;re inherently too random right now&rdquo;:</p><figure><img src=Pasted%20image%2020230805035805.png></figure><p>But, still, as I said earlier &ndash; not fully confident on this one. Maybe someone should open a <a href=https://manifold.markets/ target=_blank rel="noopener noreffer">prediction market</a>?</p><h2 id=conclusion>Conclusion</h2><ul><li>Everyone knows that OpenAI&rsquo;s GPT models are non-deterministic at temperature=0</li><li>It is typically attributed to non-deterministic CUDA optimised floating point op inaccuracies</li><li>I present a different hypothesis: <strong>batched inference in sparse MoE models are the root cause of most non-determinism in the GPT-4 API</strong>. I explain why this is a neater hypothesis than the previous one.</li><li>I empirically demonstrate that API calls to GPT-4 (and potentially some 3.5 models) are substantially more non-deterministic than other OpenAI models.</li><li>I speculate that GPT-3.5-turbo may be MoE as well, due to speed + non-det + logprobs removal.</li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on August 5, 2023&nbsp;<a class=git-hash href=https://github.com/152334H/blog/commit/0f51d5c406aa7fd346b5f747042454cce74e2b1f target=_blank title="commit by 152334H(54623771+152334H@users.noreply.github.com) 0f51d5c406aa7fd346b5f747042454cce74e2b1f: add hedging in case everything is wrong and the error is immortalized in internet history">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>0f51d5c</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://152334H.github.io/blog/non-determinism-in-gpt-4/ data-title="Non-determinism in GPT-4 is caused by Sparse MoE" data-hashtags="openai,machine learning,moe"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://152334H.github.io/blog/non-determinism-in-gpt-4/ data-hashtag=openai><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://152334H.github.io/blog/non-determinism-in-gpt-4/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://152334H.github.io/blog/non-determinism-in-gpt-4/ data-title="Non-determinism in GPT-4 is caused by Sparse MoE"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://152334H.github.io/blog/non-determinism-in-gpt-4/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://152334H.github.io/blog/non-determinism-in-gpt-4/ data-title="Non-determinism in GPT-4 is caused by Sparse MoE"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://152334H.github.io/blog/non-determinism-in-gpt-4/ data-title="Non-determinism in GPT-4 is caused by Sparse MoE"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/openai/>openai</a>,&nbsp;<a href=/tags/machine-learning/>machine learning</a>,&nbsp;<a href=/tags/moe/>moe</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/blog/dumped-blog-ideas/ class=prev rel=prev title="Dumped Blog Ideas"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Dumped Blog Ideas</a>
<a href=/blog/knowing-enough-about-moe/ class=next rel=next title="Knowing Enough About MoE to Explain Dropped Tokens in GPT-4">Knowing Enough About MoE to Explain Dropped Tokens in GPT-4<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=giscus class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app>Giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://152334H.github.io/ target=_blank>152334H</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/twemoji@14.0.2/dist/twemoji.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{giscus:{category:"Announcements",categoryId:"DIC_kwDOH03r284CQ2Tu",darkTheme:"dark_dimmed",emitMetadata:"0",inputPosition:"bottom",lang:"en",lazyLoading:!1,lightTheme:"light",mapping:"pathname",reactionsEnabled:"1",repo:"152334H/152334h.github.io",repoId:"R_kgDOH03r2w"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"},twemoji:!0}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W0STJ4D3N3",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-W0STJ4D3N3" async></script></body></html>