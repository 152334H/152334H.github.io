<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Knowing Enough About MoE to Explain Dropped Tokens in GPT-4 - 152334H</title><meta name=Description content="152334H Personal Blog"><meta property="og:title" content="Knowing Enough About MoE to Explain Dropped Tokens in GPT-4"><meta property="og:description" content="In a previous blogpost, I made a simple observation about GPT-4 from a paper I had incidentally read on a whim. After finishing the post, I realised I didn&rsquo;t actually ever figure out how token dropping could occur; only learning a black-box rule that it could occur in batched MoE inference for reasons.
This post is here to fix that &ndash; to collect enough info from important MoE papers (and alleged GPT-4 leaks) to explain the full mechanism of token dropping."><meta property="og:type" content="article"><meta property="og:url" content="https://152334H.github.io/blog/knowing-enough-about-moe/"><meta property="og:image" content="https://152334H.github.io/undefined.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-09T05:15:14+08:00"><meta property="article:modified_time" content="2023-08-09T05:40:08+08:00"><meta property="og:site_name" content="152334H"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://152334H.github.io/undefined.png"><meta name=twitter:title content="Knowing Enough About MoE to Explain Dropped Tokens in GPT-4"><meta name=twitter:description content="In a previous blogpost, I made a simple observation about GPT-4 from a paper I had incidentally read on a whim. After finishing the post, I realised I didn&rsquo;t actually ever figure out how token dropping could occur; only learning a black-box rule that it could occur in batched MoE inference for reasons.
This post is here to fix that &ndash; to collect enough info from important MoE papers (and alleged GPT-4 leaks) to explain the full mechanism of token dropping."><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://152334H.github.io/blog/knowing-enough-about-moe/><link rel=prev href=https://152334H.github.io/blog/non-determinism-in-gpt-4/><link rel=next href=https://152334H.github.io/blog/mixtral-vs-oss/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Knowing Enough About MoE to Explain Dropped Tokens in GPT-4","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/152334H.github.io\/blog\/knowing-enough-about-moe\/"},"genre":"posts","keywords":"openai, machine learning, moe","wordcount":2103,"url":"https:\/\/152334H.github.io\/blog\/knowing-enough-about-moe\/","datePublished":"2023-08-09T05:15:14+08:00","dateModified":"2023-08-09T05:40:08+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"152334H"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>All Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/><b>About </b></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>All Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title><b>About</b></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Knowing Enough About MoE to Explain Dropped Tokens in GPT-4</h1><h2 class=single-subtitle>Something slightly more technical</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://152334H.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>152334H</a></span>&nbsp;<span class=post-category>included in <a href=/categories/tech/><i class="far fa-folder fa-fw" aria-hidden=true></i>tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime="August 9, 2023">August 9, 2023</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;2103 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;10 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#moes-conceptually>MoEs, conceptually</a></li><li><a href=#routing-strategies>Routing strategies</a><ul><li><a href=#unhelpful-cutoff-dates-for-gpt-4>Unhelpful: cutoff dates for GPT-4</a></li><li><a href=#helpful-gpt-4-leaks>Helpful: GPT-4 leaks</a><ul><li><a href=#geohot-statement>Geohot statement</a></li><li><a href=#semianalysis>Semianalysis</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></li><li><a href=#the-process-of-token-choice>The process of token choice</a><ul><li><a href=#picking-a-token>Picking a token</a></li><li><a href=#capacity-factor>Capacity Factor</a><ul><li><a href=#why-not-route-to-the-k-th-most-likely-expert>Why not route to the k-th most likely expert?</a></li></ul></li><li><a href=#what-was-the-point-of-this-article-again>What was the point of this article, again?</a></li></ul></li><li><a href=#conclusion-1>Conclusion</a></li></ul></nav></div></div><div class=content id=content><p>In a previous blogpost, I made a <a href=/blog/non-determinism-in-gpt-4/ rel>simple observation</a> about GPT-4 from a <a href=https://arxiv.org/pdf/2308.00951.pdf target=_blank rel="noopener noreffer">paper</a> I had incidentally read on a whim. After finishing the post, I realised I didn&rsquo;t actually ever figure out how <em>token dropping</em> could occur; only learning a black-box rule that it could occur in batched MoE inference for <em>reasons</em>.</p><p>This post is here to fix that &ndash; to collect enough info from important MoE papers (and alleged GPT-4 leaks) to explain the full mechanism of token dropping.</p><h2 id=moes-conceptually>MoEs, conceptually</h2><p>A normal transformer layer looks like this:</p><a class=lightgallery href=/blog/knowing-enough-about-moe/Pasted%20image%2020230806223745.png title=/blog/knowing-enough-about-moe/Pasted%20image%2020230806223745.png data-thumbnail=/blog/knowing-enough-about-moe/Pasted%20image%2020230806223745.png><div style=text-align:center><img style=height:ZgotmplZpx;width:ZgotmplZpx class=lazyload src=/svg/loading.min.svg data-src=/blog/knowing-enough-about-moe/Pasted%20image%2020230806223745.png data-srcset="/blog/knowing-enough-about-moe/Pasted%20image%2020230806223745.png, /blog/knowing-enough-about-moe/Pasted%20image%2020230806223745.png 1.5x, /blog/knowing-enough-about-moe/Pasted%20image%2020230806223745.png 2x" data-sizes=auto alt=/blog/knowing-enough-about-moe/Pasted%20image%2020230806223745.png width=241 height=303></div></a><p><a href=https://lilianweng.github.io/posts/2021-09-25-train-large/ target=_blank rel="noopener noreffer">It becomes a Sparse Mixture-of-Experts block</a> when you replace the FFN layer with:</p><ul><li>$n$ feed-forward networks as experts ${E_i}^n_{i=1}$</li><li>A trainable <em>Router</em> that can map each token to a number of experts.</li></ul><figure><img src=Pasted%20image%2020230806223828.png alt="A Switch Transformer implementing a top-1 token choice router."><figcaption><p>A <em>Switch Transformer</em> implementing a <em>top-1 token choice</em> router.</p></figcaption></figure><p>Don&rsquo;t think of MoE as an ensemble/menagerie of domain experts; think of it as a novel approach for <strong>structured sparsity</strong> &ndash; the router gets to select <em>some</em> input tokens, directing each token to a fixed subset of all of the FFN weights.</p><p>There are a lot of extra details (differentiable load balancing loss, preventing representation collapse, extra tricks/hparams for training stability, etc) needed to make MoEs work at all, but for the purposes of this article, we only need to focus on one thing:</p><h2 id=routing-strategies>Routing strategies</h2><p>There are a few different routers that could&rsquo;ve been used for GPT-4:</p><ul><li>In <em><a href=https://arxiv.org/pdf/1701.06538.pdf target=_blank rel="noopener noreffer">Token Choice</a></em> routers, the router learns a probability distribution over $n$ experts, so as to route each token to a few (typically $1&lt;=k&lt;=2$) selected experts.</li><li>In <a href=https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html target=_blank rel="noopener noreffer"><em>Expert Choice</em></a> routers, the router selects the best <em>k</em> tokens for each expert.</li></ul><figure><img src=Pasted%20image%2020230809030919.png alt="Various token routing methods, as described here"><figcaption><p>Various token routing methods, as described <a href=https://arxiv.org/pdf/2209.01667.pdf target=_blank rel="noopener noreffer">here</a></p></figcaption></figure><figure><img src=Pasted%20image%2020230809034159.png></figure><p>We will <strong>ignore</strong> a number of other routing approaches, including:</p><ul><li>In <a href=https://arxiv.org/abs/2103.16716 target=_blank rel="noopener noreffer">BASE Layers</a>, token allocation is treated as a linear assignment problem &ndash; each expert gets a fixed number of tokens, and the optimal solution is something that doesn&rsquo;t involve dropping any tokens.</li><li><a href="https://ai.googleblog.com/2022/01/scaling-vision-with-sparse-mixture-of.html#:~:text=Priority%20Routing" target=_blank rel="noopener noreffer">Batch Prioritized Routing</a>, an improvement to Token Choice used in <a href=https://arxiv.org/pdf/2106.05974.pdf target=_blank rel="noopener noreffer">V-MoE</a>, <a href=https://arxiv.org/pdf/2202.08906.pdf target=_blank rel="noopener noreffer">cannot be used for decoder-only models</a></li><li>Effectively random routing methods, where tokens are probably not going to be dropped because random selection balances out well enough that some extra capacity should prevent dropping most of the time.
For example, in <a href=https://arxiv.org/abs/2106.04426 target=_blank rel="noopener noreffer">Hash Routing</a>, a hash function on the input token is used to decide what router it goes to. Similarly, <a href=https://arxiv.org/abs/2110.04260 target=_blank rel="noopener noreffer">THOR</a> randomly selects 2 experts per token.</li><li>In <a href=https://arxiv.org/pdf/2308.00951.pdf target=_blank rel="noopener noreffer">Soft MoE</a> routers, each token is assigned a per-slot dispatch weight, and each expert processes a few slots; no tokens are dropped under this paradigm either. I also find it unlikely OpenAI invented that (or any bespoke method) in-house 1.5 years ahead of time.</li><li><a href=https://arxiv.org/abs/2202.01169 target=_blank rel="noopener noreffer">Reinforcement Learning</a>, because the routing algorithm in GPT-4 is <a href=https://www.semianalysis.com/p/gpt-4-architecture-infrastructure target=_blank rel="noopener noreffer">allegedly quite simple</a></li></ul><p>So, we can roughly whittle down the options available to two: <em>Token Choice</em>, or <em>Expert Choice</em>. Can we do better?</p><h3 id=unhelpful-cutoff-dates-for-gpt-4>Unhelpful: cutoff dates for GPT-4</h3><p>GPT-4 was released on <a href=https://cdn.openai.com/papers/gpt-4.pdf target=_blank rel="noopener noreffer">27 Mar 2023</a>. They spent <a href=https://cdn.openai.com/papers/gpt-4-system-card.pdf target=_blank rel="noopener noreffer">six months</a> doing safety research prior to release, and allegedly took <a href=https://citation.needed target=_blank rel="noopener noreffer">three months</a> to train the base model. That implies that the latest date the architecture could have changed is around late <strong>June 2022</strong>, or basically any time period before 2022 Q3.</p><p>We also have a <em>lower bound</em> for the earliest date the GPT-4 architecture could&rsquo;ve stopped changing. Certain core contributors to GPT-4 were <a href=https://twitter.com/teortaxesTex/status/1671304991909326848 target=_blank rel="noopener noreffer">still writing papers for DeepMind</a> as late as <a href=https://arxiv.org/pdf/2202.01169.pdf target=_blank rel="noopener noreffer">Feb 2022</a>. A <a href=https://arxiv.org/pdf/2204.09179.pdf target=_blank rel="noopener noreffer">substantial</a> <a href=https://arxiv.org/pdf/2202.08906.pdf target=_blank rel="noopener noreffer">number</a> <a href=https://arxiv.org/pdf/2204.08396.pdf target=_blank rel="noopener noreffer">of</a> interesting papers were released in that time-period of early 2022, <strong>including (rather critically, for this article) <a href=https://arxiv.org/pdf/2202.09368.pdf target=_blank rel="noopener noreffer">Expert Choice</a></strong>. It&rsquo;s not immediately obvious which one of the ideas made it into the final product, so we&rsquo;ll have to dig deeper.</p><h3 id=helpful-gpt-4-leaks>Helpful: GPT-4 leaks</h3><p>Some information about GPT-4 has leaked to the public, over the months. I discuss the publicly known sources.</p><h4 id=geohot-statement>Geohot statement</h4><p>In the famous <a href=https://www.latent.space/p/geohot target=_blank rel="noopener noreffer">geohot interview</a> that lead to public hype about GPT-4&rsquo;s status as a Mixture of Experts, he stated that GPT-4 did &ldquo;16 inferences&rdquo;, something which was confirmed by <a href=https://twitter.com/soumithchintala/status/1671267150101721090 target=_blank rel="noopener noreffer">@soumithchintala</a>.</p><figure><img src=Pasted%20image%2020230809005454.png alt="&amp;lsquo;16 inferences&amp;rsquo;? What?"><figcaption><p>&lsquo;16 inferences&rsquo;? <em>What?</em></p></figcaption></figure><p>So, how can you make a model with 8 experts do inference 16 times?</p><ul><li>you could allocate each token to multiple ($N$) experts (okay)</li><li>you could repeat the forward pass a few ($M$) times (?)</li><li>and do that enough times that ($N * M = 16$) (???)</li></ul><p>Do I find this unlikely? Not entirely; it would go a long way to explain why GPT-4 is so slow. But there are a lot of other things that are off here, like:</p><ul><li>each &ldquo;head&rdquo; (attention head? expert model?) being 220B params each. that would never fit on a single H100, or even an <a href=https://www.anandtech.com/show/18780/nvidia-announces-h100-nvl-max-memory-server-card-for-large-language-models target=_blank rel="noopener noreffer">NVL H100 pair</a> without 4bit. Not to mention the insane batch sizes they&rsquo;d be operating at, or what <em>training</em> would be like.</li><li>Geohot stating they&rsquo;d spent &ldquo;eight times the money&rdquo; to get their model:<figure><img src=Pasted%20image%2020230809011042.png></figure>This is not how MoE works.</li></ul><p>So, let&rsquo;s discard this event, and move on to the better source:</p><h4 id=semianalysis>Semianalysis</h4><p>In contrast, we can have a much, <em>much</em> better narrative of what GPT-4 looks like from the <a href=https://www.semianalysis.com/p/gpt-4-architecture-infrastructure target=_blank rel="noopener noreffer">semianalysis</a> article about it. To avoid being <a href=https://twitter.com/dylan522p/status/1678557978637799425 target=_blank rel="noopener noreffer">sued</a> by Mr Patel, I will only include enough details to finish the main conclusions of this post. The relevant claims on GPT-4 are that,</p><ul><li>it has ~55B shared params for attention. I assume that this means their attention layers are normal <a href=https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055 target=_blank rel="noopener noreffer">dense MQA</a> stuff.</li><li>Each MLP expert is ~111B params. This matches up with <a href=https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters#How_many_parameters_are_spent_on_each_part_of_the_architecture_ target=_blank rel="noopener noreffer">what you&rsquo;d expect</a> for the MLP:attn ratio.</li><li><strong>each forward pass</strong> takes <strong>~280B params</strong> of a total of ~1.8T. <strong>2 experts</strong> are routed to per forward pass.</li><li>OpenAI&rsquo;s routing algorithms are &ldquo;allegedly quite simple&rdquo;. This is what I meant earlier, when I discarded RL routing as a possibility. Although maybe RL is simple for them? ðŸ¤”</li><li><strong>Experts receive an uneven number of tokens.</strong> Note the quote:<blockquote><p>&ldquo;it could mean 1 expert could be at a batch size of 8 and others could be at 4 or 1 or 0.&rdquo;</p></blockquote></li></ul><p>That last point is the most critical: it <strong>invalidates Expert Choice</strong> as an option, because in EC, every expert gets to pick K tokens (repeats are allowed).</p><h3 id=conclusion>Conclusion</h3><p>GPT-4 uses a simple top-2 Token Choice router for MLP MoE layers. It does not use MoE for attention.</p><hr><h2 id=the-process-of-token-choice>The process of token choice</h2><p><em>The bulk of this section is simply a verbal reconvening of information obtained via the <a href="https://www.youtube.com/watch?v=U8J32Z3qV8s" target=_blank rel="noopener noreffer">CS25 Lecture on Switch Transformers</a></em>. <em>I encourage you to watch it.</em></p><p>Now that we (roughly) know what routing strategy OpenAI is using internally, how can we explain the process of token dropping?</p><h3 id=picking-a-token>Picking a token</h3><p>MLP layers take in a list of tokens (tensor of input embeds) as input. Each token hits a Gating network, $G(x)$, that returns a list of probabilities ($g_x$) for each token being picked by an expert.</p><p>In top-2 Token Choice, the top 2 highest probability experts are selected, and the end result is calculated as,</p><p>$$ y = G(x)_iMLP_i(x) + G(x)_jMLP_j(x)$$</p><figure><img src=Pasted%20image%2020230809042016.png></figure><p>Although the training process is supposed to spread the input tokens across the experts as evenly as plausible, it&rsquo;s natural that certain inputs will cause some experts to be activated more than others (if not, why even have experts? shouldn&rsquo;t some experts do better than others&mldr;?).</p><p>In extreme cases, the same two experts might simply get <em>all tokens in the sequence</em> &ndash; or <em>in the batch</em>, if batch size > 1. GPT-4 can accept <em>32K tokens</em>; if all tokens in a batch of 32 were sent to the same expert, the effective token batch size on that single expert would be effectively <strong>>1million</strong>. In general, you could have $O(batchsize * tokens = BS_{tokens}$)$ inputs into one expert in one forward pass, if you&rsquo;re not careful.</p><div class="details admonition question"><div class="details-summary admonition-title"><i class="icon fas fa-question-circle fa-fw" aria-hidden=true></i>Wait, really?<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><p>I mean, is that really true? Can&rsquo;t you just, delay the requests, do some dynamic initalization of experts (more copies of certain experts than others to combat imbalance), split apart batches, or any other hotfix?</p><p>The other way to think of this is in historical terms. A lot of the initial work on MoE transformers was done at Google, where they use(d) Tensorflow/JAX and TPUs. The XLA compiler needs a static size for all tensors, unlike Pytorch where you could theoretically just take the dynamic inputs on an infinite vram computer. Expert Capacity was thus introduced to make token choice work where static shapes are required.</p></div></div></div><p>To prevent unstable performance, and CUDA OOM problems, most token choice MoE models implement an <strong>expert capacity</strong> &ndash; a static maximum number of tokens per expert. This is an adjustable hyperparameter that can be changed at <em>any given point of time</em> &ndash; at init, at any step, at fine-tuning, and even after training just for inference.</p><h3 id=capacity-factor>Capacity Factor</h3><p>In practice, the <strong>expert capacity</strong> ($EC$) is defined by a related hyperparameter, the <strong>capacity factor</strong> ($CF$):</p><p>$$ EC = round(CF * k * BS_{tokens} / E) $$</p><p>where top-$k$ experts (of which there are $E$) are selected per token. When $C&lt;1$, some tokens will always be dropped. When $C>1$, a <em>slack capacity</em> is added, and the frequency of dropped tokens is reduced.</p><p>In practice, certain inputs will rely on some experts more than others, and because allocating $C$ infinitely high is infeasible, <strong>some tokens will have to be dropped</strong>. When the number of tokens routed to an expert exceeds its CF, some tokens will simply be <strong>unprocessed</strong> by the MoE layer:</p><figure><img src=Pasted%20image%2020230806231319.png alt="Token dropping in Switch Transformers ($k=1$). Pretend there are double the number of arrows and columns in the experts for $k=2$."><figcaption><p>Token dropping in <em>Switch Transformers</em> ($k=1$). Pretend there are double the number of arrows and columns in the experts for $k=2$.</p></figcaption></figure><div class="details admonition info open"><div class="details-summary admonition-title"><i class="icon fas fa-info-circle fa-fw" aria-hidden=true></i>Info<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><p>If this is unintuitive to you (<em>where does the token go?</em>), remember that transformer layers do an add&norm after each FFN/Attn layer with the <em>previous input</em>, so &ldquo;dropping&rdquo; just makes the current layer a null-op for the dropped token:</p><div class=mermaid id=id-1></div><p>Sidenote: I think <em>unprocessed</em> is a better term than <em>dropped</em> for this same reason.</p></div></div></div><h4 id=why-not-route-to-the-k-th-most-likely-expert>Why not route to the k-th most likely expert?</h4><p>Instead of leaving tokens unprocessed, couldn&rsquo;t we at least <em>try</em> to process them by sending them to the 2nd, 3rd, <em>nth</em> most likely expert, as determined by the router?</p><figure><img src=Pasted%20image%2020230806232701.png></figure><p>This idea is known as <strong>No Token Left Behind</strong> in the literature, and well &ndash; it doesn&rsquo;t work.</p><blockquote><p>Interestingly, actually, this approach didn&rsquo;t empirically improve model performance. If anything, it actually kind of hurt it. And we thought that was actually very interesting.</p><p>And I think the intuition is that once the model learns it wants to send a token to one expert, it really wants to have that computation applied to it. And just applying some other computation doesn&rsquo;t have, at all, the same property, along with it, actually, maybe being potentially detrimental.</p><p>&ndash; <a href="https://www.youtube.com/watch?v=U8J32Z3qV8s" target=_blank rel="noopener noreffer">Barret Zoph</a></p></blockquote><p>MoE Transformers don&rsquo;t actually need need to process all tokens to work; having a fairly low CF with a substantial amount of token dropping is okay (and also better for less comm + vram):</p><p><figure><img src=Pasted%20image%2020230806234157.png></figure>In Switch Transformers, NLL is slightly worse on $CF=1.00$ than on $CF=1.25$, but it trains/infers much faster. We can assume the same for OpenAI&rsquo;s models &ndash; allowing some tokens to be dropped during inference is an <strong>efficient inference optimization</strong> that they don&rsquo;t have good reasons to fix.</p><h3 id=what-was-the-point-of-this-article-again>What was the point of this article, again?</h3><p>Oh, right.</p><p>Let&rsquo;s say I submit two calls to GPT-4:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s1>&#39;some weird prompt that tends to trigger close to random text&#39;</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>GPT4</span><span class=p>(</span><span class=n>s</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span> <span class=o>...</span> <span class=c1># some import openai stuff</span>
</span></span><span class=line><span class=cl><span class=n>s1</span> <span class=o>=</span> <span class=n>GPT4</span><span class=p>(</span><span class=n>prompt</span><span class=p>)</span> <span class=c1># prompt1</span>
</span></span><span class=line><span class=cl><span class=n>s2</span> <span class=o>=</span> <span class=n>GPT4</span><span class=p>(</span><span class=n>prompt</span><span class=p>)</span> <span class=c1># prompt2</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>s1</span> <span class=o>==</span> <span class=n>s2</span>
</span></span></code></pre></td></tr></table></div></div><p>And I get an assertion error. What happened? Well,</p><ol><li>OpenAI&rsquo;s servers get a lot of user inputs. (Surprising, I know)</li><li>They get so many inputs, that they tend to do <a href=https://www.anyscale.com/blog/continuous-batching-llm-inference target=_blank rel="noopener noreffer">batching</a>.</li><li><code>prompt1</code> and <code>prompt2</code> may be the same sequences, but they don&rsquo;t end up in the same <em>batch</em>.
As a toy example: <code>batch1 = ["hello", prompt]</code>, <code>batch2 = [prompt, "world"]</code></li><li>When a <strong>batch</strong> is processed by GPT-4, Expert Capacity is used to limit the tokens per expert <strong>from all sequences</strong> in the batch.</li><li>Your tokens have the same $g_x$ in both calls. Other sequences in the batch <strong>don&rsquo;t</strong>. Some of your tokens will get dropped in some calls, but not in others.</li><li>This changes the final output of the model.</li></ol><p>That&rsquo;s it.</p><h2 id=conclusion-1>Conclusion</h2><ol><li>Based on a literature review + leaked information, GPT-4 uses a top-2 Token Choice router.</li><li>Token Choice experts have an Expert Capacity that is often exceeded by the number of tokens routed to it within a batch.</li><li>Since the same sequence will appear in different batches, different tokens are dropped per API call, causing randomness in what is otherwise a deterministic model.</li><li>OpenAI will not immediately fix this &ndash; tokens being dropped are generally good for the performance of MoE models</li></ol></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on August 9, 2023&nbsp;<a class=git-hash href=https://github.com/152334H/blog/commit/b596076ab5f37c4f90edb8c0f555ea4ced43e7f8 target=_blank title="commit by 152334H(54623771+152334H@users.noreply.github.com) b596076ab5f37c4f90edb8c0f555ea4ced43e7f8: post: Knowing enough about MoE">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>b596076</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://152334H.github.io/blog/knowing-enough-about-moe/ data-title="Knowing Enough About MoE to Explain Dropped Tokens in GPT-4" data-hashtags="openai,machine learning,moe"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://152334H.github.io/blog/knowing-enough-about-moe/ data-hashtag=openai><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://152334H.github.io/blog/knowing-enough-about-moe/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://152334H.github.io/blog/knowing-enough-about-moe/ data-title="Knowing Enough About MoE to Explain Dropped Tokens in GPT-4"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://152334H.github.io/blog/knowing-enough-about-moe/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://152334H.github.io/blog/knowing-enough-about-moe/ data-title="Knowing Enough About MoE to Explain Dropped Tokens in GPT-4"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on å¾®åš" data-sharer=weibo data-url=https://152334H.github.io/blog/knowing-enough-about-moe/ data-title="Knowing Enough About MoE to Explain Dropped Tokens in GPT-4"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/openai/>openai</a>,&nbsp;<a href=/tags/machine-learning/>machine learning</a>,&nbsp;<a href=/tags/moe/>moe</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/blog/non-determinism-in-gpt-4/ class=prev rel=prev title="Non-determinism in GPT-4 is caused by Sparse MoE"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Non-determinism in GPT-4 is caused by Sparse MoE</a>
<a href=/blog/mixtral-vs-oss/ class=next rel=next title="Rough thoughts on Mixtral vs Open Source">Rough thoughts on Mixtral vs Open Source<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=giscus class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app>Giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2024</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://152334H.github.io/ target=_blank>152334H</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/twemoji@14.0.2/dist/twemoji.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/mermaid@9.1.3/dist/mermaid.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{giscus:{category:"Announcements",categoryId:"DIC_kwDOH03r284CQ2Tu",darkTheme:"dark_dimmed",emitMetadata:"0",inputPosition:"bottom",lang:"en",lazyLoading:!1,lightTheme:"light",mapping:"pathname",reactionsEnabled:"1",repo:"152334H/152334h.github.io",repoId:"R_kgDOH03r2w"}},data:{"id-1":`graph LR
    A[Token] --\u003e B(Router)
    A --\u003e D[Add \u0026 Norm]
    B --\u003e|Accepted| C[MLP Expert]
    C --\u003e D
    D --\u003e E[Output]`},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"},twemoji:!0}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W0STJ4D3N3",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-W0STJ4D3N3" async></script></body></html>