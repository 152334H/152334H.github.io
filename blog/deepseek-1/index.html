<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>DeepSeek Core Readings 1 - LLM - 152334H</title><meta name=Description content="152334H Personal Blog"><meta property="og:title" content="DeepSeek Core Readings 1 - LLM"><meta property="og:description" content="Paper summary: LLaMA-like 7B/67B pretrain (Base) + SFT&DPO (Chat). 2T tokens with strong CN/EN mix, >1mil SFT examples. Well-executed exploration of scaling laws. Good details about evals and safety. Not much described about their actual data."><meta property="og:type" content="article"><meta property="og:url" content="https://152334H.github.io/blog/deepseek-1/"><meta property="og:image" content="https://152334H.github.io/undefined.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-23T00:00:00+08:00"><meta property="article:modified_time" content="2024-06-29T22:46:20+08:00"><meta property="og:site_name" content="152334H"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://152334H.github.io/undefined.png"><meta name=twitter:title content="DeepSeek Core Readings 1 - LLM"><meta name=twitter:description content="Paper summary: LLaMA-like 7B/67B pretrain (Base) + SFT&DPO (Chat). 2T tokens with strong CN/EN mix, >1mil SFT examples. Well-executed exploration of scaling laws. Good details about evals and safety. Not much described about their actual data."><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://152334H.github.io/blog/deepseek-1/><link rel=prev href=https://152334H.github.io/blog/remain-conscious/><link rel=next href=https://152334H.github.io/blog/deepseek-0/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"DeepSeek Core Readings 1 - LLM","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/152334H.github.io\/blog\/deepseek-1\/"},"genre":"posts","keywords":"machine learning, DeepSeek, llm, series","wordcount":1552,"url":"https:\/\/152334H.github.io\/blog\/deepseek-1\/","datePublished":"2024-06-23T00:00:00+08:00","dateModified":"2024-06-29T22:46:20+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"152334H"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>All Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/><b>About </b></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>All Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title><b>About</b></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">DeepSeek Core Readings 1 - LLM</h1><h2 class=single-subtitle>Part 1/? of a <a href=/deepseek-core-readings>series</a></h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://152334H.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>152334H</a></span>&nbsp;<span class=post-category>included in <a href=/categories/tech/><i class="far fa-folder fa-fw" aria-hidden=true></i>tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime="June 23, 2024">June 23, 2024</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1552 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;8 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><ul><li><a href=#dataset>Dataset</a></li><li><a href=#arch>Arch</a></li><li><a href=#infra>Infra</a></li><li><a href=#scaling-experiments>Scaling experiments</a></li><li><a href=#alignment>Alignment</a></li><li><a href=#evals>Evals</a><ul><li><a href=#various-notes-on-results>Various Notes on Results</a></li><li><a href=#on-held-out-evals>On held out evals</a></li><li><a href=#safety>Safety</a></li><li><a href=#on-mcq-evals>On MCQ Evals</a></li><li><a href=#on-instruction-data-in-pre-training>On instruction data in pre-training</a></li><li><a href=#on-sysprompt>On sysprompt</a></li></ul></li><li><a href=#other-things>Other things</a></li></ul></li></ul></nav></div></div><div class=content id=content><p><a href=https://arxiv.org/abs/2401.02954 target=_blank rel="noopener noreffer">Paper</a> summary: LLaMA-like 7B/67B pretrain (Base) + SFT&DPO (Chat). 2T tokens with strong CN/EN mix, >1mil SFT examples. Well-executed exploration of scaling laws. Good details about evals and safety. Not much described about their actual data.</p><h3 id=dataset>Dataset</h3><p>They note that CC deduplication removes a lot more content iff done across dumps rather than within individual dumps:</p><figure><img src=Pasted%20image%2020240623132834.png></figure><p>For the unaware reader: CC archives are split by month (approximately); see <a href=https://data.commoncrawl.org/crawl-data/index.html target=_blank rel="noopener noreffer">this index</a>. So it is pretty unsurprising that there are a lot of duplicate entries across time.</p><p>I wonder if a substantial number of pretraining runs in the wild got fucked over by this implicit data-duplication, though.</p><h3 id=arch>Arch</h3><p>100k BPE tokenizer that prevents newlines/punctuation/CJK symbols from merging with one another (I don&rsquo;t know how to implement that and should probably find out), and also keeps digits as singular unmerged tokens. They train the tokenizer on 24GB of text (are there scaling laws for this?) and add 15 special tokens and pad to 102400.</p><p>All models use LLaMA/Noam arch + 4k ctxlen, and AdamW $\beta_1=0.9, \beta_2=0.95$ w/ 0.1 weight decay & grad clip 1.0</p><ul><li>7B: <code>L=30 D=4096 H=32 Hkv=32</code> <code>bs=2304 lr=4.2e-4</code></li><li>67B: <code>L=95 D=8192 H=64 Hkv=8</code> <code>bs=4608 lr=3.2e-4</code></li></ul><p>They init <strong>both</strong> models with stddev 0.006. They do not specify what distribution this is, but in several pretraining toolkits this usually refers to $\mathcal{N}(0, \sigma^2)$.</p><p>They state that <code>L=30/95</code> was picked specifically to &ldquo;facilitate model pipeline partitioning to optimize training and inference.&rdquo; That is a bit weird to me because $95 = 19*5$. A pipeline hardcoded for 5 nodes only? If you include the embedding layer, then $L=30+1$ would also be prime. <a href=https://www.lesswrong.com/s/zpCiuR4T343j9WkcK target=_blank rel="noopener noreffer">Confusing</a>.</p><p>For the LR scheduler, they use some bespoke multi-step (step as in cliff-like) thing, which <em>should</em> be implemented as</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_lr</span><span class=p>(</span><span class=n>step</span><span class=p>,</span> <span class=n>max_steps</span><span class=p>,</span> <span class=n>max_lr</span><span class=p>,</span> <span class=n>warmup_steps</span><span class=o>=</span><span class=mi>2000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>step</span> <span class=o>&lt;</span> <span class=n>warmup_steps</span><span class=p>:</span> <span class=k>return</span> <span class=n>max_lr</span> <span class=o>*</span> <span class=p>(</span><span class=n>step</span> <span class=o>/</span> <span class=n>warmup_steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>step</span> <span class=o>&lt;</span> <span class=mf>0.8</span> <span class=o>*</span> <span class=n>max_steps</span><span class=p>:</span> <span class=k>return</span> <span class=n>max_lr</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>step</span> <span class=o>&lt;</span> <span class=mf>0.9</span> <span class=o>*</span> <span class=n>max_steps</span><span class=p>:</span> <span class=k>return</span> <span class=mf>0.316</span> <span class=o>*</span> <span class=n>max_lr</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mf>0.1</span> <span class=o>*</span> <span class=n>max_lr</span>
</span></span></code></pre></td></tr></table></div></div><p>Motivation is similar to WSD and etc: training can be extended infinitely from the 80% mark as the LR will not have gone down yet.</p><h3 id=infra>Infra</h3><p>Internal <a href=https://www.high-flyer.cn/en/blog/hai-llm/ target=_blank rel="noopener noreffer">HAI-LLM</a> harness does basic 1F1B-PP/TP/SP + ZeRO + flash-attn + xentropy + other obvious stuff. They accumulate grads in fp32.</p><p>Their checkpointing is decent: async checkpointing every 5 minutes + resumption to different parallelism configs.</p><p>They use VLLM for inference.</p><h3 id=scaling-experiments>Scaling experiments</h3><p>Instead of using the traditional $C=6ND$, they do $C=MD$, where $M = 72L\cdot D^2 + 12L\cdot D\cdot l_{seq}$ is the <strong>non-embedding FLOPs per token</strong>. This accounts for SDPA flops and ignores embedding layer.</p><p>So they do some isoflops scaling experiments on [batch size, lr, compute budget $C$]:</p><ol><li><p>a grid search for optimal BS/LR @ 1e17 / 1e20 FLOPs:<figure><img src=Pasted%20image%2020240623140634.png></figure></p></li><li><p>Continuing from the 1e17 FLOPs run, they explore many continued training experiments to $C \in [1e17, 2e19]$, varying BS/LR again, to plot a graph of all the runs that have a final loss within 0.25% of the minimum. Conclusions in the blurb:<figure><img src=Pasted%20image%2020240623141322.png></figure></p></li><li><p>Using the obtained optimal BS / LRs, they do chinchilla-like experiments to find $argmin\ L(N,D)$ for varying $C=MD$:<figure><img src=Pasted%20image%2020240623143010.png></figure>Note that $L$ is defined as <a href=https://arxiv.org/pdf/2101.00027#page6 target=_blank rel="noopener noreffer">bits-per-byte</a> (rather than CELoss) on a validation set of 100M tokens.</p><p>For their dataset, $M_{opt} = 0.1715 * C^{0.5243}$ and $D_{opt} = 5.8316 * C^{0.4757}$.</p><p>In the appendix, they note that Chinchilla&rsquo;s original $C=6PD$ law leads to performance overestimation/underestimation, depending whether embedding params are included in $P$ or not.</p></li><li><p>They find that better data increases the ideal value of $M$ for fixed $C$.<figure><img src=Pasted%20image%2020240623143536.png></figure>I have contentions with their intuitions as described above, but the empirical results are important.</p></li></ol><p>Note that:</p><ul><li><p>they never mention the param initialisation used for any of their experiments.</p><p>Presumably, there was no mUP-like initialisation scaling involved, and they used $\mathcal{N}(0, 0.006^2)$ for everything. Or maybe they used default torch init?</p></li><li><p>they never state how they determine which of $L,D$ to vary (or how much to vary) when $M$ is changed. Kaplan 2020 implies it shouldn&rsquo;t matter too much whether they varied L or D more, though.</p></li><li><p>the equation for $M$ only makes sense if you have no attention mask && you do MHA && have a constant 8/3 FFN ratio w/ GLU (or a FFN ratio of 4 for a non-GLU model) && you ignore lmhead FLOPs.</p><p>If you parameterize $R_{kv} = \frac{\text{k or v heads}}{\text{q heads}}$ and $R_{\text{ffn}} = 4$, you should instead have</p><ul><li>$M_{\text{ffn}} = 2\cdot D\cdot D\cdot R_{\text{ffn}}\cdot 6 = 12 \cdot R_{\text{ffn}} \cdot D^2$ and</li><li>$M_{\text{att}} = 2\cdot D\cdot D\cdot (1 + R_{kv})\cdot 6 = 12 \cdot (1 + R_{kv}) \cdot D^2$</li><li>$M_{\text{sdpa}} \approx 6\cdot D\cdot L\cdot l_{seq}$ for causal attention</li><li>$M_{\text{lmhead}} = V \cdot D \cdot 6$ for an output vocab of size $V$.</li></ul><p>for a total of $M = 12D^2L(1 + R_{kv} + R_{ffn}) + 6DL\cdot l_{seq} + 6DV$</p><p>$= 6D(2DL(1+R_{kv}+R_{ffn}) + L\cdot l_{seq} + V)$.</p></li></ul><p>In any case, their predictions hold up to 67B scale, so I don&rsquo;t have a good empirical reason to complain.</p><h3 id=alignment>Alignment</h3><p>1.5 mil CN+EN instructions. 20% of those are for safety refusals; of the remaining 80% they have around $\frac{2}{9}$ code $\frac{4}{9}$ maths $\frac{1}{3}$ language tasks.</p><p>They do 4 epoches $lr=10^{-5}$ for 7B and 2 epoches $lr=5*10^{-6}$ for 67B (avoid overfit).</p><p>Interestingly, they notice that their SFT model tended to <strong>degrade into repetition more often as the quantity of math SFT data increased</strong>, allegedly because their math data repeats information to do reasoning. To fix this, they do a 2nd fine-tuning round on their 7B model with only conversational data (and no math/code); this doesn&rsquo;t negatively affect their HumanEval/GSM8K results.</p><p>They do DPO w/ lr 5e-6 and $BS=512$ and warmup cosine scheduler. Dataset is collected prompts &ndash;> SFT model generations &ndash;> ratings.</p><h3 id=evals>Evals</h3><p>They use an internal eval framework (no details) on MMLU/C-Eval/CMMLU/HellaSwag/PIQA/ARC/OpenBookQA/BBH/TriviaQA/NaturalQuestions/RACE/DROP/C3/WinoGrande/CLUEWSC/Pile/CHID/CCPM/GSM8K/MATH/CMath/HumanEval/MBPP/AGIEval.</p><p>For MCQ evals, they mostly use (token?) <a href=https://blog.eleuther.ai/multiple-choice-normalization/ target=_blank rel="noopener noreffer">length-normalized completion probabilities</a>, except for ARC/OpenBookQA where they use unconditional normalization.</p><p>For generation-based tasks, they use greedy decoding. They also do BPB evaluation on Pile-test.</p><h4 id=various-notes-on-results>Various Notes on Results</h4><p>They note that their models perform similarly to LLaMA 2 on English benchmarks, despite having much less English data. Their 67B model beats l2-70B more than their 7B model beats l2-67B, possibly implying that dual CN/EN language modelling has a more painful impact for smaller models.</p><p>They give arguments for why changes in TriviaQA/MMLU/C-Eval/BBH/NaturalQuestions perf between Base and Chat models are not indicative of &ldquo;acquisition or loss of knowledge&rdquo; or &ldquo;learn[ing] reasoning capabilities&rdquo; after SFT, but are just learning the correct formatting / zero-shot behaviour.</p><p>They note that Base models are naturally better at direct cloze/completion tasks like HellaSwag, and that their Chat model performs much better on math/coding tasks, presumably learned through the SFT data.</p><p>Ritualistically, they repeatedly note that their model outperforms ChatGPT 3.5 and is only second to GPT-4 on evals XYZ, because every pretraining org has to say that for some reason.</p><p>For open-ended evaluation, they use <a href=https://github.com/THUDM/AlignBench target=_blank rel="noopener noreffer">AlignBench</a> for Chinese, optimising the generation temperature used depending on domain. For English, they use <a href=https://arxiv.org/pdf/2306.05685 target=_blank rel="noopener noreffer">MT-Bench</a>. Both of these involve GPT-4 dogfooding.</p><h4 id=on-held-out-evals>On held out evals</h4><p>They use novel LeetCode weekly contest problems to test the coding capabilities of the model. They say,</p><blockquote><p>The model’s coding capabilities are depicted in the Figure below, where the y-axis represents the pass@1 score on in-domain human evaluation testing, and the x-axis represents the pass@1 score on out-domain LeetCode Weekly Contest problems.</p></blockquote><p><strong>However, this Figure does not actually exist in the paper.</strong> Neither does it exist in the Deepseek Coder paper. <strong>To the dear reader</strong>: Please contact me / leave a comment if you are able to find it.</p><p>Anyway: They find that their model outperforms everything other than GPT-4 on LeetCode/Hungarian Exam/IFEval.</p><h4 id=safety>Safety</h4><p>They take safety seriously enough to develop a taxonomy of unsafe topics, ensuring that their model is well-rounded and does not fall to basic jailbreaks/loopholes. They have an &ldquo;expert team&rdquo; dedicated to developing a high quality diverse safety test set of 2.4k questions, involving &ldquo;inducement, role-playing, multi-turn dialogues, preset positions, and etc&rdquo;, and create a guideline constitution for their safety reviews.</p><p>They are better than GPT-4/ChatGPT (&mldr;) on <a href=https://arxiv.org/pdf/2308.13387 target=_blank rel="noopener noreffer">Do-Not-Answer</a> by 0.1%. Thankfully, they do not obtain l2-7b-chat&rsquo;s performance of 99.4%.</p><h4 id=on-mcq-evals>On MCQ Evals</h4><p>DeepSeek discovered that their MCQ-based evals are drastically improved when they add 20M multiple-choice Chinese questions to their alignment dataset:</p><p><figure><img src=Pasted%20image%2020240623153313.png></figure>Heroically, they chose to <strong>exclude MC data</strong> from their final training runs, as they believe it is merely benchmark-fitting and not representative of true model intelligence.</p><h4 id=on-instruction-data-in-pre-training>On instruction data in pre-training</h4><p>Some labs include SFT data in pre-training to improve benchmarks. DeepSeek tested between:</p><ul><li>adding 5M instructions (i.e. ~1M SFT + ~4M MCQ) to the last 10% of pre-training</li><li>not adding and then doing 5M SFT later
and found the benchmark results were ~thesame.</li></ul><p>Adding instruction data during pre-training is reasonable if the dataset is big enough, but because they only use 1.5M instructions in the end, they don&rsquo;t.</p><h4 id=on-sysprompt>On sysprompt</h4><p>On MT Bench, their 7B model is slightly worse (-0.04) with a sysprompt, but their 67B model is substantially better (+0.23) with one.</p><h3 id=other-things>Other things</h3><p>DeepSeek 1.0 was released over 6 months ago. In the conclusion, they promised:</p><p><figure><img src=Pasted%20image%2020240623154206.png></figure>Today, it is incredibly impressive to see how they&rsquo;ve delivered on this vision with DeepSeek-Coder V2.</p><p>&mldr;</p><p>Also, this is the sole and only underlined segment in their entire paper: <img class=lazyload src=/svg/loading.min.svg data-src=/blog/deepseek-1/Pasted%20image%2020240623132544.png data-srcset="/blog/deepseek-1/Pasted%20image%2020240623132544.png, /blog/deepseek-1/Pasted%20image%2020240623132544.png 1.5x, /blog/deepseek-1/Pasted%20image%2020240623132544.png 2x" data-sizes=auto alt=/blog/deepseek-1/Pasted%20image%2020240623132544.png title=/blog/deepseek-1/Pasted%20image%2020240623132544.png width=235 height=62></p><p>Amusing.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on June 29, 2024&nbsp;<a class=git-hash href=https://github.com/152334H/blog/commit/d7afce7b0bdc875bac6361c20d67f16a9d45af48 target=_blank title="commit by 152334H(54623771+152334H@users.noreply.github.com) d7afce7b0bdc875bac6361c20d67f16a9d45af48: deepseek main page update">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>d7afce7</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://152334H.github.io/blog/deepseek-1/ data-title="DeepSeek Core Readings 1 - LLM" data-hashtags="machine learning,DeepSeek,llm,series"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://152334H.github.io/blog/deepseek-1/ data-hashtag="machine learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://152334H.github.io/blog/deepseek-1/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://152334H.github.io/blog/deepseek-1/ data-title="DeepSeek Core Readings 1 - LLM"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://152334H.github.io/blog/deepseek-1/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://152334H.github.io/blog/deepseek-1/ data-title="DeepSeek Core Readings 1 - LLM"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://152334H.github.io/blog/deepseek-1/ data-title="DeepSeek Core Readings 1 - LLM"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/machine-learning/>machine learning</a>,&nbsp;<a href=/tags/deepseek/>DeepSeek</a>,&nbsp;<a href=/tags/llm/>llm</a>,&nbsp;<a href=/tags/series/>series</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/blog/remain-conscious/ class=prev rel=prev title="Basic tips for remaining conscious"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Basic tips for remaining conscious</a>
<a href=/blog/deepseek-0/ class=next rel=next title="DeepSeek Core Readings 0 - Coder">DeepSeek Core Readings 0 - Coder<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=giscus class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app>Giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2024</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://152334H.github.io/ target=_blank>152334H</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/twemoji@14.0.2/dist/twemoji.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{giscus:{category:"Announcements",categoryId:"DIC_kwDOH03r284CQ2Tu",darkTheme:"dark_dimmed",emitMetadata:"0",inputPosition:"bottom",lang:"en",lazyLoading:!1,lightTheme:"light",mapping:"pathname",reactionsEnabled:"1",repo:"152334H/152334h.github.io",repoId:"R_kgDOH03r2w"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"},twemoji:!0}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W0STJ4D3N3",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-W0STJ4D3N3" async></script></body></html>