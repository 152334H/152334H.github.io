<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Rough thoughts on Mixtral vs Open Source - 152334H</title><meta name=Description content="152334H Personal Blog"><meta property="og:title" content="Rough thoughts on Mixtral vs Open Source"><meta property="og:description" content="Here&rsquo;s a thesis (hypothesis, predicate, etc) to chew on:

The mixture-of-experts paradigm is fundamentally a hinderance to open source development, and mixtral-8x5B+2B will be summarily supplanted by a dense model like llama3/mistral-70b/yi/qwen/&mldr; in the near future.
"><meta property="og:type" content="article"><meta property="og:url" content="https://152334H.github.io/blog/mixtral-vs-oss/"><meta property="og:image" content="https://152334H.github.io/undefined.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-12-13T20:12:34+08:00"><meta property="article:modified_time" content="2023-12-13T20:46:28+08:00"><meta property="og:site_name" content="152334H"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://152334H.github.io/undefined.png"><meta name=twitter:title content="Rough thoughts on Mixtral vs Open Source"><meta name=twitter:description content="Here&rsquo;s a thesis (hypothesis, predicate, etc) to chew on:

The mixture-of-experts paradigm is fundamentally a hinderance to open source development, and mixtral-8x5B+2B will be summarily supplanted by a dense model like llama3/mistral-70b/yi/qwen/&mldr; in the near future.
"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://152334H.github.io/blog/mixtral-vs-oss/><link rel=prev href=https://152334H.github.io/blog/knowing-enough-about-moe/><link rel=next href=https://152334H.github.io/blog/2023/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Rough thoughts on Mixtral vs Open Source","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/152334H.github.io\/blog\/mixtral-vs-oss\/"},"genre":"posts","keywords":"machine learning, llms, moe","wordcount":1261,"url":"https:\/\/152334H.github.io\/blog\/mixtral-vs-oss\/","datePublished":"2023-12-13T20:12:34+08:00","dateModified":"2023-12-13T20:46:28+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"152334H"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>All Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/><b>About </b></a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=152334H>Simple Thoughts</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>All Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title><b>About</b></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Rough thoughts on Mixtral vs Open Source</h1><h2 class=single-subtitle>Draft post, incomplete, epistemtically uncertain, etc</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://152334H.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>152334H</a></span>&nbsp;<span class=post-category>included in <a href=/categories/tech/><i class="far fa-folder fa-fw" aria-hidden=true></i>tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime="December 13, 2023">December 13, 2023</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1261 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;6 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#fine-tuning-difficulties>Fine-tuning difficulties</a><ul><li><a href=#fine-tuning-will-become-easier>Fine-tuning will become easier</a></li></ul></li><li><a href=#the-vram-problem>The vRAM problem</a><ul><li><a href=#does-not-apply-for-some-users>Does not apply for some users</a></li><li><a href=#is-currently-not-even-true>Is currently not even true</a></li><li><a href=#will-be-defeated-soon>Will be defeated soon</a></li></ul></li><li><a href=#what-will-not-work>What will not work</a><ul><li><a href=#offloading>Offloading</a></li></ul></li><li><a href=#final-note>Final note</a></li></ul></nav></div></div><div class=content id=content><p>Here&rsquo;s a <strong>thesis</strong> (hypothesis, predicate, etc) to chew on:</p><blockquote><p>The mixture-of-experts paradigm is fundamentally a hinderance to open source development, and mixtral-8x5B+2B will be summarily supplanted by a dense model like <a href=https://twitter.com/futuristflower/status/1716555972452184463 target=_blank rel="noopener noreffer">llama3</a>/<a href=https://techcrunch.com/2023/11/09/theres-something-going-on-with-ai-startups-in-france/ target=_blank rel="noopener noreffer">mistral-70b</a>/yi/qwen/&mldr; in the near future.</p></blockquote><p>The case in favor of the take is pretty simple, involving points like,</p><ul><li><a href rel>increased fine-tuning difficulty</a> vs dense models</li><li>most consumers being more vram limited than anything else</li><li>expectation that mistral&rsquo;s capabilities are easily replicable via <a href=https://arxiv.org/abs/2305.16264 target=_blank rel="noopener noreffer">multi-epoch training</a></li></ul><p>The rest of this article will be dedicated to finding counterarguments / surprise outcomes that may disrupt this narrative.</p><hr><h2 id=fine-tuning-difficulties>Fine-tuning difficulties</h2><p>As of current writing, mixtral QLoRA experiments are showing fairly bad results, with people reporting <a href=https://old.reddit.com/r/LocalLLaMA/comments/18gz54r/llm_comparisontest_mixtral8x7b_mistral_decilm/ target=_blank rel="noopener noreffer">odd behaviours on models</a> or plainly seeing loss curves explode:</p><figure><a class=lightgallery href=https://i.imgur.com/aaaaaaa.jpg title=https://i.imgur.com/aaaaaaa.jpg data-thumbnail=https://i.imgur.com/aaaaaaa.jpg data-sub-html="<h2>this is a placeholder image</h2>"><div style=text-align:center><img style=height:20%;width:20% style=height:ZgotmplZpx;width:ZgotmplZpx class=lazyload src=/svg/loading.min.svg data-src=https://i.imgur.com/aaaaaaa.jpg data-srcset="https://i.imgur.com/aaaaaaa.jpg, https://i.imgur.com/aaaaaaa.jpg 1.5x, https://i.imgur.com/aaaaaaa.jpg 2x" data-sizes=auto alt=https://i.imgur.com/aaaaaaa.jpg></div></a><figcaption class=image-caption>this is a placeholder image</figcaption></figure><p>This is the kind of thing you&rsquo;d expect in advance for a variety of reasons &ndash; most of which are covered in papers like <a href=https://arxiv.org/pdf/2202.08906.pdf target=_blank rel="noopener noreffer">ST-MOE</a> &ndash; including:</p><ul><li>lack of auxiliary/balancing/z-loss. people who chuck in MixtralForCausalLM into normal dense model trainers will end up with <a href=https://github.com/huggingface/transformers/blob/680c610f9733561fb09e21c0e6ed513d69564f56/src/transformers/models/mixtral/modeling_mixtral.py#L1243 target=_blank rel="noopener noreffer">no auxiliary loss</a> &ndash; <code>output_router_logits</code> is <a href=https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/commit/3de0408ae8b591d9ac516a2384925dd98ebc66f4 target=_blank rel="noopener noreffer">disabled by default</a> &ndash; much less additional stabilisers like <a href rel>router z-loss</a>:
<img class=lazyload src=/svg/loading.min.svg data-src=./Pasted%20image%2020231213170124.png data-srcset="./Pasted%20image%2020231213170124.png, ./Pasted%20image%2020231213170124.png 1.5x, ./Pasted%20image%2020231213170124.png 2x" data-sizes=auto alt=./Pasted%20image%2020231213170124.png title=./Pasted%20image%2020231213170124.png></li><li>lack of MoE-training-specific hyperparameters like Capacity Factor, Expert Dropout, routing jitter, etc.</li><li>Batch sizes being too small. The <a href=https://huggingface.co/blog/moe target=_blank rel="noopener noreffer">HF MoE Blog</a> notes that the ST-MOE paper recommends &ldquo;small&rdquo; batch sizes&mldr;<figure><img src=Pasted%20image%2020231213173422.png></figure>&mldr;but &ldquo;small&rdquo; in this case refers to a token batch size of 65k tokens (or a sequence batch size of <strong>~128</strong>, since this is for T5 which has <code>ctxlen==512</code>). Most consumer LoRA runs do not have a batch size that large.</li><li>router being trained in half precision at all.</li><li>having a LoRA adapter over the routing layers at all. Each <code>layers.[0..32].ffn.gate</code> layer in mixtral is a <code>Linear(dim,8)</code>; training it a LoRA with $r>=8$ creates an adapter <em>bigger than the gate itself</em>:<figure><img src=Pasted%20image%2020231213171556.png></figure>I am not quite sure what the effect of this is yet, but I suspect it is not a good thing to be doing.</li></ul><p>Most of these issues are engineering issues that <em>can</em> be solved with time.</p><p>Emphasis on <strong>can</strong>: it is always <em>possible</em> to make things work (&ldquo;Neural networks want to work&rdquo;), but attention in the open source community is fickle, and each failed training run strikes against the momentum of Mixtral&rsquo;s release. It doesn&rsquo;t help that mistral improved their alignment recipe for Mixtral-Instruct &ndash; having a decent baseline makes it all the more difficult for fine-tuning hobbyists to produce a cool release.</p><p>But&mldr;</p><h3 id=fine-tuning-will-become-easier>Fine-tuning will become easier</h3><p>Despite problems, mixtral is an inherently strong model, and consequently, there are a lot of people trying to make it work. I expect many, if not all of the above issues to be addressed within a week.</p><p>Additionally, many of the above tricks for MoE fine-tuning stability may prove themselves to be unnecessary over time.</p><ul><li>Plausibly, QLoRA itself will act as some form of regularization / divergence penalty that helps to keep MoE fine-tuning more stable than it has been reported to be in the past.</li><li>There is a large space of dumb ideas that may &ldquo;just work&rdquo;, e.g. freezing the routers, or forcing expert randomization, or throwing compute at hparam search, or&mldr;</li><li>In general, having a learned bias against unreplicated academic papers that say &ldquo;This is the way things should be done&rdquo;.</li></ul><hr><h2 id=the-vram-problem>The vRAM problem</h2><p>There are many axes of comparison to flatter MoE models with. On <em>steps-to-reach-loss</em>, <em>FLOPs-to-reach-benchmark</em>, inference speed, etc, the mixture-of-experts paradigm gets anywhere between 2x~5x improvement over an appropriate dense baseline.</p><p><strong>vram-to-loss</strong> is not one of those axes. <em>Fundamentally</em>, the mixture-of-experts paradigm is <strong>unfriendly to high FLOPs & low vRAM developers</strong>, a condition which adequately describes the vast majority of the open LLM community, operating on consumer hardware.</p><p>Given two models &ndash; a dense model with <em>P</em> parameters, and an MoE model with <em>P</em> total parameters &ndash; trained on the same dataset <em>D</em>, the dense model will always win 100% of the time. This is often misunderstood in mind-numbing directions (e.g. &ldquo;MoE models train worse&rdquo;), but for the purposes of an end user that <em>receives a model checkpoint</em> and is planning on <em>running at batch size 1</em>, the accusation of MoE being useless is difficult to shake.</p><p>But&mldr;</p><h3 id=does-not-apply-for-some-users>Does not apply for some users</h3><p>Even if MoE models are a poor fit for most NVIDIA/AMD consumers, they are still generally useful for other local users; primarily CPU && Apple Silicon devs, e.g. It is currently possible to obtain ~60tok/s on an M2 Ultra on the <a href=https://github.com/ggerganov/llama.cpp/pull/4406 target=_blank rel="noopener noreffer">llama.cpp fork for mixtral</a>. In general, local MoE models are a good fit for people who have more RAM than they know what to do with, and who have too few FLOPs or memory bandwidth to utilize a dense model large enough to occupy their entire memory.</p><p>Also, some open source companies <em>are</em> big enough to afford datacenters. For those people, MoE models are a pareto improvement on memory bandwidth || FLOPs required for capabilities, and it&rsquo;s an obvious benefit to be using them when decent integrations with e.g. <a href=https://github.com/vllm-project/vllm target=_blank rel="noopener noreffer">vllm</a> already exist.</p><h3 id=is-currently-not-even-true>Is currently not even true</h3><p>It&rsquo;s also <em>currently not even the case</em> that we have a dense model outperforming Mixtral while being the same size (or less) than it. None of the other open LLM releases in 2023 have matched up with Mistral&rsquo;s sheer parameter efficiency, and it&rsquo;s debatable whether we should expect any dense releases <code>&lt;=40B</code> to beat Mixtral soon.</p><p><em>Personally</em>, I expect the vast majority of Mistral&rsquo;s capabilities to be the result of <a href=https://arxiv.org/abs/2305.16264 target=_blank rel="noopener noreffer">multi-epoch</a> overtraining, which necessarily leads me to expect a dense baseline beating Mixtral soon. But this is an expectation that could be disproven with time.</p><h3 id=will-be-defeated-soon>Will be defeated soon</h3><p>I find it reasonable to expect that MoE models will have a better &ldquo;theoretical compressibility&rdquo; than their dense counterparts. Sparse-but-undertrained models like <a href=https://huggingface.co/google/switch-c-2048 target=_blank rel="noopener noreffer">switch-c-2048</a> or <a href=https://github.com/facebookresearch/fairseq/blob/main/examples/moe_lm/model_card.md target=_blank rel="noopener noreffer">fairseq&rsquo;s</a> have been <a href=https://arxiv.org/abs/2310.16795 target=_blank rel="noopener noreffer">empirically demonstrated</a> to be compressible to sub-1-bit precision, although it&rsquo;s far less likely that an overtrained model like Mixtral will be equally compressible.</p><p>There are also <a href=https://twitter.com/georgejrjrjr/status/1734633927275380861 target=_blank rel="noopener noreffer">ongoing efforts</a> by the bitsandbytes developer, Tim Dettmers, to drastically sparsify MoE experts. And there are various hacky efforts by the OSS community at large to try to merge expert layers via existing techniques.</p><p>We can imagine some scale of compressibility &ndash; one that starts with &ldquo;no better than dense 4bit quants&rdquo;, and one that ends with &ldquo;we perfectly merged this back into a 12B model&rdquo;:</p><figure><img src=line.png alt="forgive the bad handwriting please"><figcaption><p>forgive the bad handwriting please</p></figcaption></figure><p>We will probably land somewhere in the white chunk of the line, and we will probably figure out some vram-saving hacks that come at a partial performance hit.</p><h2 id=what-will-not-work>What will not work</h2><h3 id=offloading>Offloading</h3><p>Complete offloading will not work. It may be possible to solve the <em>latency</em> of expert fetching via <a href=https://arxiv.org/abs/2308.12066 target=_blank rel="noopener noreffer">some experimental techniques</a>, but the strong bottleneck is ultimately the <strong>expert throughput</strong>, aka your PCIe bandwidth.</p><p>On a standard 4x16 link, a GPU can request 32GB/s unidirectionally from CPU memory. <em>Assuming nothing else on the system presents a greater bottleneck</em>, that locks the user at <code>1/(2*size_of_layer_expert_in_GB) ~= 4</code> tokens per second. And this is the best you will ever get without resorting to non-standard inference strategies like some hypothetical <a href=https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1853289262 target=_blank rel="noopener noreffer">dynamic top-k</a></p><h2 id=final-note>Final note</h2><p>This article is an <strong>artefact of its time</strong> &ndash; it is an attempt to describe the state of affairs as of 13th December 2023, with facts, claims, and predictions appropriate for the date. It is highly likely that many of the assumptions & beliefs described in this article will be invalidated by the end of the week.</p><p>I publicise this for the sole purpose of gathering my thoughts on the matter, as well as to potentially spark useful discussions on the topic.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on December 13, 2023&nbsp;<a class=git-hash href=https://github.com/152334H/blog/commit/e86e4dc06342055c2029a4ddf602d722bc18d48c target=_blank title="commit by 152334H(54623771+152334H@users.noreply.github.com) e86e4dc06342055c2029a4ddf602d722bc18d48c: compile">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>e86e4dc</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://152334H.github.io/blog/mixtral-vs-oss/ data-title="Rough thoughts on Mixtral vs Open Source" data-hashtags="machine learning,llms,moe"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://152334H.github.io/blog/mixtral-vs-oss/ data-hashtag="machine learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://152334H.github.io/blog/mixtral-vs-oss/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://152334H.github.io/blog/mixtral-vs-oss/ data-title="Rough thoughts on Mixtral vs Open Source"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://152334H.github.io/blog/mixtral-vs-oss/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://152334H.github.io/blog/mixtral-vs-oss/ data-title="Rough thoughts on Mixtral vs Open Source"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://152334H.github.io/blog/mixtral-vs-oss/ data-title="Rough thoughts on Mixtral vs Open Source"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/machine-learning/>machine learning</a>,&nbsp;<a href=/tags/llms/>llms</a>,&nbsp;<a href=/tags/moe/>moe</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/blog/knowing-enough-about-moe/ class=prev rel=prev title="Knowing Enough About MoE to Explain Dropped Tokens in GPT-4"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Knowing Enough About MoE to Explain Dropped Tokens in GPT-4</a>
<a href=/blog/2023/ class=next rel=next title=2023>2023<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=giscus class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app>Giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://152334H.github.io/ target=_blank>152334H</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/twemoji@14.0.2/dist/twemoji.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{giscus:{category:"Announcements",categoryId:"DIC_kwDOH03r284CQ2Tu",darkTheme:"dark_dimmed",emitMetadata:"0",inputPosition:"bottom",lang:"en",lazyLoading:!1,lightTheme:"light",mapping:"pathname",reactionsEnabled:"1",repo:"152334H/152334h.github.io",repoId:"R_kgDOH03r2w"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"},twemoji:!0}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W0STJ4D3N3",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-W0STJ4D3N3" async></script></body></html>