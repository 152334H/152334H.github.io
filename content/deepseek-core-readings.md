---
title: "DeepSeek Core Readings"
date: 2024-06-23T00:00:00+08:00
author: "152334H"

lastmod: 2024-06-23T00:00:01+08:00

subtitle: "Status: <i>In progress</i>"
tags: [ machine learning, DeepSeek, WIP, series ]
categories: [ "not a post" ]
toc: false

---

These are a set of personal notes about [the deepseek core readings](https://x.com/teortaxesTex/status/1787866166242763217) ([extended](https://x.com/teortaxesTex/status/1802686282516074728)) ([elab](https://x.com/teortaxesTex/status/1805055350011232352)).

![](https://pbs.twimg.com/media/GQzX36eXIAANDLR?format=jpg&name=4096x4096)

They are **not** meant for mass public consumption (though you are free to read/cite), as I will only be noting down information that I care about.

<!--more-->

| Links | Post | TL;DR |
|-------|------|-------|
| [2401.14196](https://arxiv.org/pdf/2401.14196) [Repo](https://github.com/deepseek-ai/deepseek-coder/) | [DeepSeek Coder](/blog/deepseek-0) | V1 1.3/6.7/33B Code models. Evals beat other OSS + GPT-3.5. Arguably bad post-training + continued-pretraining. |
| [2401.02954](https://arxiv.org/pdf/2401.02954) [Repo](https://github.com/deepseek-ai/DeepSeek-LLM) | [DeepSeek LLM](/blog/deepseek-1) | V1 7B/67B Base/Chat models. Great details about scaling laws, thoughtful evaluation/alignment. |
| [2401.06066](https://arxiv.org/pdf/2401.06066) [Repo](https://github.com/deepseek-ai/DeepSeek-MoE) | DeepSeek MoE | |
| [2402.03300](https://arxiv.org/pdf/2402.03300) [Repo](https://github.com/deepseek-ai/DeepSeek-Math) | DeepSeek Math | |
| [2403.05525](https://arxiv.org/pdf/2403.05525) [Repo](https://github.com/deepseek-ai/DeepSeek-VL) | DeepSeek VL | |
| [paper.pdf](https://github.com/deepseek-ai/DeepSeek-V2/blob/main/deepseek-v2-tech-report.pdf) [Repo](https://github.com/deepseek-ai/DeepSeek-V2/tree/main) | DeepSeek V2 | |
| [paper.pdf](https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf) [Repo](https://github.com/deepseek-ai/DeepSeek-Coder-V2) | DeepSeek Coder V2 | . |

---

{{< series_summary "DeepSeek" >}}

---

I will be skipping the following papers:
| Paper | Reason |
|-------|--------|
| [DeepSeek Prover](https://arxiv.org/pdf/2405.14333) | No Code No Weights No Data. Also not really interested in LEAN-related LLM projects. |

